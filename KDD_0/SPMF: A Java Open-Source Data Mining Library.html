<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SPMF: A Java Open-Source Data Mining Library</title>
<style type="text/css">
<!--
.Style1 {
	color: #FF0000;
	font-weight: bold;
	font-size: x-large;
}
.StyleCounter {font-size: small}
-->
</style>
</head>

<body>
<table border="0" height="300px" width="100%">

<tbody><tr>

<td colspan="3" bgcolor="#FFFFCC" height="50px">
  <img src="SPMF:%20A%20Java%20Open-Source%20Data%20Mining%20Library_files/spmf.png" alt="SPMF" style="border: none;" height="69" width="196"><span class="Style1">An Open-Source Data Mining Library</span></td>

</tr>

<tr>

<td bgcolor="#FFFFCC" valign="top" width="121">

<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php">Introduction</a>
</p>


<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=algorithms.php">Algorithms</a>
</p>

<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=download.php">Download</a>
</p>


<p>
<b><a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=documentation.php">Documentation</a>
</b></p>

<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=datasets.php">Datasets</a>
</p>


<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=FAQ.php">FAQ</a>
</p>


<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=license.php">License</a>
</p>

<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=contributors.php">Contributors</a>
</p>

<p>
<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=citations.php">Citations</a>
</p>
<p>
    <a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=performance.php">Performance</a>
  </p>

<p>
    <a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=developers.php">Developers' guide</a>
  </p>


<p><a href="http://forum.ai-directory.com/list.php?5" target="_blank">Forum </a></p>


<p>
    <a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=mailinglist.php">Mailing-list</a>
  </p>


<p><a href="http://data-mining.philippe-fournier-viger.com/">Blog</a></p>
<p><span class="StyleCounter"><strong>
  <span class="counter" style="padding: 1px 1px 1px 1px;background: #ffffff;border: 0px solid #cccccc;"><span style="font: 12px;color: #000000; :link, :visited, :active, :link:hover, :visited:hover, :active:hover font: 12px;color: #000000;">273864</span></span>
</strong>  <strong>visitors </strong>since 2010-02</span></p>
</td>

<td colspan="2" valign="top" width="878">

ï»¿

  
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

  
  <style type="text/css">
<!--
.Style2 {
	color: #0033CC;
	font-weight: bold;
}
.Style3 {
	color: #0000FF;
	font-weight: bold;
}
.Style4 {
	color: #000000;
	font-weight: bold;
}
.Style5 {
	color: #FF0000
}
.Style6 {
	color: #FF0000;
	font-weight: bold;
}
.Style7 {
	color: #000000
}
.Style8 {
	color: #ff0000;
	font-weight: bold;
}
.Style9 {
	color: #0000FF
}
-->
  </style>
  <title></title>

  

<h2>Documentation</h2>

<p>This section provides <strong> examples</strong> of how to use SPMF
to perform various data mining tasks. </p>

<p>If you have any question or if you want to <strong>report a bug</strong>,
you can check the <a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=FAQ.php">FAQ</a>,
post in the <a href="http://forum.ai-directory.com/list.php?5">forum</a>
or <a href="http://www.philippe-fournier-viger.com/">contact me</a>.
You can also have a look at the various articles that I have referenced
on the <a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=algorithms.php">algorithms</a>
page of this website to learn more about each algorithm.</p>

<h3>List of examples</h3>

<blockquote>
  <p><strong>Itemset Mining (Frequent Itemsets</strong><strong>, Rare
Itemsets, etc.</strong>)</p>
</blockquote>

<ul>

  <li><a href="#example1">Example 1 : Mining Frequent
Itemsets by Using the Apriori Algorithm</a></li>
  <li><a href="#aprioritid">Example 2 : Mining Frequent
Itemsets by Using the AprioriTID Algorithm</a></li>
  <li><a href="#growth">Example 3 : Mining Frequent
Itemsets by Using the FP-Growth Algorithm</a></li>
  <li><a href="#c23">Example 4 : Mining Frequent
Itemsets by Using the Relim Algorithm</a></li>
  <li><a href="#e1">Example 5 : Mining Frequent
Itemsets by Using the Eclat / dEclat Algorithm</a></li>
  <li><a href="#hmine">Example 6 : Mining Frequent
Itemsets by Using the H-Mine Algorithm</a></li>
  <li><a href="#FIN">Example 7 : Mining Frequent
Itemsets by Using the FIN Algorithm</a></li>
  <li><a href="#PrePost">Example 8 : Mining Frequent
Itemsets by Using the PrePost / PrePost+ Algorithm</a></li>
  <li><a href="#LCMFreq">Example 9 : Mining Frequent
Itemsets by Using the LCMFreq Algorithm</a></li>
  <li><a href="#example2">Example 10 : Mining Frequent
Closed Itemsets Using the AprioriClose Algorithm</a></li>
  <li><a href="#dciclosed">Example 11 : Mining Frequent
Closed Itemsets Using the DCI_Closed Algorithm</a></li>
  <li><a href="#e2">Example 12 : Mining Frequent Closed
Itemsets Using the Charm / dCharm Algorithm</a></li>
  <li><a href="#LCM">Example 13 : Mining Frequent Closed
Itemsets Using the LCM Algorithm</a></li>
  <li><a href="#fpclose">Example 14 : Mining Frequent
Closed Itemsets Using the FPClose Algorithm</a></li>
  <li><a href="#fpmax">Example 15 : Mining Frequent
Maximal Itemsets Using the FPMax Algorithm</a></li>
<!--<li><a href="#LCMMax">Example

    <!-- ?php 

  echo $a++;? -->
  <li><a href="#e3">Example 16 : Mining Frequent Maximal
Itemsets Using the Charm-MFI Algorithm</a></li>
  <li><a href="#defme">Example 17 : Mining Frequent
Generator Itemsets Using the DefMe Algorithm</a></li>
  <li><a href="#pascal">Example 18 : Mining Frequent
Itemsets and Identify the Generators Using the Pascal Algorithm</a></li>
  <li><a href="#zart">Example 19 : Mining Frequent
Closed Itemsets and Minimal Generators Using the Zart Algorithm </a></li>
  <li><a href="#example17">Example 20 : Mining Minimal
Rare Itemsets Using the AprioriRare Algorithm</a></li>
  <li><a href="#example18">Example 21 : Mining Perfectly
Rare Itemsets Using the AprioriInverse Algorithm</a></li>
  <li><a href="#cori">Example 22 : Mining Rare
Correlated Itemsets Using the CORI Algorithm</a></li>
  <li><a href="#example22">Example 23 : Mining Closed
Itemsets from a Data Stream Using the CloStream Algorithm</a> (source
code version only)</li>
  <li><a href="#estdec">Example 24 : Mining Recent
Frequent Itemsets from a Data Stream Using the estDec Algorithm</a>
(source code version only)</li>
  <li><a href="#estdec+">Example 25 : Mining Recent
Frequent Itemsets from a Data Stream Using the estDec+ Algorithm</a>
(source code version only)</li>
  <li><a href="#uapriori">Example 26 : Mining Frequent
Itemsets from Uncertain Data with the U-Apriori Algorithm</a></li>
  <li><a href="#erasable">Example 27 : Mining Erasable
Itemsets from a Product Database with the VME algorithm</a></li>
  <li><a href="#itree">Example 28 : Building, updating
incrementally and using an Itemset-Tree to generate targeted frequent
itemsets and association rules</a> (source code version only)</li>
  <li><a href="#meit">Example 29 : Building, updating
incrementally and using a Memory-Efficient Itemset-Tree to generate
targeted frequent itemsets and association rules</a> (source code
version only)</li>
  <li><a href="#msapriori">Example 30 : Mining Frequent
Itemsets with Multiple Support Thresholds Using the MSApriori Algorithm</a></li>
  <li><a href="#cfpgrowth">Example 31 : Mining Frequent
Itemsets with Multiple Support Thresholds Using the CFPGrowth++
Algorithm</a></li>
</ul>

<blockquote>
  <p><strong>High-Utility Pattern Mining</strong></p>
</blockquote>

<ul>

  <li><a href="#twophase">Example 32 : Mining
High-Utility Itemsets from a Transaction Database with Utility
Information using the Two-Phase Algorithm</a></li>
  <li><a href="#fhm">Example 33 : Mining High-Utility
Itemsets from a Transaction Database with Utility Information using the
FHM Algorithm</a></li>
  <li><a href="#efim">Example 34 : Mining High-Utility
Itemsets from a Transaction Database with Utility Information using the
EFIM Algorithm</a></li>
  <li><a href="#huiminer">Example 35 : Mining
High-Utility Itemsets from a Transaction Database with Utility
Information using the HUI-Miner Algorithm</a></li>
  <li><a href="#hupminer">Example 36 : Mining
High-Utility Itemsets from a Transaction Database with Utility
Information using the HUP-Miner Algorithm</a></li>
  <li><a href="#upgrowth">Example 37 : Mining
High-Utility Itemsets from a Transaction Database with Utility
Information using the UP-Growth / UP-Growth+ Algorithm</a></li>
  <li><a href="#ihup">Example 38 : Mining High-Utility
Itemsets from a Transaction Database with Utility Information using the
IHUP Algorithm</a></li>
  <li><a href="#d2hup">Example 39 : Mining High-Utility
Itemsets from a Transaction Database with Utility Information using the
d2HUP Algorithm</a></li>
  <li><a href="#fhmplus">Example 40 : Mining High-Utility Itemsets from a
 Transaction Database with Utility Information while considering Length 
Constraints, using the FHM+ algorithm</a></li>
  <li><a href="#fchm">Example 41 : Mining Correlated High-Utility
    Itemsets in a Transaction Database with Utility Information using the FCHM algorithm
  </a></li>
  <li><a href="#fhmfreq">Example 42 : Mining Frequent
High-Utility Itemsets from a Transaction Database with Utility
Information using the FHMFreq Algorithm</a></li>
  <li><a href="#fhn">Example 43 : Mining High-Utility
Itemsets from a Transaction Database with Positive or Negative Unit
Profit using the FHN Algorithm</a></li>
  <li><a href="#huiniv">Example 44 : Mining High-Utility
Itemsets from a Transaction Database with Positive or Negative Unit
Profit using the HUINIV-Mine Algorithm</a></li>
  <li><a href="#foshu">Example 45 : Mining On-Shelf
High-Utility Itemsets from a Transaction Database using the FOSHU
Algorithm</a></li>
  <li><a href="#tshoun">Example 46 : Mining On-Shelf
High-Utility Itemsets from a Transaction Database using the TS-HOUN
Algorithm</a></li>
  <li><a href="#eihi">Example 47 : Incremental
High-Utility Itemset Mining in a Transaction Database with utility
information using the EIHI Algorithm</a> (source code version only)</li>
  <li><a href="#huilistins">Example 48 : Incremental
High-Utility Itemset Mining in a Transaction Database with utility
information using the HUI-LIST-INS Algorithm</a> (source code version
only)</li>
  <li><a href="http://www.philippe-fournier-viger.com/spmf/documentation.php#efimclosed">Example 49 :
Mining Closed
High-Utility Itemsets from a transaction database with utility
information using the EFIM-Closed Algorithm</a></li>
  <li><a href="http://www.philippe-fournier-viger.com/spmf/documentation.php#chuiminer">Example 50 :
Mining Closed
High-Utility Itemsets from a transaction database with utility
information using the CHUI-Miner Algorithm</a></li>
  <li><a href="#ghuiminer">Example 51 :
Mining Generators of High-Utility Itemsets from a transaction
database with utility information using the GHUI-Miner Algorithm</a></li>
  <li><a href="#hugminer">Example 52 :
Mining
High-Utility
Generator Itemsets from a transaction database with utility information
using the HUG-Miner Algorithm</a></li>
  <li><a href="#minfhm">Example 53 :
Mining
Minimal High-Utility
Itemsets from a transaction database with utility information
using the MinFHM Algorithm</a></li>
  <li><a href="#skymine">Example 54 :
Mining
Skyline High-Utility
Itemsets in a transaction database with utility information
using the SkyMine Algorithm</a></li>
  <li><a href="#husrm">Example 55 : Mining High-Utility
Sequential Rules from a Sequence Database with utility information
using the HUSRM Algorithm</a></li>
  <li><a href="#uspan">Example 56 : Mining High-Utility
    Sequential Patterns from a Sequence Database with utility information
  using the USPAN Algorithm</a></li>
</ul>

<blockquote>
  <p><strong>Association Rule Mining</strong></p>
</blockquote>

<ul>

  <li><a href="#allassociationrules">Example 57 : Mining
All Association Rules </a></li>
  <li><a href="#lift">Example 58 : Mining All
Association Rules with the lift measure </a></li>
  <li><a href="#gcd">Example 59 : Mining All Association Rules using the GCD algorithm</a></li>
  <li><a href="#example7">Example 60 : Mining the IGB
basis of Association Rules</a></li>
  <li><a href="#example19">Example 61 : Mining Perfectly
Sporadic Association Rules</a></li>
  <li><a href="#example20">Example 62 : Mining Closed
Association Rules</a></li>
  <li><a href="#example21">Example 63 : Mining Minimal
Non Redundant Association Rules</a></li>
  <li><a href="#indirect">Example 64 : Mining Indirect
Association Rules with the INDIRECT algorithm</a></li>
  <li><a href="#FHSAR">Example 65 : Hiding Sensitive
Association Rules with the FHSAR algorithm.</a></li>
  <li><a href="#topkrules">Example 66 : Mining the Top-K
Association Rules</a></li>
  <li><a href="#tns">Example 67 : Mining the Top-K
Non-Redundant Association Rules</a></li>
<!--<li><a href="#tnr">Example

    < ? p h p   echo $a++;?>    : Mining the Top-K Non-Redundant Association Rules</a></li>-->
</ul>

<blockquote>
  <p><strong>Clustering</strong></p>
</blockquote>

<ul>

  <li><a href="#example8">Example 68 : Clustering Values
with the K-Means algorithm</a></li>
  <li><a href="#dbscan">Example 69 : Clustering Values
with the DBScan algorithm</a></li>
  <li><a href="#optics">Example 70 : Using Optics to
extract a cluster-ordering of points and DB-Scan style clusters</a></li>
  <li><a href="#bisecting">Example 71 : Clustering
Values with the Bisecting K-Means algorithm</a></li>
  <li><a href="#example10">Example 72 : Clustering
Values with a Hierarchical Clustering algorithm</a></li>
</ul>

<blockquote>
  <p><strong>Sequential Pattern Mining </strong></p>
</blockquote>

<ul>

  <li><a href="#examplePrefixSpan">Example 73 : Mining
Frequent Sequential Patterns Using the PrefixSpan Algorithm</a></li>
  <li><a href="#gsp">Example 74 : Mining Frequent
Sequential Patterns Using the GSP Algorithm</a></li>
  <li><a href="#spade">Example 75 : Mining Frequent
Sequential Patterns Using the SPADE Algorithm</a></li>
  <li><a href="#cmspade">Example 76 : Mining Frequent
Sequential Patterns Using the CM-SPADE Algorithm</a></li>
  <li><a href="#spam">Example 77 : Mining Frequent
Sequential Patterns Using the SPAM Algorithm</a></li>
  <li><a href="#cmspam">Example 78 : Mining Frequent
Sequential Patterns Using the CM-SPAM Algorithm</a></li>
  <li><a href="#lapin">Example 79 : Mining Frequent
Sequential Patterns Using the LAPIN Algorithm</a></li>
  <li><a href="#clasp">Example 80 : Mining Frequent
Closed Sequential Patterns Using the ClaSP Algorithm</a></li>
  <li><a href="#cmclasp">Example 81 : Mining Frequent
Closed Sequential Patterns Using the CM-ClaSP Algorithm</a></li>
  <li><a href="#clospan">Example 82 : Mining Frequent
Closed Sequential Patterns Using the CloSpan Algorithm</a></li>
  <li><a href="#exampleBIDE">Example 83 : Mining
Frequent Closed Sequential Patterns Using the BIDE+ Algorithm</a></li>
  <li><a href="#postclosed">Example 84 : Mining Frequent
Closed Sequential Patterns by Post-Processing using SPAM or PrefixSpan</a></li>
  <li><a href="#maxsp">Example 85 : Mining Frequent
Maximal Sequential Patterns Using the MaxSP Algorithm</a></li>
  <li><a href="#vmsp">Example 86 : Mining Frequent
Maximal Sequential Patterns Usin</a></li>
  <li><a href="#vmsp">g the VMSP Algorithm</a></li>
  <li><a href="#feat">Example 87 : Mining Frequent
Sequential Generator Patterns Using the FEAT Algorithm</a></li>
  <li><a href="#fsgp">Example 88 : Mining Frequent
Sequential Generator Patterns Using the FSGP Algorithm</a></li>
  <li><a href="#vgen">Example 89 : Mining Frequent
Sequential Generator Patterns Using the VGEN Algorithm</a></li>
  <li><a href="#gokrimp">Example 90 : Mining Compressing
Sequential Patterns Using the GoKrimp Algorithm</a></li>
  <li><a href="#tks">Example 91 : Mining Frequent Top-K
Sequential Patterns Using the TKS Algorithm</a></li>
  <li><a href="#tsp">Example 92 : Mining Frequent Top-K
Sequential Patterns Using the TSP Algorithm</a></li>
  <li><a href="#exampleMDSPM1">Example 93 : Mining
Frequent Multi-dimensional Sequential Patterns Using SeqDIM (with
PrefixSpan and Apriori</a>)</li>
  <li><a href="#exampleMDSPM2">Example 94 : Mining
Frequent Closed Multi-dimensional Sequential Patterns Using
SeqDIM/Songram (with Bide+ and AprioriClose)</a></li>
  <li><a href="#example11">Example 95 : Mining
Sequential Patterns with Time Constraints from a Time-Extended Sequence
Database</a></li>
  <li><a href="#example12">Example 96 : Mining Closed
Sequential Patterns with Time Constraints from a Time-Extended Sequence
Database</a></li>
  <li><a href="#example13">Example 97 : Mining
Sequential Patterns with Time Constraints from a Time-Extended Sequence
Database containing Valued Items</a> (source code version only)</li>
  <li><a href="#example14">Example 98 : Mining Closed
Multi-dimensional Sequential Patterns from a Time-Extended Sequence
Database</a></li>
</ul>

<blockquote>
  <p><strong>Sequential Rule Mining </strong></p>
</blockquote>

<ul>

  <li><a href="#cmrules">Example 99 : Mining Sequential
Rules Common to Several Sequences with the CMRules algorithm</a></li>
  <li><a href="#example12">Example 100 : </a><a href="#cmdeo">Mining Sequential Rules Common to Several Sequences with
the CMDeo algorithm</a></li>
  <li><a href="#rulegrowth">Example 101 : Mining
Sequential Rules Common to Several Sequences with the RuleGrowth
algorithm</a></li>
  <li><a href="#erminer">Example 102 : Mining Sequential
Rules Common to Several Sequences with the ERMiner algorithm</a></li>
  <li><a href="#rulegen">Example 103 : Mining Sequential
Rules between Sequential Patterns with the RuleGen algorithm</a></li>
  <li><a href="#trulegrowth">Example 104 : Mining
Sequential Rules Common to Several Sequences with the Window Size
Constraint using TRuleGrowth </a></li>
  <li><a href="#topseqrules">Example 105 : Mining the
Top-K Sequential rules</a></li>
  <li><a href="#tns">Example 106 : Mining the Top-K
Non-Redundant Sequential rules</a></li>
</ul>

<blockquote>
  <p><strong>Sequence Prediction</strong> (source code version only)</p>
</blockquote>

<ul>

  <li><a href="#cptPlus">Example 107 : Perform Sequence
Prediction using the CPT+ Sequence Prediction Model</a></li>
  <li><a href="#cpt">Example 108 : Perform Sequence
Prediction using the CPT Sequence Prediction Model</a></li>
  <li><a href="#ppm">Example 109 : Perform Sequence
Prediction using the PPM Sequence Prediction Model</a></li>
  <li><a href="#dg">Example 110 : Perform Sequence
Prediction using the DG Sequence Prediction Model</a></li>
  <li><a href="#akom">Example 111 : Perform Sequence
Prediction using the AKOM Sequence Prediction Model</a></li>
  <li><a href="#tdag">Example 112 : Perform Sequence
Prediction using the TDAG Sequence Prediction Model</a></li>
  <li><a href="#lz78">Example 113 : Perform Sequence
Prediction using the LZ78 Sequence Prediction Model</a></li>
  <li><a href="#compareseqmodel">Example 114 : Comparing
Several Sequence Prediction Models</a></li>
</ul>

<blockquote>
  <p><strong>Periodic pattern mining</strong></p>
</blockquote>
<ul>
  <li><a href="#pfpm">Example 115 : Mining Periodic Frequent Patterns using the PFPM algorithm</a></li>
  <li><a href="#phm">Example 116 : Mining Periodic High-Utility Itemsets using the PHM algorithm</a></li>
</ul>
<blockquote>
  <p><strong>Text Mining</strong></p>
</blockquote>

<ul>

  <li><a href="#TextClusterer">Example 117 : Clustering
Texts with a text clusterer</a></li>
  <li><a href="#textclassifier">Example 118 : Classifying
Text documents using a Naive Bayes approach</a> (source code version
only)</li>
</ul>

<blockquote>
  <p><strong>Classification</strong></p>
</blockquote>

<ul>

  <li><a href="#id3">Example 119 : Creating a decision
tree with the ID3 algorithm to predict the value of a target attribute</a>
(source code version only)</li>
</ul>

<blockquote>
  <p><strong>Tools</strong></p>
</blockquote>

<ul>

  <li><a href="#convseq">Example 120 : Converting a
sequence database to SPMF format (CSV, KOSARAK, IBM, BMS, Snake...)</a></li>
  <li><a href="#convtdb">Example 121 : Converting a
transaction database to SPMF format (CSV...)</a></li>
  <li><a href="#convseqtrans">Example 122 : Converting a
sequence database to a transaction database</a></li>
  <li><a href="#convtransseq">Example 123 : Converting a
transaction database to a sequence database</a></li>
  <li><a href="#genseq">Example 124 : Generating a
synthetic sequence database</a></li>
  <li><a href="#genseqt">Example 125 : Generating a </a><a href="#genseq">synthetic </a><a href="#genseqt">sequence database
with timestamps</a></li>
  <li><a href="#gentrans">Example 126 : Generating a </a><a href="#genseq">synthetic</a> <a href="#gentrans">transaction database</a></li>
  <li><a href="#genutilvalues">Example 127 : Generating
synthetic utility values for a transaction database without utility
values</a></li>
  <li><a href="#statsseq">Example 128 : Calculating
statistics for a sequence database</a></li>
  <li><a href="#stattrans">Example 129 : Calculating
statistics for a transaction database</a></li>
  <li><a href="#addtimestamps">Example 130 : Add
consecutive timestamps to a sequence database without timestamps</a></li>
  <li><a href="#arff">Example </a><a href="#statsseq">131    </a><a href="#arff"> : Using the ARFF format in the source code
version of
SPMF</a></li>
  <li><a href="#fixtdb">Example </a><a href="#statsseq">132    </a><a href="#fixtdb"> : Fix a transaction database</a></li>
  <li><a href="#removeutility">Example </a><a href="#statsseq">133 </a><a href="#removeutility"> : Remove utility information from a transaction database</a></li>
  <li><a href="#removeutility">Example </a><a href="#resize">134</a><a href="#resizedatabase"> : Resize a database in SPMF format (a text file)</a></li>
</ul>

<blockquote> </blockquote>

<h3><strong><span class="centered"><a name="example1" id="example1"> </a></span></strong>Example 1 : Mining Frequent Itemsets by Using the Apriori
Algorithm </h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Apriori</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run Apriori
contextPasquier99.txt output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestApriori_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Apriori?</p>

<blockquote>
  <p><strong>Apriori</strong> is an algorithm for discovering frequent
itemsets in transaction databases. It was proposed by Agrawal &amp;
Srikant (1993).</p>
</blockquote>

<p>What is the input of the Apriori algorithm?</p>

<blockquote>
  <p>The input is a <strong>transaction database</strong> (aka binary
context) and a threshold named <em><strong>minsup</strong></em> (a
value between 0 and 100 %). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
</blockquote>

<table align="center" border="1" width="316">

  <tbody>
    <tr>
      <td width="144"><strong>Transaction id</strong></td>
      <td width="156"><strong>Items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 3, 4}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{2, 5}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{1, 2, 3, 5}</td>
    </tr>
  </tbody>
</table>

<p>What is the output of the Apriori algorithm?</p>

<blockquote>
  <p>Apriori is an algorithm for discovering itemsets (group of items)
occurring frequently in a transaction database (<strong>frequent
itemsets</strong>). A frequent itemset is an itemset appearing in at
least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a parameter given by the user. </p>
  <p>For example, if <strong>Apriori</strong> is run on the previous
transaction database with a minsup of 40 % (2 transactions), Apriori
produces the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote> In the results, each itemset is annotated with its
support. The <strong>support</strong> of an itemset is how many times
the itemset appears in the transaction database. For example, the
itemset {2, 3 5} has a support of 3 because it appears in transactions
t2, t3 and t5. It is a frequent itemset because its support is higher
or equal to the <em>minsup </em>parameter.</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> for Apriori is defined as
follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The Apriori algorithm is an important algorithm for historical
reasons and also because it is a simple algorithm that is easy to
learn. However, faster and more memory efficient algorithms have been
proposed. If efficiency is required, it is recommended to use a more
efficient algorithm like FPGrowth instead of Apriori. You can see a
performance comparison of Apriori, FPGrowth, and other frequent itemset
mining algorithms by clicking on the "<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=performance.php">performance</a>"
section of this website.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p> In SPMF, there is also an implementation of Apriori that uses a<strong>
hash-tree </strong>as an internal structure to store candidates. This
structure provide a more efficient way to count the support of
itemsets. This version of Apriori is named "<strong>Apriori_with_hash_tree</strong>"
in the GUI of SPMF and the command line. For the source code version,
it can be run by executing the test file <strong>MainTestAprioriHT_saveToFile.java</strong>.
This version of Apriori can be up to twice faster than the regular
version in some cases but it uses more memory. This version of Apriori
has two parameters: (1) minsup and (2) the number of child nodes that
each node in the hash-tree should have. For the second parameter, we
suggest to use the value 30.</p>
</blockquote>

<p>Where can I get more information about the Apriori algorithm?</p>

<blockquote>
  <p> This is the technical report published in 1994 describing
Apriori. </p>
  <p><em>R. Agrawal and R. Srikant. <a href="http://www.philippe-fournier-viger.com/spmf/apriori_longer.pdf" rel="nofollow">Fast algorithms for mining association rules in large
databases</a>. Research Report RJ 9839, IBM Almaden Research Center,
San Jose, California, June 1994.</em></p>
  <p>You can also read <a rel="nofollow" href="http://www-users.cs.umn.edu/%7Ekumar/dmbook/ch6.pdf">chapter 6
of the book "introduction to data mining"</a> which provide a nice and
easy to understand introduction to Apriori.</p>
</blockquote>

<h3><strong><span class="centered"><a name="aprioritid" id="example16">
</a></span></strong>Example 2 : Mining Frequent
Itemsets by Using the AprioriTid Algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">Apriori_TID</span>"</strong>
algorithm <strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run Apriori_TID
contextPasquier99.txt output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAprioriTID_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is AprioriTID?</p>

<blockquote>
  <p>AprioriTID is an algorithm for discovering frequent itemsets
(groups of items appearing frequently) in a transaction database. It
was proposed by Agrawal &amp; Srikant (1993).</p>
  <p>AprioriTID is a variation of the Apriori algorithm. It was
proposed in the same article as Apriori as an alternative
implementation of Apriori. It produces the same output as Apriori. But
it uses a different mechanism for counting the support of itemsets.</p>
</blockquote>

<p>What is the input of the AprioriTID algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the AprioriTID algorithm?</p>

<blockquote>
  <p><strong>AprioriTID</strong> is an algorithm for discovering
itemsets (group of items) occurring frequently in a transaction
database (<strong>frequent itemsets</strong>). A frequent itemset is an
itemset appearing in at least <em>minsup </em>transactions from the
transaction database, where <em>minsup </em>is a parameter given by
the user. </p>
  <p>For example, if <strong>AprioriTID</strong> is run on the
previous transaction database with a minsup of 40 % (2 transactions), <strong>AprioriTID</strong>
produces the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each itemset is annotated with its support. The <strong>support</strong>
of an itemset is how many times the itemset appears in the transaction
database. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by AprioriTID is
defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The Apriori and AprioriTID algorithms are important algorithms for
historical reasons and also because they are simple algorithms that are
easy to learn. However, faster and more memory efficient algorithms
have been proposed. For efficiency, it is recommended to use more
efficient algorithms like <strong>FPGrowth</strong> instead of
AprioriTID or Apriori. You can see a performance comparison of Apriori,
AprioriTID, FPGrowth, and other frequent itemset mining algorithms by
clicking on the "<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=performance.php">performance</a>"
section of this website.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">There are <strong>two</strong>
versions of <strong>AprioriTID</strong> in SPMF. The first one is
called </span></span><span class="Style2">AprioriTID</span><span class="Style5"><span class="Style7"> and is the regular <strong>AprioriTID</strong>
algorithm.</span></span> The second one is called <span class="Style2">AprioriTID_Bitset
  </span>and uses <strong>bitsets</strong> as internal structures
instead of HashSet of Integers to represent sets of transactions IDs.
The advantage of the bitset version is that using bitsets for
representing sets of transactions IDs is more memory efficient and
performing the intersection of two sets of transactions IDs is more
efficient with bitsets (it is done by doing the logical AND operation).</p>
</blockquote>

<p>Where can I get more information about the AprioriTID algorithm?</p>

<blockquote> This is the technical report published in 1994 describing
Apriori and AprioriTID.
  <p><em>R. Agrawal and R. Srikant. <a href="http://www.philippe-fournier-viger.com/spmf/apriori_longer.pdf" rel="nofollow">Fast algorithms for mining association rules in large
databases</a>. Research Report RJ 9839, IBM Almaden Research Center,
San Jose, California, June 1994.</em></p>
  <p>You can also read <a rel="nofollow" href="http://www-users.cs.umn.edu/%7Ekumar/dmbook/ch6.pdf">chapter 6
of the book "introduction to data mining"</a> which provide a nice and
easy to understand introduction to Apriori.</p>
</blockquote>

<h3><strong><span class="centered"><a name="growth" id="c"> </a></span></strong>Example 3 : Mining Frequent Itemsets by Using the FP-Growth
Algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FPGrowth_itemsets</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">FPGrowth_itemsets</span></strong> contextPasquier99.txt
output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFPGrowth_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is FPGrowth?</p>

<blockquote>
  <p>FPGrowth is an algorithm for discovering frequent itemsets in a
transaction database. It was proposed by Han et al. (2000). FPGrowth is
a very fast and memory efficient algorithm. It uses a special internal
structure called an FP-Tree. </p>
</blockquote>

<p>What is the input of the FPGrowth algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input of <strong>FPGrowth</strong> is a <strong>transaction
database</strong> (aka binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the FPGrowth algorithm?</p>

<blockquote>
  <p><strong>FPGrowth</strong> is an algorithm for discovering itemsets
(group of items) occurring frequently in a transaction database (<strong>frequent
itemsets</strong>). A frequent itemset is an itemset appearing in at
least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a parameter given by the user. </p>
  <p>For example, if<strong> FPGrowth </strong>is run on the previous
transaction database with a minsup of 40 % (2 transactions),<strong>
FPGrowth</strong> produces the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote> In the results, each itemset is annotated with its
support. The <strong>support</strong> of an itemset is how many times
the itemset appears in the transaction database. For example, the
itemset {2, 3 5} has a support of 3 because it appears in transactions
t2, t3 and t5. It is a frequent itemset because its support is higher
or equal to the <em>minsup </em>parameter.</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by FPGrowth is
defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>There exists several algorithms for mining frequent itemsets. In
SPMF, you can try for example Apriori, AprioriTID, Eclat, HMine, Relim
and more. Among all these algorithms, <strong>FPGrowth</strong> is
generally the fastest and most memory efficient algorithm. You can see
a performance comparison by clicking on the "<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=performance.php">performance</a>"
section of this website.</p>
</blockquote>

<p>Where can I get more information about the FPGrowth algorithm?</p>

<blockquote> This is the journal article describing FPGrowth:
  <p><em>Jiawei Han, Jian Pei, Yiwen Yin, Runying Mao: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/fpgrowth_04.pdf">Mining
Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree
Approach</a>. Data Min. Knowl. Discov. 8(1): 53-87 (2004)</em> </p>
  <p>You can also read <a rel="nofollow" href="http://www-users.cs.umn.edu/%7Ekumar/dmbook/ch6.pdf">chapter 6
of the book "introduction to data mining"</a> which provide an easy to
understand introduction to FPGrowth (but does not give all the details).</p>
</blockquote>

<h3><strong><span class="centered"><a name="c23" id="c23"> </a></span></strong>Example 4 : Mining Frequent Itemsets by Using the Relim
Algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">Relim</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Relim </span></strong>contextPasquier99.txt
output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestRelim.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Relim?</p>

<blockquote>
  <p><strong>Relim</strong> is an algorithm for discovering frequent
itemsets in a transaction database. Relim was proposed by Borgelt
(2005). It is not a very efficient algorithm. It is included in SPMF
for comparison purposes.</p>
</blockquote>

<p>What is the input of the Relim algorithm?</p>

<blockquote>
  <p>The input is a <strong>transaction database</strong> (aka binary
context) and a threshold named <em><strong>minsup</strong></em> (a
value between 0 and 100 %). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the Relim algorithm?</p>

<blockquote>
  <p><strong>Relim</strong> is an algorithm for discovering itemsets
(group of items) occurring frequently in a transaction database (<strong>frequent
itemsets</strong>). A frequent itemset is an itemset appearing in at
least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a parameter given by the user. </p>
  <p>For example, if<strong> Relim</strong> is run on the previous
transaction database with a minsup of 40 % (2 transactions),<strong>
Relim </strong>produces the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each itemset is annotated with its support. The <strong>support</strong>
of an itemset is how many times the itemset appears in the transaction
database. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by Relim is defined
as follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>There exists several algorithms for mining frequent itemsets.
Relim is not a very efficient algorithm. For efficiency, it is
recommended to use FPGrowth for better performance. You can see a
performance comparison by clicking on the "<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=performance.php">performance</a>"
section of this website.</p>
</blockquote>

<p>Where can I get more information about the FPGrowth algorithm?</p>

<blockquote>
  <p>This is the conference article describing Relim: </p>
  <p><em><a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/relim.pdf">Keeping
Things Simple: Finding Frequent Item Sets by Recursive Elimination </a>Christian
Borgelt. Workshop Open Source Data Mining Software (OSDM'05, Chicago,
IL), 66-70. ACM Press, New York, NY, USA 2005</em></p>
  <p>Note that the author of Relim and collaborators have proposed
extensions and additional optimizations of Relim that I have not
implemented.</p>
</blockquote>

<h3><strong><span class="centered"><a name="e1" id="e1"> </a></span></strong>Example 5 : Mining Frequent Itemsets by Using the Eclat /
dEclat Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style2">Eclat</span>"</strong> or <strong>"<span class="Style2">dEclat</span>"</strong> algorithm<strong>, </strong>
(2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> Eclat</strong>
contextPasquier99.txt output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPasquier99.txt</span>. </li>
  <li><strong>If you are using the source code version of SPMF, </strong>to
run respectively <strong>Eclat </strong>or<strong> dEclat, </strong>launch
the file <strong><span class="Style2">"MainTestEclat_saveToMemory.java"</span></strong>
or <strong><span class="Style2">"MainTestDEclat_bitset_saveToMemory.java"</span></strong>in
the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Eclat ?</p>

<blockquote>
  <p><strong>Eclat</strong> is an algorithm for discovering frequent
itemsets in a transaction database. It was proposed by Zaki (2001).
Contrarily to algorithms such as Apriori, <strong>Eclat</strong> uses
a depth-first search for discovering frequent itemsets instead of a
breath-first search.</p>
  <p><strong>dEclat </strong>is a variation of the Eclat algorithm
that is implemented using a structure called "diffsets" rather than
"tidsets".</p>
</blockquote>

<p>What is the input of the Eclat algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the Eclat algorithm?</p>

<blockquote>
  <p><strong>Eclat</strong> is an algorithm for discovering itemsets
(group of items) occurring frequently in a transaction database (<strong>frequent
itemsets</strong>). A frequent itemset is an itemset appearing in at
least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a parameter given by the user. </p>
  <p>For example, if<strong> Eclat</strong> is run on the previous
transaction database with a <em>minsup</em> of 40 % (2 transactions),<strong>
Eclat </strong>produces the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote>
  <p>Each frequent itemset is annotated with its support. The <strong>support</strong>
of an itemset is how many times the itemset appears in the transaction
database. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by ECLAT is defined
as follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>There exists several algorithms for mining frequent itemsets.
Eclat is one of the best. But generally, FPGrowth is a better
algorithm. You can see a performance comparison by clicking on the "<a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=performance.php">performance</a>"
section of this website. Note that recently (SPMF v0.96e), the Eclat
implementation was optimized and is sometimes faster than FPGrowth.</p>
  <p>Nevertheless, the Eclat algorithm is interesting because it uses a
depth-first search. For some extensions of the problem of itemset
mining such as mining high utility itemsets (see the HUI-Miner
algorithm), the search procedure of Eclat works very well.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In SPMF, there are four versions of ECLAT. The first one is named "<span class="Style2">Eclat</span>" and uses HashSets of Integers for
representing sets of transaction IDs (tidsets). The second version is
named "<span class="Style2">Eclat_bitset</span>" and uses bitsets for
representing tidsets. Using bitsets has the advantage of generally
being more memory efficient and can also make the algorithm faster
depending on the dataset.</p>
  <p>There is also two versions of <strong>dEclat</strong>, which
utilizes a structure called diffsets instead of tidsets. The versions
having diffsets implemented as HashSets of integers and the version
having diffsets implemented as bitsets are respectively named "<strong class="Style2">dEclat_bitset</strong>" and "<strong class="Style2">dEcla</strong>t"</p>
</blockquote>

<p>Where can I get more information about the Eclat algorithm?</p>

<blockquote>
  <p>Here is an article describing the Eclat algorithm:</p>
  <p><em>Mohammed Javeed Zaki: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/zaki2000.pdf">Scalable
Algorithms for Association Mining</a>. IEEE Trans. Knowl. Data Eng.
12(3): 372-390 (2000)</em></p>
  <p>Here is an article describing the dEclat variation:</p>
  <p><em>Zaki, M.J., Gouda, K.: <a href="http://www.philippe-fournier-viger.com/spmf/dEclat_dCharm.pdf" rel="nofollow">Fast vertical mining using diffsets</a>. Technical
Report 01-1, Computer Science Dept., Rensselaer Polytechnic Institute
(March 2001) 10</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="hmine" id="hmine"> </a></span></strong>Example 6 : Mining Frequent Itemsets by Using the HMine
Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HMine</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">0.4 </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9">HMine</span></strong> contextPasquier99.txt output.txt
0.4</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestHMine.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is H-Mine ?</p>

<blockquote>
  <p>H-Mine is an algorithm for discovering frequent itemsets in
transaction databases, proposed by Pei et al. (2001). Contrarily to
previous algorithms such as Apriori, H-Mine uses a pattern-growth
approach to discover frequent itemsets.</p>
</blockquote>

<p>What is the input of the H-Mine algorithm?</p>

<blockquote>
  <p>The input of H-Mine is a <strong>transaction database</strong>
(aka binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the H-Mine algorithm?</p>

<blockquote>
  <p><strong>H-Mine</strong> is an algorithm for discovering itemsets
(group of items) occurring frequently in a transaction database (<strong>frequent
itemsets</strong>). A frequent itemset is an itemset appearing in at
least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a parameter given by the user. </p>
  <p>For example, if<strong> H-Mine</strong> is run on the previous
transaction database with a minsup of 40 % (2 transactions),<strong>
H-Mine </strong>produces the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote> In the results, each itemset is annotated with its
support. The <strong>support</strong> of an itemset is how many times
the itemset appears in the transaction database. For example, the
itemset {2, 3 5} has a support of 3 because it appears in transactions
t2, t3 and t5. It is a frequent itemset because its support is higher
or equal to the <em>minsup </em>parameter.</blockquote>

<p>Performance</p>

<blockquote>
  <p>There exists several algorithms for mining frequent itemsets.
H-Mine is claimed to be one of the best by their author. The
implementation offered in SPMF is well-optimized. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by H-Mine is defined
as follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Where can I get more information about the H-Mine algorithm?</p>

<blockquote>
  <p>Here is an article describing the H-Mine algorithm:</p>
  <p><em>J. Pei, J. Han, H. Lu, S. Nishio, S. Tang, and D. Yang. "<a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/Hmine.pdf">H-Mine:
Fast and space-preserving frequent pattern mining in large databases</a>".
IIE Transactions, Volume 39, Issue 6, pages 593-605, June 2007, Taylor
&amp; Francis.</em></p>
</blockquote>

<blockquote> </blockquote>

<h3><strong><span class="centered"><a name="FIN" id="hmine2"> </a></span></strong>Example 7 : Mining Frequent Itemsets by Using the FIN Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FIN</span>"</strong> algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9">FIN</span></strong> contextPasquier99.txt output.txt 0.4</span>
in a folder containing <span class="Style2">spmf.jar</span> and the
example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFIN.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is FIN?</p>

<blockquote>
  <p>FIN is a very recent algorithm (2014)for discovering frequent
itemsets in transaction databases, proposed by Deng et al. (2014). It
is very fast. </p>
  <p>This implementation is very faithful to the original. It was
converted from the original C++ source code provided by Deng et al, and
only contains some minor modifications.</p>
</blockquote>

<p>What is the input of the FIN algorithm?</p>

<blockquote>
  <p>The input of FIN is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the FIN algorithm?</p>

<blockquote>
  <p><strong>FIN</strong> is an algorithm for discovering itemsets
(group of items) occurring frequently in a transaction database (<strong>frequent
itemsets</strong>). A frequent itemset is an itemset appearing in at
least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a parameter given by the user. </p>
  <p>For example, if<strong> FIN</strong> is run on the previous
transaction database with a minsup of 40 % (2 transactions),<strong> FIN</strong>produces
the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote> In the results, each itemset is annotated with its
support. The <strong>support</strong> of an itemset is how many times
the itemset appears in the transaction database. For example, the
itemset {2, 3 5} has a support of 3 because it appears in transactions
t2, t3 and t5. It is a frequent itemset because its support is higher
or equal to the <em>minsup </em>parameter.</blockquote>

<p>Performance</p>

<blockquote>
  <p>There exists several algorithms for mining frequent itemsets. FIN
is claimed to be one of the best, and is certainly one of the top
algorithms available in SPMF. The implementation is well optimized and
faithful to the original version (it was converted from C++ to Java
with only minor modifications).</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by FIN is defined as
follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Where can I get more information about the FIN algorithm?</p>

<blockquote>
  <p>Here is an article describing the FIN algorithm:</p>
  <p><em>Zhi-Hong Deng, Sheng-Long Lv: <a href="http://www.philippe-fournier-viger.com/spmf/FIN.pdf" rel="nofollow">Fast
mining frequent itemsets using Nodesets.</a> Expert Syst. Appl. 41(10):
4505-4512 (2014)</em></p>
</blockquote>

<blockquote> </blockquote>

<h3><strong><span class="centered"><a name="PrePost" id="hmine3"> </a></span></strong>Example 8 : Mining Frequent Itemsets by Using the PrePost /
PrePost+ Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">PrePost</span>"</strong> or <strong>"<span class="Style9">PrePost</span>+"</strong> algorithm<strong>, </strong>
(2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9">PrePost</span></strong> contextPasquier99.txt
output.txt 0.4</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>
    <br>
or <br>
    <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9">PrePost+</span></strong> contextPasquier99.txt
output.txt 0.4</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>
  </li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestPrePost.java"</span></strong>
or <strong><span class="Style2">"MainTestPrePost+.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is PrePost / PrePost+?</p>

<blockquote>
  <p>PrePost is a very recent algorithm (2012) for discovering frequent
itemsets in transaction databases, proposed by Deng et al. (2012). </p>
  <p>PrePost+ is a variation designed by Deng et al. (2015). It is
reported to be faster than PrePost. Both implementations are offered in
SPMF.</p>
  <p>These implementations are faithful to the original. They were
converted from the original C++ source code provided by Deng et al, and
only contains some minor modifications.</p>
</blockquote>

<p>What is the input of the PrePost / PrePost+ algorithms?</p>

<blockquote>
  <p>The input of <strong>PrePost</strong> and <strong>PrePost+ </strong>is
a <strong>transaction database</strong> (aka binary context) and a
threshold named <em><strong>minsup</strong></em> (a value between 0
and 100 %). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the PrePost / PrePost+ algorithms?</p>

<blockquote>
  <p><strong>PrePost</strong> and <strong>PrePost+ </strong>are
algorithms for discovering itemsets (group of items) occurring
frequently in a transaction database (<strong>frequent itemsets</strong>).
A frequent itemset is an itemset appearing in at least <em>minsup </em>transactions
from the transaction database, where <em>minsup </em>is a parameter
given by the user. </p>
  <p>For example, if<strong> PrePost</strong> or <strong>PrePost+ </strong>are
run on the previous transaction database with a minsup of 40 % (2
transactions), they produce the following result:</p>
</blockquote>

<table align="center" border="1" width="199">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote> In the results, each itemset is annotated with its
support. The <strong>support</strong> of an itemset is how many times
the itemset appears in the transaction database. For example, the
itemset {2, 3 5} has a support of 3 because it appears in transactions
t2, t3 and t5. It is a frequent itemset because its support is higher
or equal to the <em>minsup </em>parameter.</blockquote>

<p>Performance</p>

<blockquote>
  <p>There exists several algorithms for mining frequent itemsets.
PrePost is claimed to be one of the best by their author. The PrePost+
algorithm by the same authors is supposed to be faster though (also
offered in SPMF).</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by PrePost is
defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
  <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Where can I get more information about the PrePost / PrePost+
algorithm?</p>

<blockquote>
  <p>Here is an article describing the PrePost algorithm:</p>
  <p><em>Zhihong Deng, Zhonghui Wang, Jia-Jian Jiang: <a href="http://www.philippe-fournier-viger.com/spmf/PrePost.pdf" rel="nofollow">A new algorithm for fast mining
frequent itemsets using N-lists</a>. SCIENCE CHINA Information Sciences
55(9): 2008-2030 (2012)</em></p>
  <p>And another describing PrePost+:</p>
  <p><em>Zhihong Deng, Sheng-Dong Lv: <a href="http://www.philippe-fournier-viger.com/spmf/prepost+.pdf" rel="nofollow">PrePost + : An efficient N-lists-based algorithm for
mining frequent itemsets via ChildrenâParent Equivalence pruning</a>.
Expert Systems and Applications, 42: 5424- 5432 (2015)</em></p>
</blockquote>

<blockquote>
  <blockquote> </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="LCMFreq" id="hmine5"> </a></span></strong>Example 9 : Mining Frequent Itemsets by Using the LCMFreq
Algorithm</h3>

<blockquote>
  <p>How to run this example?</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">LCMFreq</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9">LCMFreq</span></strong> contextPasquier99.txt
output.txt 0.4</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestLCMFreq_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
    </li>
  </ul>
  <p>What is LCMFreq?</p>
  <blockquote>
    <p>LCMFreq is an algorithm of the LCM familly of algorithms for
mining frequent itemsets. LCM is the winner of the FIMI 2004
competition. It is supposed to be one of the fastest itemset mining
algorithm. </p>
    <p>In this implementations,we have attempted to replicate LCM v2
used in FIMI 2004. Most of the key features of LCM have been replicated
in this implementation (anytime database reduction, occurrence
delivery, etc.). However, a few optimizations have been left out for
now (transaction merging, removing locally infrequent items). They may
be added in a future version of SPMF.</p>
<!--<p>Three versions of LCM  named  LCMFreq, LCM and LCMMax for respectively mining all frequent itemsets, closed itemsets and maximal itemsets.</p>-->
  </blockquote>
  <p>What is the input of the LCMFreq algorithm?</p>
  <blockquote>
    <p>The input of <strong>LCMFreq</strong> is a <strong>transaction
database</strong> (aka binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
    <table align="center" border="1" width="316">
      <tbody>
        <tr>
          <td width="144"><strong>Transaction id</strong></td>
          <td width="156"><strong>Items</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>{1, 3, 4}</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>{2, 3, 5}</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>{1, 2, 3, 5}</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>{2, 5}</td>
        </tr>
        <tr>
          <td><strong>t5</strong></td>
          <td>{1, 2, 3, 5}</td>
        </tr>
      </tbody>
    </table>
  </blockquote>
  <p>What is the output of the LCMFreq algorithm?</p>
  <blockquote>
    <p><strong>LCMFreq </strong>is an algorithm for discovering
itemsets (group of items) occurring frequently in a transaction
database (<strong>frequent itemsets</strong>). A frequent itemset is an
itemset appearing in at least <em>minsup </em>transactions from the
transaction database, where <em>minsup </em>is a parameter given by
the user. </p>
    <p>For example, if<strong> LCMFreq </strong>is run on the previous
transaction database with a minsup of 40 % (2 transactions),<strong>
LCMFreq </strong>produces the following result:</p>
  </blockquote>
  <table align="center" border="1" width="199">
    <tbody>
      <tr>
        <td width="114"><strong>itemsets</strong></td>
        <td width="69"><strong>support</strong></td>
      </tr>
      <tr>
        <td>{1}</td>
        <td>3</td>
      </tr>
      <tr>
        <td>{2}</td>
        <td>4</td>
      </tr>
      <tr>
        <td>{3}</td>
        <td>4</td>
      </tr>
      <tr>
        <td>{5}</td>
        <td>4</td>
      </tr>
      <tr>
        <td>{1, 2}</td>
        <td>2</td>
      </tr>
      <tr>
        <td>{1, 3}</td>
        <td>3</td>
      </tr>
      <tr>
        <td>{1, 5}</td>
        <td>2</td>
      </tr>
      <tr>
        <td>{2, 3}</td>
        <td>3</td>
      </tr>
      <tr>
        <td>{2, 5}</td>
        <td>4</td>
      </tr>
      <tr>
        <td>{3, 5}</td>
        <td>3</td>
      </tr>
      <tr>
        <td>{1, 2, 3}</td>
        <td>2</td>
      </tr>
      <tr>
        <td>{1, 2, 5}</td>
        <td>2</td>
      </tr>
      <tr>
        <td>{1, 3, 5}</td>
        <td>2</td>
      </tr>
      <tr>
        <td>{2, 3, 5}</td>
        <td>3</td>
      </tr>
      <tr>
        <td>{1, 2, 3, 5}</td>
        <td>2</td>
      </tr>
    </tbody>
  </table>
  <p>How should I interpret the results?</p>
  <blockquote> In the results, each itemset is annotated with its
support. The <strong>support</strong> of an itemset is how many times
the itemset appears in the transaction database. For example, the
itemset {2, 3 5} has a support of 3 because it appears in transactions
t2, t3 and t5. It is a frequent itemset because its support is higher
or equal to the <em>minsup </em>parameter.</blockquote>
  <p>Performance</p>
  <blockquote>
    <p>There exists several algorithms for mining frequent itemsets. <strong>LCMFreq</strong>
is the winner of the FIMI 2004 competition so it is probably one of the
best. In this implementation, we have attempted to replicate v2 of the
algorithm. But some optimizations have been left out (transaction
merging and removing locally infrequent items). The algorithm seems to
perform well on sparse datasets.</p>
  </blockquote>
  <p>Implementation details</p>
  <blockquote>
    <p> In the source code version of SPMF, there are two versions of <strong>LCMFreq.
    </strong>The version "<span class="Style9">MainTestLCMFreq.java</span>"
keeps the result into memory. The version named "<span class="Style9">MainTestLCMFreq_saveToFile.java</span>"
saves the result to a file. In the graphical user interface and command
line interface only the second version is offered.</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong> input file format</strong> is defined as follows.
It is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
    <p>For example, for the previous example, the input file is defined
as follows:</p>
    <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
    <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
  </blockquote>
  <p>Output file format</p>
  <blockquote>
    <p>The <strong>output file format</strong> is defined as follows.
It is a text file, where each line represents a frequent itemset. On
each line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, here is the output file for this example.
The first line indicates the frequent itemset consisting of the item 1
and it indicates that this itemset has a support of 3 transactions.</p>
    <p>1 #SUP: 3<br>
2 #SUP: 4<br>
3 #SUP: 4<br>
5 #SUP: 4<br>
1 2 #SUP: 2<br>
1 3 #SUP: 3<br>
1 5 #SUP: 2<br>
2 3 #SUP: 3<br>
2 5 #SUP: 4<br>
3 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 5 #SUP: 2<br>
1 3 5 #SUP: 2<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
    <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
  </blockquote>
  <p>Where can I get more information about the <strong>LCMFreq</strong>
algorithm?</p>
  <blockquote>
    <p>Here is an article describing the LCM v2 familly of algorithms:</p>
    <p><em>Takeaki Uno, Masashi Kiyomi and Hiroki Arimura (2004). <a href="http://www.philippe-fournier-viger.com/spmf/LCM2.pdf" rel="nofollow">LCM ver. 2: Efficient Mining Algorithms
for Frequent/Closed/Maximal Itemsets</a>. Proc. IEEE ICDM Workshop on
Frequent Itemset Mining Implementations Brighton, UK, November 1, 2004</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="example2" id="example2"> </a></span></strong>
Example 10 : Mining Frequent Closed Itemsets Using the
AprioriClose Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">AprioriClose</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9">AprioriClose</span> </strong> contextPasquier99.txt
output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAprioriClose1.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is AprioriClose?</p>

<blockquote>
  <p><strong>AprioriClose</strong> (aka <strong>Close</strong>) is an
algorithm for discovering <strong> frequent closed itemsets</strong>
in a transaction database. It was proposed by Pasquier et al. (1999). </p>
</blockquote>

<p>What is the input of the AprioriClose algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the AprioriClose algorithm?</p>

<blockquote>
  <p><strong>AprioriClose</strong> outputs <strong>frequent closed
itemsets. </strong> To explain what is a frequent closed itemset, it
is necessary to review a few definitions. </p>
  <p> An itemset is an unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, consider the itemset {1, 3}. It has a support of
3 because it appears in three transactions (t1, t3 and t5) from the
transaction database . </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database,
where <em>minsup </em>is a threshold set by the user. A <strong>frequent
closed itemset </strong>is a frequent itemset that is not included in
a proper superset having exactly the same support. The set of <strong>frequent
closed itemsets</strong> is thus a subset of the set of frequent
itemsets. Why is it interesting to discover frequent closed itemset ?
The reason is that the set of frequent closed itemsets is usually much
smaller than the set of frequent itemsets and it can be shown that no
information is lost (all the frequent itemsets can be regenerated from
the set of frequent closed itemsets - see Pasquier(1999) for more
details).</p>
  <p>If we apply <strong>AprioriClose</strong> on the previous
transaction database with a <em>minsup</em> of 40 % (2 transactions),
we get the following five frequent closed itemsets:</p>
</blockquote>

<table align="center" border="1" width="356">

  <tbody>
    <tr>
      <td width="231"><strong>frequent closed itemsets</strong></td>
      <td width="109"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote> If you would apply the regular Apriori algorithm instead
of AprioriClose, you would get 15 itemsets instead of 5, which shows
that the set of frequent closed itemset can be much smaller than the
set of frequent itemsets.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent closed itemset is annotated with its
support. For example, the itemset {2, 3, 5} has a support of 3 because
it appears in transactions t2, t3 and t5. The itemset {2, 3, 5} is a
frequent itemset because its support is higher or equal to the <em>minsup
  </em>parameter. Furthermore, it is a closed itemsets because it has
no proper superset having exactly the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by AprioriClose is
defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong>frequent closed
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. The second line indicates the frequent
itemset consisting of the item 1 and 3, and it indicates that this
itemset has a support of 4 transactions.</p>
  <p>3 #SUP: 4<br>
1 3 #SUP: 3<br>
2 5 #SUP: 4<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> The <strong>AprioriClose</strong> algorithm is important for
historical reasons because it is the first algorithm for mining
frequent closed itemsets. However, there exists several other
algorithms for mining frequent closed itemsets. In SPMF, it is
recommended to use <strong>DCI_Closed </strong>or <strong>Charm</strong>
instead of <strong>AprioriClose</strong>, because they are more
efficient. </p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In SPMF, there are two versions of <strong>AprioriClose</strong>.
The first version is named "<span class="Style2">AprioriClose</span>"
and is based on the "Apriori" algorithm. The second version is named "<span class="Style2">Apriori_TIDClose</span>" and is based on the AprioriTID
algorithm instead of Apriori (it uses tidsets to calculate support to
reduce the number of database scans). Both version are available in the
graphical user interface of SPMF. In the source code, the files <strong><span class="Style2">"MainTestAprioriClose1.java" </span></strong><span class="Style7">and</span>"<strong><span class="Style2">MainTestAprioriTIDClose.java</span></strong>"
respectively correspond to these two versions.</p>
</blockquote>

<p>Where can I get more information about the AprioriClose algorithm?</p>

<blockquote>
  <p>The following article describes the AprioriClose algorithm:</p>
  <p><em>Nicolas Pasquier, Yves Bastide, Rafik Taouil, Lotfi Lakhal: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/pasquier99.pdf">Discovering
Frequent Closed Itemsets for Association Rules</a>. ICDT 1999: 398-416</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="dciclosed" id="example27"> </a></span></strong>
Example 11 : Mining Frequent Closed Itemsets Using the
DCI_Closed Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">DCI_Closed</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">2 </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> <span class="Style9"> </span></strong> <strong><span class="Style9">DCI_Closed</span></strong>
contextPasquier99.txt output.txt 2</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestDCI_Closed.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is DCI_Closed?</p>

<blockquote>
  <p><strong>DCI_Closed</strong> is an algorithm for discovering <strong>
frequent closed itemsets</strong> in a transaction database. DCI_Closed
was proposed by Lucchese et al. (2004). </p>
</blockquote>

<p>What is the input of the DCI_Closed algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the DCI_Closed algorithm?</p>

<blockquote>
  <p><strong>DCI_Closed</strong> outputs <strong>frequent closed
itemsets. </strong> To explain what is a frequent closed itemset, it
is necessary to review a few definitions. </p>
  <p> An itemset is a unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, consider the itemset {1, 3}. It has a support of
3 because it appears in three transactions (t1, t3, t5) from the
transaction database. </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database.
A <strong>frequent closed itemset </strong>is a frequent itemset that
is not included in a proper superset having exactly the same support.
The set of frequent closed itemsets is thus a subset of the set of
frequent itemsets. Why is it interesting to discover frequent closed
itemsets ? The reason is that the set of frequent closed itemsets is
usually much smaller than the set of frequent itemsets and it can be
shown that no information is lost (all the frequent itemsets can be
regenerated from the set of frequent closed itemsets - see Lucchese
(2004) for more details).</p>
  <p>If we apply <strong>DCI_Closed</strong> on the transaction
database with a minsup of 2 transactions, we get the following result:</p>
</blockquote>

<table align="center" border="1" width="356">

  <tbody>
    <tr>
      <td width="231"><strong>frequent closed itemsets</strong></td>
      <td width="109"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote>If you compare this result with the output of a frequent
itemset mining algorithm like Apriori, you would notice that only 5
closed itemsets are found by <strong>DCI_Closed</strong> instead of
about 15 itemsets by Apriori, which shows that the set of frequent
closed itemset can be much smaller than the set of frequent itemsets.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent closed itemset is annotated with its
support. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.
It is a closed itemsets because it has no proper superset having
exactly the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by DCI_Closed is
defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong>frequent closed
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. The second line indicates the frequent
itemset consisting of the item 1 and 3, and it indicates that this
itemset has a support of 4 transactions.</p>
  <p>3 #SUP: 4<br>
1 3 #SUP: 3<br>
2 5 #SUP: 4<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> The <strong>DCI_Closed</strong> algorithm is one of the fastest
algorithms for frequent closed itemset mining. The version in SPMF is
optimized and very efficient. SPMF also offers other algorithms for
frequent closed itemset mining such as <strong>Charm</strong> and <strong>AprioriClose</strong>.
  <strong>DCI_Closed</strong> and <strong>Charm</strong> are more
efficient than AprioriClose.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In the source code version of SPMF, there are<strong> two versions
of DCI_Closed</strong>. The first one uses HashSet to store the
transaction ids. The second one is an optimized version that uses a bit
matrix to store transactions ids, and also includes additional
optimizations. The first version can be tested by running <strong><span class="Style2">MainTestDCI_Closed.java</span></strong> and the second
version by running <strong><span class="Style2">MainTestDCI_Closed_Optimized.java.</span></strong>
In the release version of SPMF, only the optimized version of <strong>DCI_Closed</strong>
is available in the graphical user interface and command line interface.</p>
</blockquote>

<p>Where can I get more information about the DCI_Closed algorithm?</p>

<blockquote>
  <p>Here is an article describing the DCI_Closed algorithm:</p>
  <p><em>Claudio Lucchese, Salvatore Orlando, Raffaele Perego: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/DCI_Closed.pdf">DCI
Closed: A Fast and Memory Efficient Algorithm to Mine Frequent Closed
Itemsets</a>. FIMI 2004</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="e2" id="e2"> </a></span></strong>
Example 12 : Mining Frequent Closed Itemsets Using the
Charm / dCharm Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Charm_bitset</span></strong>"or<strong>"<span class="Style9">dCharm_bitset</span></strong>" algorithm<strong>, </strong>
(2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command
line, then execute this command: </strong><br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Charm_bitset </span></strong>contextPasquier99.txt
output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF </strong>and
want to respectively launch <strong>Charm</strong> or <strong>dCharm</strong>,
then launch the file <strong><span class="Style2">"MainTestCharm_bitset_saveToMemory.java"</span></strong>
or <strong><span class="Style2">"MainTestDCharm_bitset_saveToMemory.java"</span></strong>in
the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Charm?</p>

<blockquote>
  <p><strong>Charm</strong> is an algorithm for discovering <strong>
frequent closed itemsets</strong> in a transaction database. It was
proposed by Zaki (2002). </p>
  <p><strong>dCharm</strong> is a variation of the Charm algorithm that
is implemented with diffsets rather than tidsets. It has the same
output and input as Charm.</p>
</blockquote>

<p>What is the input of the Charm / dCharm algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the Charm / dCharm algorithm?</p>

<blockquote>
  <p><strong>Charm</strong> outputs <strong>frequent closed itemsets. </strong>
To explain what is a frequent closed itemset, it is necessary to review
a few definitions. </p>
  <p> An itemset is a unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, the itemset {1, 3} has a support of 3 because it
appears in three transactions (t1, t3, t5) from the previous
transaction database. </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database.
A <strong>frequent closed itemset </strong>is a frequent itemset that
is not included in a proper superset having exactly the same support.
The set of frequent closed itemsets is thus a subset of the set of
frequent itemsets. Why is it interesting to discover frequent closed
itemsets ? The reason is that the set of frequent closed itemsets is
usually much smaller than the set of frequent itemsets and it can be
shown that no information is lost by discovering only frequent closed
itemsets (because all the frequent itemsets can be regenerated from the
set of frequent closed itemsets - see Zaki (2002) for more details).</p>
  <p>If we apply <strong>Charm</strong> on the previous transaction
database with a minsup of 40 % (2 transactions), we get the following
result:</p>
</blockquote>

<table align="center" border="1" width="356">

  <tbody>
    <tr>
      <td width="231"><strong>frequent closed itemsets</strong></td>
      <td width="109"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote> If you compare this result with the output from a frequent
itemset mining algorithm like Apriori, you would notice that only 5
closed itemsets are found by Charm instead of about 15 itemsets by
Apriori, which shows that the set of frequent closed itemset can be
much smaller than the set of frequent itemsets.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent closed itemset is annotated with its
support. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.
It is a closed itemset because it has no proper superset having exactly
the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by CHARM is defined
as follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong>frequent closed
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. The second line indicates the frequent
itemset consisting of the item 1 and 3, and it indicates that this
itemset has a support of 4 transactions.</p>
  <p>3 #SUP: 4<br>
1 3 #SUP: 3<br>
2 5 #SUP: 4<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> The <strong>Charm</strong> algorithm is an important algorithm
because it is one of the first depth-first algorithm for mining
frequent closed itemsets. In SPMF, Charm and DCI_Closed are the two
most efficient algorithms for frequent closed itemset mining.</p>
</blockquote>

<p>Where can I get more information about the Charm algorithm?</p>

<blockquote>
  <p>This article describes the <strong>Charm</strong> algorithm:</p>
  <p><em>Mohammed Javeed Zaki, Ching-Jiu Hsiao: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/Charm02.pdf">CHARM:
An Efficient Algorithm for Closed Itemset Mining</a>. SDM 2002.</em></p>
  <p>Here is an article describing the <strong>dCharm</strong>
variation:</p>
  <p><em>Zaki, M.J., Gouda, K.: <a href="http://www.philippe-fournier-viger.com/spmf/dEclat_dCharm.pdf" rel="nofollow">Fast vertical mining using diffsets</a>. Technical
Report 01-1, Computer Science Dept., Rensselaer Polytechnic Institute
(March 2001) 10</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="LCM" id="e4"> </a></span></strong>
Example 13 : Mining Frequent Closed Itemsets Using the
LCM Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">LCM</span></strong>" algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command
line, then execute this command: </strong><br>
    <span class="Style2">java -jar spmf.jar run LCM
contextPasquier99.txt output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF </strong>and
want to launch <strong>LCM</strong> on the file <span class="Style2">contextPasquier99.txt</span>,
then launch the file <strong><span class="Style2">"MainTestLCM_saveToMemory.java"</span></strong>in
the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is LCM?</p>

<blockquote>LCM is an algorithm of the LCM familly of algorithms for
mining frequent closed itemsets. LCM is the winner of the FIMI 2004
competition. It is supposed to be one of the fastest closed itemset
mining algorithm.
  <p>In this implementations,we have attempted to replicate LCM v2 used
in FIMI 2004. Most of the key features of LCM have been replicated in
this implementation (anytime database reduction, occurrence delivery,
etc.). However, a few optimizations have been left out for now
(transaction merging, removing locally infrequent items). They may be
added in a future version of SPMF.</p>
<!--<p>Three versions of LCM are offered in SPMF, named  LCMFreq, LCM and LCMMax for respectively mining all frequent itemsets, closed itemsets and maximal itemsets. </p> -->
</blockquote>

<p>What is the input of the LCM algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the LCM algorithm?</p>

<blockquote>
  <p><strong>LCM</strong> outputs <strong>frequent closed itemsets. </strong>
To explain what is a frequent closed itemset, it is necessary to review
a few definitions. </p>
  <p> An itemset is a unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, the itemset {1, 3} has a support of 3 because it
appears in three transactions (t1, t3, t5) from the previous
transaction database. </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database.
A <strong>frequent closed itemset </strong>is a frequent itemset that
is not included in a proper superset having exactly the same support.
The set of frequent closed itemsets is thus a subset of the set of
frequent itemsets. Why is it interesting to discover frequent closed
itemsets ? The reason is that the set of frequent closed itemsets is
usually much smaller than the set of frequent itemsets and it can be
shown that no information is lost by discovering only frequent closed
itemsets (because all the frequent itemsets can be regenerated from the
set of frequent closed itemsets - see Zaki (2002) for more details).</p>
  <p>If we apply <strong>LCM</strong> on the previous transaction
database with a minsup of 40 % (2 transactions), we get the following
result:</p>
</blockquote>

<table align="center" border="1" width="356">

  <tbody>
    <tr>
      <td width="231"><strong>frequent closed itemsets</strong></td>
      <td width="109"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote> If you compare this result with the output from a frequent
itemset mining algorithm like Apriori, you would notice that only 5
closed itemsets are found by <strong>LCM</strong> instead of about 15
itemsets by Apriori, which shows that the set of frequent closed
itemset can be much smaller than the set of frequent itemsets.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent closed itemset is annotated with its
support. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.
It is a closed itemset because it has no proper superset having exactly
the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by LCM is defined as
follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong>frequent closed
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. The second line indicates the frequent
itemset consisting of the item 1 and 3, and it indicates that this
itemset has a support of 4 transactions.</p>
  <p>3 #SUP: 4<br>
1 3 #SUP: 3<br>
2 5 #SUP: 4<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> There exists several algorithms for mining closed itemsets. LCM
is the winner of the FIMI 2004 competition so it is probably one of the
best. In this implementation, we have attempted to replicate v2 of the
algorithm. But some optimizations have been left out (transaction
merging and removing locally infrequent items). The algorithm seems to
perform very well on sparse datasets. According to some preliminary
experiments, it can be faster than Charm, dCharm and DCI_closed on
sparse datasets, but may perform less well on dense datasets.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p> In the source code version of SPMF, there are two versions of <strong>LCM.
  </strong>The version "<span class="Style9">MainTestLCM.java</span>"
keeps the result into memory. The version named "<span class="Style9">MainTestLCM_saveToFile.java</span>"
saves the result to a file. In the graphical user interface and command
line interface only the second version is offered.</p>
</blockquote>

<p>Where can I get more information about the LCM algorithm?</p>

<blockquote>
  <p>This article describes the <strong>LCM</strong> algorithm:</p>
  <p>Here is an article describing the LCM v2 familly of algorithms:</p>
  <p><em>Takeaki Uno, Masashi Kiyomi and Hiroki Arimura (2004). <a href="http://www.philippe-fournier-viger.com/spmf/LCM2.pdf" rel="nofollow">LCM ver. 2: Efficient Mining Algorithms
for Frequent/Closed/Maximal Itemsets</a>. Proc. IEEE ICDM Workshop on
Frequent Itemset Mining Implementations Brighton, UK, November 1, 2004</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="fpclose" id="e8"> </a></span></strong>
Example 14 : Mining Frequent Closed Itemsets Using the
FPClose Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FPClose</span></strong>"algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command
line, then execute this command: </strong><br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FPClose </span></strong>contextPasquier99.txt
output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF </strong>and
want to launch <strong>FPClose </strong>on the example input file <span class="Style2">contextPasquier99.txt</span>, then launch the file <strong><span class="Style2">"MainTestFPClose_saveToMemory.java"</span> </strong>in
the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is FPClose?</p>

<blockquote>FPClose is an algorithm of the FPGrowth familly of
algorithms, designed for mining frequent closed itemsets. FPClose is
supposed to be one of the fastest closed itemset mining algorithm.
  <p>In this implementations,we have attempted to implement most of the
optimizations proposed in the FPClose paper, except that we did not
implement the triangular matrix from FPGrowth* and the local CFI trees.
These optimizations may be added in a future version of SPMF.</p>
<!--<p>Three versions of LCM are offered in SPMF, named  LCMFreq, LCM and LCMMax for respectively mining all frequent itemsets, closed itemsets and maximal itemsets. </p> -->
</blockquote>

<p>What is the input of the FPClose algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the FPClose algorithm?</p>

<blockquote>
  <p><strong>FPClose</strong> outputs <strong>frequent closed
itemsets. </strong> To explain what is a frequent closed itemset, it
is necessary to review a few definitions. </p>
  <p> An itemset is a unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, the itemset {1, 3} has a support of 3 because it
appears in three transactions (t1, t3, t5) from the previous
transaction database. </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database.
A <strong>frequent closed itemset </strong>is a frequent itemset that
is not included in a proper superset having exactly the same support.
The set of frequent closed itemsets is thus a subset of the set of
frequent itemsets. Why is it interesting to discover frequent closed
itemsets ? The reason is that the set of frequent closed itemsets is
usually much smaller than the set of frequent itemsets and it can be
shown that no information is lost by discovering only frequent closed
itemsets (because all the frequent itemsets can be regenerated from the
set of frequent closed itemsets - see Zaki (2002) for more details).</p>
  <p>If we apply <strong>FPClose</strong> on the previous transaction
database with a minsup of 40 % (2 transactions), we get the following
result:</p>
</blockquote>

<table align="center" border="1" width="356">

  <tbody>
    <tr>
      <td width="231"><strong>frequent closed itemsets</strong></td>
      <td width="109"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote> If you compare this result with the output from a frequent
itemset mining algorithm like Apriori, you would notice that only 5
closed itemsets are found by <strong>FPClose</strong> instead of about
15 itemsets by Apriori, which shows that the set of frequent closed
itemset can be much smaller than the set of frequent itemsets.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent closed itemset is annotated with its
support. For example, the itemset {2, 3 5} has a support of 3 because
it appears in transactions t2, t3 and t5. It is a frequent itemset
because its support is higher or equal to the <em>minsup </em>parameter.
It is a closed itemset because it has no proper superset having exactly
the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by FPClose is
defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong>frequent closed
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. The second line indicates the frequent
itemset consisting of the item 1 and 3, and it indicates that this
itemset has a support of 4 transactions.</p>
  <p>3 #SUP: 4<br>
1 3 #SUP: 3<br>
2 5 #SUP: 4<br>
2 3 5 #SUP: 3<br>
1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> There exists several algorithms for mining closed itemsets.
FPClose is one of the fastest in the FIMI 2004 competition so it is
probably one of the best. In this implementation, we have attempted
most of the optimizations. But some optimizations have been left out
(local CFI trees and the triangular matrix of FPGrowth*). The algorithm
seems to perform very well.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p> In the source code version of SPMF, there are two versions of <strong>FPClose.
  </strong>The version "<span class="Style9">MainTestFPClose_saveToMemory.java</span>"
keeps the result into memory. The version named "<span class="Style9">MainTestFPClose_saveToFile.java</span>"
saves the result to a file. In the graphical user interface and command
line interface only the second version is offered.</p>
</blockquote>

<p>Where can I get more information about the FPClose algorithm? </p>

<blockquote>
  <p>This article describes the <strong>FPClose</strong> algorithm: </p>
  <p><em>Grahne, G., &amp; Zhu, J. (2005). Fast algorithms for frequent
itemset mining using fp-trees. Knowledge and Data Engineering, IEEE
Transactions on, 17(10), 1347-1362. </em></p>
</blockquote>

<h3> <strong><span class="centered"><a name="fpmax" id="example55"> </a></span>Example 15 : Mining Frequent Maximal Itemsets by Using the
FPMax Algorithm </strong></h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FPMax</span></strong>"
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run FPMax
contextPasquier99.txt output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestFPMax.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is FPMax?</p>

<blockquote>
  <p><strong>FPMax</strong> is an algorithm for discovering <strong>
frequent maximal itemsets</strong> in a transaction database. <br>
  <br>
  <strong>FPMax </strong>is based on the famous FPGrowth algorithm and
includes several strategies for mining maximal itemsets efficiently
while pruning the search space.</p>
</blockquote>

<p>What is the input of the FPMax algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the FPMax algorithm?</p>

<blockquote>
  <p><strong>FPMax</strong> outputs <strong>frequent maximal itemsets.
  </strong>To explain what is a frequent maximal itemset, it is
necessary to review a few definitions. </p>
  <p> An itemset is a unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, the itemset {1, 3} has a support of 3 because it
appears in three transactions (t1,t3, t5) from the previous transaction
database. </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database.
A <strong>frequent closed itemset </strong>is a frequent itemset that
is not included in a proper superset having the same support. A <strong>frequent
maximal itemset </strong>is a frequent itemset that is not included in
a proper superset that is a frequent itemset. The set of frequent
maximal itemsets is thus a subset of the set of frequent closed
itemsets, which is a subset of frequent itemsets. Why it is interesting
to discover frequent maximal itemsets ? The reason is that the set of
frequent maximal itemsets is usually much smaller than the set of
frequent itemsets and also smaller than the set of frequent closed
itemsets. However, unlike frequent closed itemsets, frequent maximal
itemsets are not a lossless representation of the set of frequent
itemsets (it is possible to regenerate all frequent itemsets from the
set of frequent maximal itemsets but it would not be possible to get
their support without scanning the database).</p>
  <p>If we apply <strong>FPMax </strong>on the previous transaction
database with a minsup of 40 % (2 transactions), we get the following
result:</p>
</blockquote>

<table align="center" border="1" width="400">

  <tbody>
    <tr>
      <td width="152"><strong>frequent maximal itemsets</strong></td>
      <td width="72"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote> This itemset is the only maximal itemsets itemsets and it
has a support of 2 because it appears in two transactions.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent maximum itemset is annotated with
its support. For example, the itemset {1, 2, 3 5} is a maximal itemset
having a support of 2 because it appears in transactions t3 and t5. The
itemset {2, 5} has a support of 4 and is not a maximal itemset because
it is included in {2, 3, 5}, which is a frequent itemset.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by <strong>FPMax</strong>
is defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong> maximal itemset</strong>.
On each line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, we show below the output file for this
example consisting of a single line. The only line here indicates the <strong>maximal
itemset </strong>consisting of the item 1, item 2, item 3 and item 5.
This lines indicates that this itemset has a support of 2 transactions.</p>
  <p> 1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The <strong>FPMax </strong> algorithm is a very efficient
algorithm for maximal itemset mining. I have tried to implement all the
optimizations in the paper and optimize the implementation. However, it
may be possible to still optimize it a little bit. </p>
</blockquote>

<p>Where can I get more information about the FPMax algorithm?</p>

<blockquote>
  <p>The FPMax algorithm is described in this thesis (in French
language only):</p>
  <p><em>Grahne, G., &amp; Zhu, J. (2003, May). High performance mining
of maximal frequent itemsets. In 6th International Workshop on
High Performance Data Mining.</em></p>
</blockquote>

<h3><a name="e3" id="e3"> </a> Example 16 : Mining
Frequent Maximal Itemsets Using the Charm-MFI Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Charm_MFI</span></strong>"
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">Charm_MFI</span></strong> contextPasquier99.txt
output.txt 40%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestCharmMFI.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is Charm-MFI?</p>

<blockquote>
  <p><strong>Charm-MFI</strong> is an algorithm for discovering <strong>
frequent maximal itemsets</strong> in a transaction database. <br>
  <br>
  <strong>Charm-MFI</strong> is not an efficient algorithm because it
discovers maximal itemsets by performing post-processing after
discovering frequent closed itemsets with the Charm algorithm (hence
the name: Charm-MFI). A more efficient algorithm for mining maximal
itemsets named FPMax is provided in SPMF.</p>
  <p>Moreover, note that the original Charm-MFI algorithm is not
correct. In SPMF, it has been fixed so that it generates the correct
result. </p>
</blockquote>

<p>What is the input of the Charm-MFI algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the Charm-MFI algorithm?</p>

<blockquote>
  <p><strong>Charm-MFI</strong> outputs <strong>frequent maximal
itemsets. </strong>To explain what is a frequent maximal itemset, it
is necessary to review a few definitions. </p>
  <p> An itemset is a unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset. For example, the itemset {1, 3} has a support of 3 because it
appears in three transactions (t1,t3, t5) from the previous transaction
database. </p>
  <p>A <strong>frequent itemset</strong> is an itemset that appears in
at least <em>minsup </em>transactions from the transaction database.
A <strong>frequent closed itemset </strong>is a frequent itemset that
is not included in a proper superset having the same support. A <strong>frequent
maximal itemset </strong>is a frequent itemset that is not included in
a proper superset that is a frequent itemset. The set of frequent
maximal itemsets is thus a subset of the set of frequent closed
itemsets, which is a subset of frequent itemsets. Why it is interesting
to discover frequent maximal itemsets ? The reason is that the set of
frequent maximal itemsets is usually much smaller than the set of
frequent itemsets and also smaller than the set of frequent closed
itemsets. However, unlike frequent closed itemsets, frequent maximal
itemsets are not a lossless representation of the set of frequent
itemsets (it is possible to regenerate all frequent itemsets from the
set of frequent maximal itemsets but it would not be possible to get
their support without scanning the database).</p>
  <p>If we apply <strong>Charm</strong>-<strong>MFI</strong> on the
previous transaction database with a minsup of 40 % (2 transactions),
we get the following result:</p>
</blockquote>

<table align="center" border="1" width="400">

  <tbody>
    <tr>
      <td width="152"><strong>frequent maximal itemsets</strong></td>
      <td width="72"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote> This itemset is the only maximal itemsets itemsets and it
has a support of 2 because it appears in two transactions.</blockquote>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, each frequent maximum itemset is annotated with
its support. For example, the itemset {1, 2, 3 5} is a maximal itemset
having a support of 2 because it appears in transactions t3 and t5. The
itemset {2, 5} has a support of 4 and is not a maximal itemset because
it is included in {2, 3, 5}, which is a frequent itemset.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by <strong>CHARM-MFI</strong>
is defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a <strong> maximal itemset</strong>.
On each line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For example, we show below the output file for this
example consisting of a single line. The only line here indicates the <strong>maximal
itemset </strong>consisting of the item 1, item 2, item 3 and item 5.
This lines indicates that this itemset has a support of 2 transactions.</p>
  <p> 1 2 3 5 #SUP: 2 </p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The <strong>Charm-MFI</strong> algorithm is not a very efficient
algorithm because it finds frequent maximal itemsets by post-processing
instead of finding them directly. </p>
  <p>A more efficient algorithm for mining maximal itemsets named <strong>FPMax</strong>
is provided in SPMF.</p>
</blockquote>

<p>Where can I get more information about the Charm-MFI algorithm?</p>

<blockquote>
  <p>The Charm-MFI algorithm is described in this thesis (in French
language only):</p>
  <p><em>L. Szathmary (2006). Symbolic Data Mining Methods with the
Coron Platform.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="defme" id="example41"> </a></span></strong>
Example 17 : Mining Frequent Generator Itemsets Using
the DefMe Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">DefMe</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextZart.txt"</span></strong>, (3) set the output
file name (e.g. "<span class="Style2">output.txt</span>") (4) set
minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">DefMe</span></strong> contextZart.txt output.txt 40%</span>
in a folder containing <span class="Style2">spmf.jar</span> and the
example input file <span class="Style2">contextZart.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestDefMe_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>DefMe</strong>?</p>

<blockquote>
  <p><strong>DefMe</strong> is an algorithm proposed at PAKDD 2014 for
discovering <strong> minimal patterns in set systems. </strong>If it
is applied to itemset mining, it will discover <strong>frequent
itemset generator</strong>. In SPMF, we have implemented it for this
purpose.<br>
  <br>
  <strong>DefMe</strong> is the our knowledge the only real depth-first
search algorithm for mining<strong> generator itemsets (it does not
need to use a hash table or store candidates).</strong> It is
interesting to have a depth-first search algorithm since depth-first
search algorithm are generally faster than Apriori-based algorithms.</p>
  <p>Another important point about <strong>DefMe</strong> is that
unlike <strong>Pascal</strong>, <strong>DefMe </strong>only find
frequent generator itemsets rather than generating all frequent
itemsets and identifying which one are generators.</p>
</blockquote>

<p>What is the input of the DefMe algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextZart.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the DefMe algorithm?</p>

<blockquote>
  <p>The output of the <strong>DefMe</strong> algorithm for a
transaction database and a minimum support threshold <em>minsup </em>is
the set of all frequent itemsets and their support, and a flag
indicating which itemsets is a generator.</p>
  <p>To explain what is a frequent itemset and a <strong>generator</strong>,
it is necessary to review a few definitions. </p>
  <p> An <strong>itemset</strong> is a unordered set of distinct
items. The <strong>support of an itemset</strong> is the number of
transactions that contain the itemset. For example, the itemset {1, 3}
has a support of 3 because it appears in three transactions from the
database (t2, t3 and t5). A <strong>frequent itemset</strong> is an
itemset that appears in at least <em>minsup </em>transactions from
the transaction database. A <strong>generator</strong> is an itemset X
such that there does not exist an itemset Y strictly included in X that
has the same support.</p>
  <p>By running <strong>DefMe</strong> with the previous transaction
database and a minsup of 40% (2 transactions), we obtain the following
result:</p>
</blockquote>

<table align="center" border="1" width="404">

  <tbody>
    <tr>
      <td><strong>itemsets</strong></td>
      <td><strong>support</strong></td>
    </tr>
    <tr>
      <td>{}</td>
      <td>5</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, for each generator itemset found, its support is
indicated. For example, the itemset {1,2,3} has a support of 2 because
it appears in 2 transactions (t3 and t5).</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by <strong>DefMe </strong>
is defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. For instance, the first line indicates that the empty set
is a generator having a support of 5 transactions. The second line
indicates that the itemset {1} has a support of 4 transactions.</p>
  <p>#SUP: 5<br>
1 #SUP: 4<br>
1 2 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 3 #SUP: 3<br>
1 3 5 #SUP: 2<br>
1 5 #SUP: 3<br>
2 #SUP: 4<br>
2 3 #SUP: 3<br>
3 #SUP: 4<br>
3 5 #SUP: 3<br>
5 #SUP: 4</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The <strong>DefMe</strong> algorithm should be more efficient
than Apriori-based algorithm such as Zart or Pascal. However, no
performance comparison has been done by the authors of DefMe. </p>
</blockquote>

<p>Where can I get more information about the Pascal algorithm?</p>

<blockquote>
  <p>The <strong>DefMe</strong> algorithm is described in this paper:</p>
  <p><em>Arnaud Soulet, FranÃ§ois Rioult (2014). <a href="http://www.philippe-fournier-viger.com/spmf/defme.pdf" rel="nofollow">Efficiently Depth-First Minimal Pattern Mining</a>.
PAKDD (1) 2014: 28-39</em></p>
</blockquote>

<blockquote> </blockquote>

<h3><strong><span class="centered"><a name="pascal" id="example15"> </a></span></strong>
Example 18 : Mining Frequent Closed Itemsets and
Identify Generators Using the Pascal Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Pascal</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextZart.txt"</span></strong>, (3) set the output
file name (e.g. "<span class="Style2">output.txt</span>") (4) set
minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Pascal </span></strong>contextZart.txt output.txt 40%</span>
in a folder containing <span class="Style2">spmf.jar</span> and the
example input file <span class="Style2">contextZart.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestPascal.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Pascal?</p>

<blockquote>
  <p><strong>Pascal</strong> is an algorithm for discovering <strong>
frequent itemsets </strong>and at the same time identify which ones
are <strong>generators</strong> in a transaction database. <br>
  <br>
  <strong>Pascal</strong> is an Apriori-based algorithm. It uses a
special pruning property that can avoid counting the support of some
candidate itemsets. This property is based on the fact that if an
itemset of size k is not a generator, then its support is the support
of the minimum support of its subsets of size k-1.</p>
</blockquote>

<p>What is the input of the Pascal algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextZart.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the Pascal algorithm?</p>

<blockquote>
  <p>The output of the <strong>Pascal</strong> algorithm for a
transaction database and a minimum support threshold <em>minsup </em>is
the set of all frequent itemsets and their support, and a flag
indicating which itemsets is a generator.</p>
  <p>To explain what is a frequent itemset and a generator, it is
necessary to review a few definitions. </p>
  <p> An <strong>itemset</strong> is a unordered set of distinct
items. The <strong>support of an itemset</strong> is the number of
transactions that contain the itemset. For example, the itemset {1, 3}
has a support of 3 because it appears in three transactions from the
database (t2, t3 and t5). A <strong>frequent itemset</strong> is an
itemset that appears in at least <em>minsup </em>transactions from
the transaction database. A <strong>generator</strong> is an itemset X
such that there does not exist an itemset Y strictly included in X that
has the same support.</p>
  <p>By running <strong>Pascal</strong> with the previous transaction
database and a minsup of 40% (2 transactions), we obtain the following
result:</p>
</blockquote>

<table align="center" border="1" width="404">

  <tbody>
    <tr>
      <td><strong>itemsets</strong></td>
      <td><strong>is a generator?</strong></td>
      <td><strong>support</strong></td>
    </tr>
    <tr>
      <td>{}</td>
      <td><strong>yes</strong></td>
      <td>5</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td><strong>yes</strong></td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td><strong>yes</strong></td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td><strong>yes</strong></td>
      <td>4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td><strong>yes</strong></td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td><strong>yes</strong></td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td><strong>yes</strong></td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td><strong>yes</strong></td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td><strong>yes</strong></td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td><strong>yes</strong></td>
      <td>4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td><strong>yes</strong></td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td><strong>yes</strong></td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>no</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td><strong>yes</strong></td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>no</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>no</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, all frequent itemsets are shown. Each frequent
itemset that is a generator is marked as such ("yes"). For each
itemset, its support is indicated. For example, the itemset {1,2,3,5}
has a support of 2 because it appears in 2 transactions (t3 and t5) and
it is not a generator because it has a subset {1,2,3} that has the same
support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by <strong>Pascal </strong>
is defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents a frequent itemset. On each
line, the items of the itemset are first listed. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the keyword "#SUP:" appears, which is followed by an
integer indicating the support of the itemset, expressed as a number of
transactions. After, all the items, the keyword "#IS_GENERATOR:"
appears, which is followed by a boolean indicating if the itemset is a
generator (true) or not (false). For example, here is the output file
for this example. The first line indicates the frequent itemset
consisting of the item 1 and it indicates that this itemset has a
support of 3 transactions and is a generator.</p>
  <p>1 #SUP: 0 #IS_GENERATOR true<br>
2 #SUP: 0 #IS_GENERATOR true<br>
3 #SUP: 0 #IS_GENERATOR true<br>
5 #SUP: 0 #IS_GENERATOR true<br>
1 2 #SUP: 2 #IS_GENERATOR true<br>
1 3 #SUP: 3 #IS_GENERATOR true<br>
1 5 #SUP: 2 #IS_GENERATOR true<br>
2 3 #SUP: 3 #IS_GENERATOR true<br>
2 5 #SUP: 4 #IS_GENERATOR true<br>
3 5 #SUP: 3 #IS_GENERATOR true<br>
1 2 3 #SUP: 2 #IS_GENERATOR false<br>
1 2 5 #SUP: 2 #IS_GENERATOR false<br>
1 3 5 #SUP: 2 #IS_GENERATOR false<br>
2 3 5 #SUP: 3 #IS_GENERATOR false<br>
1 2 3 5 #SUP: 2 #IS_GENERATOR false</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The <strong>Pascal</strong> algorithm should be more or less as
efficient as Apriori since it is an Apriori-based algorithm. Pascal
utilizes a pruning strategies that is supposed to make it faster by
avoiding counting the support of some candidates. But to see really
which one is better, experiments would need to be done to compare it.</p>
</blockquote>

<p>Where can I get more information about the Pascal algorithm?</p>

<blockquote>
  <p>The <strong>Pascal</strong> algorithm is described in this paper:</p>
  <p>Bastide, Y., Taouil, R., Pasquier, N., Stumme, G., &amp; Lakhal,
L. (2000). <a href="http://www.philippe-fournier-viger.com/spmf/pascal2000.pdf" rel="nofollow">Mining frequent
patterns with counting inference</a>. <em>ACM SIGKDD Explorations
Newsletter</em>, <em>2</em>(2), 66-75.</p>
</blockquote>

<h3><strong><span class="centered"><a name="zart" id="example3"> </a></span></strong>
Example 19 : Mining Frequent Closed Itemsets and Minimal
Generators Using the Zart Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Zart</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextZart.txt"</span></strong>, (3) set the output
file name (e.g. "<span class="Style2">output.txt</span>") (4) set
minsup to <span class="Style2">40% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Zart </span></strong>contextZart.txt output.txt 40%</span>
in a folder containing <span class="Style2">spmf.jar</span> and the
example input file <span class="Style2">contextZart.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestZart.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Zart?</p>

<blockquote>
  <p><strong>Zart</strong> is an algorithm for discovering <strong>
frequent closed itemsets and their corresponding generators </strong>in
a transaction database. <br>
  <br>
Zart is an Apriori-based algorithm. Why is it useful to discover closed
itemsets and their generators at the same time? One reason is that this
information is necessary to generate some special kind of association
rules such as the IGB basis of association rules (see the example for
IGB for more information about IGB association rules).</p>
</blockquote>

<p>What is the input of the Zart algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextZart.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the Zart algorithm?</p>

<blockquote>
  <p>The output of the <strong>Zart</strong> algorithm for a
transaction database and a minimum support threshold <em>minsup </em>is
the set of all frequent closed itemsets and their support, and the
associated generator(s) for each closed frequent itemset.</p>
  <p>To explain what is a frequent closed itemset and a generator, it
is necessary to review a few definitions. </p>
  <p> An <strong>itemset</strong> is a unordered set of distinct
items. The <strong>support of an itemset</strong> is the number of
transactions that contain the itemset. For example, the itemset {1, 3}
has a support of 3 because it appears in three transactions from the
database (t2, t3 and t5). A <strong>frequent itemset</strong> is an
itemset that appears in at least <em>minsup </em>transactions from
the transaction database. A <strong>frequent closed itemset </strong>is
a frequent itemset that is not included in a proper superset having the
same support. A generator Y of a closed itemset X is an itemset such
that (1) it has the same support as X and (2) it does not have any
subset having the same support.</p>
  <p>By running <strong>Zart</strong> with the previous transaction
database and a minsup of 40% (2 transactions), we obtain the following
result:</p>
</blockquote>

<table align="center" border="1" width="404">

  <tbody>
    <tr>
      <td><strong>itemsets</strong></td>
      <td><strong>support</strong></td>
      <td><strong>is closed?</strong></td>
      <td><strong>minimal generators</strong></td>
    </tr>
    <tr>
      <td>{}</td>
      <td>5</td>
      <td><strong>yes</strong></td>
      <td>{}</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>4</td>
      <td><strong>yes</strong></td>
      <td>{1}</td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>4</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
      <td><strong>yes</strong></td>
      <td>{3}</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>4</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>3</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
      <td><strong>yes</strong></td>
      <td>{1,3}</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>3</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
      <td><strong>yes</strong></td>
      <td>{2}, {5}</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>3</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>2</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>3</td>
      <td><strong>yes</strong></td>
      <td>{1, 2}, {1, 5}</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>2</td>
      <td colspan="2">no</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
      <td><strong>yes</strong></td>
      <td>{2, 3}, {3, 5}</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
      <td><strong>yes</strong></td>
      <td>{1, 2, 3}, {1, 3, 5}</td>
    </tr>
  </tbody>
</table>

<p>How should I interpret the results?</p>

<blockquote>
  <p>In the results, all frequent itemsets are shown. Each frequent
itemset that is a closed itemset is marked as such ("yes"). For each
closed itemset, its support is indicated and its list of generators.
For example, the itemset {1,2,3,5} has a support of 2 because it
appears in 2 transactions (t3 and t5). It is a closed itemset because
it has no proper superset having the same support. Moreover is has two
generators: {1, 2, 3} and {1, 3, 5}. By definition, these generators
have the same support as {1, 2, 3, 5}.</p>
  <p>Another example. The itemset {1, 3, 5} is not closed and it has a
support of 2. It is not closed because it has a proper superset {1, 2,
3, 5} that has the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by <strong>Zart </strong>
is defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, containing two sections. </p>
  <p>The first section starts with "======= List of closed itemsets and
their generators ============" on the first line of the file. Then,
each closed itemset is indicated on a single line as follows. A closed
itemset is represented by a line starting with "CLOSED :" followed by
the itemset itself and then the support of the itemset. An itemset is
represented by a list of integers, where each integer represents an
item and where integers (items) are separated by single spaces. The
support of a closed itemset is indicated by an integer immediately
following the special keyword "#SUP:" on the same line. The support is
expressed as a number of transactions. On the lines immediately
following a closed itemset, the keyword "GENERATORS :" appears. Then,
on the immediately following line, the generators of the itemsets are
listed, one per line. A generator is represented by the keyword "="
followed by the itemset representing the generator. If a generator is
the empty set, then it is represented by the keyword EMPTYSET.</p>
  <p>The second sections starts with "======= List of frequent itemsets
============" on a single line. Then all frequent itemsets are listed
on the following lines, one per line. On each line, the keyword
"ITEMSET :" appears followed by the items of the itemset. Each item is
represented by an integer and it is followed by a single space. After,
all the items, the special keyword "#SUP:" appears, which is followed
by an integer indicating the support of the itemset, expressed as a
number of transactions. </p>
  <p>For example, we show below the output file for the previous
example.</p>
  <pre>======= List of closed itemsets and their generators ============<br> CLOSED : <br>   EMPTYSET #SUP: 5<br>   GENERATOR(S) :<br>    EMPTYSET<br> CLOSED : <br>   1  #SUP: 4<br>   GENERATOR(S) :<br>    1 <br> CLOSED : <br>   3  #SUP: 4<br>   GENERATOR(S) :<br>    3 <br> CLOSED : <br>   1 3  #SUP: 3<br>   GENERATOR(S) :<br>    1 3 <br> CLOSED : <br>   2 5  #SUP: 4<br>   GENERATOR(S) :<br>     2 <br>     5 <br> CLOSED : <br>   1 2 5  #SUP: 3<br>   GENERATOR(S) :<br>     1 2 <br>     1 5 <br> CLOSED : <br>   2 3 5  #SUP: 3<br>   GENERATOR(S) :<br>     2 3 <br>     3 5 <br> CLOSED : <br>   1 2 3 5  #SUP: 2<br>   GENERATOR(S) :<br>     1 2 3 <br>     1 3 5 <br>======= List of frequent itemsets ============<br> ITEMSET : EMPTYSET #SUP: 5<br> ITEMSET : 1  #SUP: 4<br> ITEMSET : 2  #SUP: 4<br> ITEMSET : 3  #SUP: 4<br> ITEMSET : 5  #SUP: 4<br> ITEMSET : 1 2  #SUP: 3<br> ITEMSET : 1 3  #SUP: 3<br> ITEMSET : 2 3  #SUP: 3<br> ITEMSET : 1 5  #SUP: 3<br> ITEMSET : 2 5  #SUP: 4<br> ITEMSET : 3 5  #SUP: 3<br> ITEMSET : 1 2 3  #SUP: 2<br> ITEMSET : 1 2 5  #SUP: 3<br> ITEMSET : 1 3 5  #SUP: 2<br> ITEMSET : 2 3 5  #SUP: 3<br> ITEMSET : 1 2 3 5  #SUP: 2<br></pre>
  <p>In this example, the first lines of the first section indicates
that the empty set is a closed itemset with a support of 5 and that it
is the generator of itself. The following lines indicates that the
itemset {3} is closed, has a support of 4 and is the generator of
itself. The following lines indicates that the itemset {1, 3} is
closed, has a support of 3 and that the itemset {1} is the only
generator for that itemset. The following lines of this section
indicates in the same way the remaining closed itemsets and their
associated generators.</p>
  <p>In the same example, the first lines of the second sections
indicates that the empty set is a frequent itemset with a support of 5
transactions, that the itemset 1 is frequent with a support of 3
transactions and that the itemset {2} is frequent with a support of 4
transactions. In the same way, the following lines indicates all the
other frequent itemsets.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p> In the source code version of SPMF, there are two versions of <strong>Zart.
  </strong>The version "<span class="Style9">MainTestZart.java</span>"
keeps the result into memory. The version named "<span class="Style9">MainTestZart_saveToFile.java</span>"
saves the result to a file. In the graphical user interface and command
line interface only the second version is offered.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The <strong>Zart</strong> algorithm is not a very efficient
algorithm because it is based on Apriori. If someone only want to
discover closed itemsets and do not need the information about
generators, then he should instead use <strong>DCI_Closed or</strong> <strong>Charm</strong>,
which are more efficient for closed itemset mining. However, in some
cases it is desirable to discover closed itemset and their
corresponding generators (for example to generate IGB association
rules). For these cases, Zart is an appropriate algorithm.</p>
</blockquote>

<p>Where can I get more information about the Zart algorithm?</p>

<blockquote>
  <p>The <strong>Zart</strong> algorithm is described in this paper:</p>
  <p><em>L. Szathmary, A. Napoli, S. O. Kuznetsov. <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/zart.pdf">ZART: A
Multifunctional Itemset Mining Algorithm</a>. Laszlo Szathmary, Amedeo
Napoli, Sergei O. Kuznetsov In: CLA, 2007.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example15" id="example17"> </a></span></strong>
Example 20 : Mining Minimal Rare Itemsets</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">AprioriRare</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextZart.txt"</span></strong>, (3) set the output
file name (e.g. "<span class="Style2">output.txt</span>") (4) set
minsup to <span class="Style2">60% </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> AprioriRare </span></strong>contextZart.txt
output.txt 60%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextZart.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAprioriRare.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is AprioriRare?</p>

<blockquote>
  <p><strong>AprioriRare</strong> is an algorithm for mining minimal
rare itemsets from a transaction database. It is an Apriori-based
algorithm. It was proposed by Szathmary et al. (2007).</p>
</blockquote>

<p>What is the input ?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextZart.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output of <strong>AprioriRare</strong> is the set of <strong>minimal
rare itemsets.</strong> To explain what it a minimal rare itemset, it
is necessary to review a few definitions. An itemset is a unordered set
of distinct items. The <strong>support of an itemset</strong> is the
number of transactions that contain the itemset divided by the total
number of transactions. For example, the itemset {1, 2} has a support
of 60% because it appears in 3 transactions out of 5 in the previous
database (it appears in t1, t2 and t5). A<strong> frequent itemset</strong>
is an itemset that has a support no less than the <em>minsup </em>parameter.
A <strong>minimal rare itemset</strong> is an itemset that is not a
frequent itemset and that all its subsets are frequent itemsets.</p>
  <p>For example, if we run <strong>AprioriRare</strong> algorithm
with minsup = 60 % and the previous transaction database, we obtain the
following set of <strong>minimal rare itemsets</strong>: </p>
</blockquote>

<table align="center" border="1" width="399">

  <tbody>
    <tr>
      <td><strong>Minimal Rare Itemsets</strong></td>
      <td><strong>Support</strong></td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>20 %</td>
    </tr>
    <tr>
      <td height="25">{1, 3, 5}</td>
      <td>40 %</td>
    </tr>
    <tr>
      <td height="25"> {1, 2, 3}</td>
      <td>40 % </td>
    </tr>
  </tbody>
</table>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>AprioriRARE</strong>
is defined as follows. It is a text file. An item is represented by a
positive integer. A transaction is a line in the text file. In each
line (transaction), items are separated by a single space. It is
assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>AprioriRARE</strong>
is defined as follows. It is a text file, where each line represents a <strong>maximal
rare itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. </p>
  <p>4 #SUP: 1<br>
1 2 #SUP: 2<br>
1 5 #SUP: 2 </p>
  <p>The output file here consists of three lines which indicates that
the itemsets {4}, {1, 2} {1, 5} are perfectly rare itemsets having
respectively a support of 1 transaction, 2 transactions and 2
transactions.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>AprioriRare</strong> is the only algorithm for minimal
rare itemset mining offered in SPMF. Since it is based on Apriori, it
suffers from the same fundamental limitations (it may generate too much
candidates and it may generate candidates that do not appear in the
database).</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>AprioriRare</strong> algorithm is described in this
paper:</p>
  <p><em>Laszlo Szathmary, Amedeo Napoli, Petko Valtchev: <a href="http://www.philippe-fournier-viger.com/spmf/apriorirare.pdf" rel="nofollow">Towards Rare Itemset Mining</a>.
ICTAI (1) 2007: 305-312</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example18" id="example18"> </a></span></strong>
Example 21 : Mining Perfectly Rare Itemsets Using the
AprioriInverse Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">AprioriInverse</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextInverse.txt"</span></strong>, (3) set the output
file name (e.g. "<span class="Style2">output.txt</span>") (4) set
minsup = 0.1 % and maxsup of 60 % <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> AprioriInverse </span></strong>contextZart.txt
output.txt 10% 60%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextInverse.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAprioriInverse.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is AprioriInverse?</p>

<blockquote>
  <p><strong>AprioriInverse</strong> is an algorithm mining <strong>perfectly
rare itemsets</strong>. Why mining perfectly rare itemsets? One reason
is that it is useful for generating the set of <strong>sporadic
association </strong>rules.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextInverse.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output of <strong>AprioriInverse</strong> is the set of all
perfectly rare itemsets in the database such that their support is
lower than <em>maxsup </em>and higher than <em>minsup</em>. To
explain what it a perfectly rare itemset, it is necessary to review a
few definitions. An itemset is an unordered set of distinct items. The <strong>support
of an itemset</strong> is the number of transactions that contain the
itemset divided by the total number of transactions. For example, the
itemset {1, 2} has a support of 60% because it appears in 3
transactions out of 5 in the previous database (it appears in t1, t2
and t5). A <strong>frequent itemset </strong>is an itemset that has a
support no less than the <em>maxsup </em>parameter. A <strong>perfectly
rare itemset </strong>(aka<strong> sporadic itemset</strong>) is an
itemset that is not a frequent itemset and that all its proper subsets
are also not frequent itemsets. Moreover, it has to have a support
higher or equal to the <em>minsup</em> threshold.</p>
  <p>By running the <strong>AprioriInverse</strong> algorithm with
minsup = 0.1 % and maxsup of 60 % and this transaction database, we
obtain the following set of <strong>perfectly rare itemsets </strong>(see
  <a href="http://www.philippe-fournier-viger.com/spmf/koh3005.pdf" rel="nofollow">Koh &amp; Roundtree 2005</a> for
further details):</p>
</blockquote>

<table align="center" border="1" width="399">

  <tbody>
    <tr>
      <td><strong>Perfectly Rare Itemsets</strong></td>
      <td><strong>Support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>60 %</td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>40 %</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>60 %</td>
    </tr>
    <tr>
      <td height="25">{4, 5}</td>
      <td>40 %</td>
    </tr>
    <tr>
      <td height="25"> {3, 5}</td>
      <td>20 % </td>
    </tr>
  </tbody>
</table>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>AprioriInverse
  </strong>is defined as follows. It is a text file. An item is
represented by a positive integer. A transaction is a line in the text
file. In each line (transaction), items are separated by a single
space. It is assumed that all items within a same transaction (line)
are sorted according to a total order (e.g. ascending order) and that
no item can appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>AprioriInverse</strong>
is defined as follows. It is a text file, where each line represents a <strong>perfectly
rare itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, we show below the
output file for this example. </p>
  <p>3 #SUP: 3<br>
4 #SUP: 2<br>
5 #SUP: 3<br>
3 5 #SUP: 1<br>
4 5 #SUP: 2 </p>
  <p>The output file here consists of five lines which indicate that
the itemsets {3}, {4}, {5}, {3, 5}, {4, 5} are perfectly rare itemsets
having respectively a support of 3, 2, 3 1 and 2 transactions.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>AprioriInverse</strong> is the only algorithm for
perfectly rare itemset mining offered in SPMF. Since it is based on
Apriori, it suffers from the same fundamental limitations (it may
generate too much candidates and may generate candidates that do not
appear in the database).</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The<strong> AprioriInverse </strong>algorithm is described in
this p aper:</p>
  <p><em>Yun Sing Koh, Nathan Rountree: <a href="http://www.philippe-fournier-viger.com/spmf/koh2005.pdf" rel="nofollow">Finding Sporadic Rules Using Apriori-Inverse</a>. PAKDD
2005: 97-106</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="cori" id="example56"> </a></span></strong>
Example 22 : Mining Rare Correlated Itemsets Using the
CORI Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">CORI</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set minsup = 0.1 % and maxsup of 60 % <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CORI </span></strong><strong>contextPasquier99</strong>.txt
output.txt 10% 60%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPasquier99.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCORI_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is CORI?</p>

<blockquote>
  <p><strong>CORI</strong> is an algorithm for mining <strong>rare
correlated itemsets</strong>. </p>
  <p>It is an extension of the ECLAT algorithm. It uses two measures
called the <em>support </em>and the<em> bond</em> to evaluate if an
itemset is interesting and should be output.</p>
</blockquote>

<p>What is the input of the CORI algorithm?</p>

<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (aka
binary context) and a threshold named <em><strong>minsup</strong></em>
(a value between 0 and 100 %). </p>
    <p>A <strong>transaction database</strong> is a set of
transactions. Each <strong>transaction</strong> is a set of items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 3 and 4. This
database is provided as the file <strong>contextPasquier99.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of the CORI algorithm?</p>

<blockquote>
  <p><strong>CORI</strong> is an algorithm for discovering itemsets
(group of items) that are rare and correlated in a transaction database
(<strong>rare correlated itemsets</strong>). A<strong> rare itemset</strong>
is an itemset such that its <em>support </em>is no less than a <em>minsup</em>
threshold set by the user<em>.</em> The <em>support</em> of an itemset
is the number of transactions containing the itemset.</p>
  <p>A<strong> correlated itemset</strong> is an itemset such that its <em>bond
  </em>is no less than a <em>minbond</em> threshold set by the user<em>.</em>
The <em>bond </em>of an itemsets is the number of transactions
containing the itemset divided by the number of transactions containing
any of its items. The bond is a value in the [0,1] interval. A high
value means a highly correlated itemset. Note that single items have by
default a bond of 1.</p>
  <p>For example, if <strong>CORI</strong> is run on the previous
transaction database with a <em>minsup = </em>80% and <em>minbond = </em>20%,
  <strong>CORI</strong> outputs the following rare correlated itemsets:</p>
</blockquote>

<table align="center" border="1" width="274">

  <tbody>
    <tr>
      <td width="114"><strong>itemsets</strong></td>
      <td width="69"><strong>bond</strong></td>
      <td width="69"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>{1, 4}</td>
      <td>0.33</td>
      <td>1</td>
    </tr>
    <tr>
      <td>{3, 4}</td>
      <td>0.25</td>
      <td>1</td>
    </tr>
    <tr>
      <td>{1, 3, 4}</td>
      <td>0.25</td>
      <td>1</td>
    </tr>
    <tr>
      <td>{1, 2}</td>
      <td>0.4</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 3}</td>
      <td>0.4</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 5}</td>
      <td>0.4</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>0.4</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>0.75</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>0.4</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{1, 5}</td>
      <td>0.4</td>
      <td>2</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>0.6</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>0.6</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>0.6</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> used by CORI is defined
as follows. It is a text file. An item is represented by a positive
integer. A transaction is a line in the text file. In each line
(transaction), items are separated by a single space. It is assumed
that all items within a same transaction (line) are sorted according to
a total order (e.g. ascending order) and that no item can appear twice
within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>CORI</strong>
is defined as follows. It is a text file, where each line represents a <strong>correlated
rare itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it is followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. After, all the items, the
keyword "#BOND:" appears, which is followed by a double value
indicating the bond of the itemset. For example, we show below the
output file for this example. </p>
  <p>1 #SUP: 3 #BOND: 1.0<br>
4 #SUP: 1 #BOND: 1.0<br>
4 1 #SUP: 1 #BOND: 0.3333333333333333<br>
4 3 #SUP: 1 #BOND: 0.25<br>
4 1 3 #SUP: 1 #BOND: 0.25<br>
1 2 #SUP: 2 #BOND: 0.4<br>
1 2 3 #SUP: 2 #BOND: 0.4<br>
1 2 5 #SUP: 2 #BOND: 0.4<br>
1 2 3 5 #SUP: 2 #BOND: 0.4<br>
1 3 #SUP: 3 #BOND: 0.75<br>
1 3 5 #SUP: 2 #BOND: 0.4<br>
1 5 #SUP: 2 #BOND: 0.4<br>
2 3 #SUP: 3 #BOND: 0.6<br>
2 3 5 #SUP: 3 #BOND: 0.6<br>
3 5 #SUP: 3 #BOND: 0.6</p>
  <p>The output file here consists of 15 lines. Consider the last line.
It indicates that the itemset {3, 5} is a rare correlated itemset
having a support and bond of respectively 3 and 0.6.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>CORI</strong> is the only algorithm for mining correlated
rare itemsets offered in SPMF. The implementation is well optimized. It
is a quite simple extension of the ECLAT algorithm.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The<strong> CORI </strong>algorithm is described in this paper:</p>
  <p><em>Bouasker, S., Yahia, S. B. (2015). Key correlation mining by
simultaneous monotone and anti-monotone constraints checking. Proc. of
the 2015 ACM Symposium on Applied Computing (SAC 2015), pp. 851-856. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example22" id="example22"> </a></span></strong>
Example 23 : Mining Closed Itemsets from a Data Stream
Using the CloStream Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This example is not available in the release version of
SPMF.</strong></li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestCloStream.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is CloStream?</p>

<blockquote>
  <p><strong>CloStream</strong> is an algorithm for <strong>incrementally
mining</strong><strong> closed itemsets </strong>from a data stream.
It was proposed by Yen et al. (2009).</p>
  <p>Why is it useful? Because most closed itemset mining algorithms
such as Charm, DCI_Closed and AprioriClose are batch algorithms. This
means that if the transaction database is updated, we need to run the
algorithms again to update the set of closed itemsets. If there is
constant insertion of new transactions and the results need to be
updated often, it may become very costly to use these algorithms. A <strong>stream
mining algorithm</strong> like <strong>CloStream</strong> is specially
designed to handle this situation. It assumes that each transaction in
a database can only be read once and that new transaction appears
regularly. Every time that a new transaction appear, the result is
updated by CloStream.</p>
</blockquote>

<p>What is the input of CloStream?</p>

<blockquote>
  <p>The input of <strong>CloStream</strong> is a stream of
transactions. Each <strong>transaction</strong> is a set of items
(symbols). For example, consider the following five transactions (t1,
t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 2 and 4. It is important to
note that an item is not allowed to appear twice in the same
transaction and that items are assumed to be sorted by lexicographical
order in a transaction. <strong>CloStream</strong> is an algorithm for
processing a stream. This means that CloStream is allowed to read each
transaction only once because a stream is assumed to be potentially
infinite and coming at high speed.</p>
</blockquote>

<table align="center" border="1" width="404">

  <tbody>
    <tr>
      <td width="116"><strong>Transaction ID</strong></td>
      <td width="203"><strong>Items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1,2 4}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{2, 5}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
  </tbody>
</table>

<p>What is the output of CloStream?</p>

<blockquote> <strong>CloStream</strong> produces as output the set of <strong>closed
itemset</strong>s contained in the transactions that it has seen until
now. An itemset is an unordered set of distinct items. The <strong>support
of an itemset </strong>is the number of transactions that contains the
itemset. For example, the itemset {1, 2, 4} has a support of 1 because
it only appear in t1. A <strong>closed itemset </strong>is an itemset
that is not included in another itemset having the same support. For
example, if we apply <strong>CloStream</strong> to the five following
transactions, the final result is:</blockquote>

<table align="center" border="1" width="267">

  <tbody>
    <tr>
      <td><strong>closed itemsets</strong></td>
      <td><strong>support</strong></td>
    </tr>
    <tr>
      <td>{}</td>
      <td>5</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 3, 4}</td>
      <td>1</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>4</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
    </tr>
    <tr>
      <td>{1, 2, 3, 5}</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For example, the itemset {2, 3, 5} has a support of 3 because it
appears in transactions t2, t4 and t5. It is a closed itemset because
it has no proper superset having the same support.</p>
</blockquote>

<p>Input and output file format</p>

<blockquote>
  <p>This is not applicable for this algorithm since it is designed for
a stream of data (see the source code example referenced above to
understand how to use this algorithm).</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>CloStream</strong> is a reasonably efficient algorithm. A
limitation of this algorithm is that it is not possible to set a
minimum support threshold. Therefore, if the number of closed itemsets
is large, this algorithm may use too much memory. However, CloStream
has the advantage of being very simple an easy to implement.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>CloStream</strong> algorithm is described in this
paper:</p>
  <p>Show-Jane Yen, Yue-Shi Lee, Cheng-Wei Wu, Chin-Lin Lin:<a href="http://www.philippe-fournier-viger.com/spmf/yen2009.pdf" rel="nofollow"> An Efficient Algorithm for Maintaining Frequent Closed
Itemsets over Data Stream</a>. IEA/AIE 2009: 767-776.</p>
</blockquote>

<h3><strong><span class="centered"><a name="estdec" id="example6"> </a></span></strong>
Example 24 : Mining Recent Frequent Itemsets from a Data
Stream Using the estDec Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This example is not available in the release version of
SPMF.</strong></li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestEstDec_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is estDec?</p>

<blockquote>
  <p><strong>estDec</strong> is an algorithm for <strong>mining recent
frequent itemsets </strong>from a<strong> data stream</strong>. It was
proposed by Chang et al. (2003).</p>
  <p>Why is it useful? Because most itemset mining algorithms such as
Apriori, FPGrowth and Eclat are batch algorithms. This means that if
the input transaction database is updated, those algorithms need to be
run again from zero to update the result, which is inefficient. <strong>Stream
mining algorithms</strong> such as <strong>estDec</strong> are
designed for discovering patterns in a stream (a potentially infinite
sequence of transactions) and for updating the results incrementally
after each new transaction. Stream mining algorithms assume that each
transaction in a database can only be read once. The <strong>estDec</strong>
  <strong>algorithm</strong> is also interesting because it mines <strong>recent
  </strong>frequent itemsets, which means that it put more weight on
recent transactions than on older transactions when searching from
frequent itemsets. This allows <strong>estDec </strong>to learn new
trends and to forgot older trends.</p>
</blockquote>

<p>What is the input of estDec?</p>

<blockquote>
  <p>The input of <strong>estDec</strong> is a stream of transactions
and a support threshold <em>minsup</em>. Each <strong>transaction</strong>
is a set of items (symbols). For example, consider the following six
transactions (t1, t2, ..., t6) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. It is
important to note that an item is not allowed to appear twice in the
same transaction and that items are assumed to be sorted by
lexicographical order in a transaction. <strong>estDec</strong> is an
algorithm for processing a stream. This means that <strong>estDec</strong>
is allowed to read each transaction only once because a stream is
assumed to be potentially infinite and coming at high speed.</p>
</blockquote>

<table align="center" border="1" width="404">

  <tbody>
    <tr>
      <td width="116"><strong>Transaction ID</strong></td>
      <td width="203"><strong>Items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 2, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{1, 2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{1, 2, 3, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t6</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
  </tbody>
</table>

<p>What is the output of estDec?</p>

<blockquote>
  <p><strong>estDec</strong> produces as output the set of <strong>recent
frequent itemset</strong>s contained in the transactions that <strong>estDec</strong>
has seen until now. It is said that <strong>estDec </strong>mines <strong>recent
  </strong>frequent itemsets because estDec utilizes a decay function
so that <strong>estDec </strong>puts more weight on recent
transactions than on older ones (frequency of an itemset). This allows <strong>estDec
  </strong>to learn new trends and to forgot older trends.</p>
  <p>The output is a set of <strong>recent frequent itemsets</strong>.
The <strong>support count of an itemset </strong>is the number of
transactions that contains the itemset. For example, the itemset {1, 2,
4} has a support count of 1 because it only appear in t1. The <strong>support
  </strong>of an itemset is the number of transaction were the itemset
appears divided by the total number of transactions seen until now. A
frequent itemset is an itemset that has a support higher or equal to <em>minsup</em>.
  </p>
  <p>The <strong>estDec</strong> algorithm is an approximate
algorithm. It approximate the support of itemsets and returns itemsets
that have an estimated support higher than <em>minsup</em>.</p>
  <p>For example, consider the example <strong><span class="Style2">MainTestEstDec_saveToFile.java</span></strong>.
This example consists of loading the transactions from a file named "<span class="Style2">contextIGB.tx</span>t" provided in the SPMF
distribution. Then, this example show how to save the result to a file.
Here is the output:</p>
  <p>3 5 #SUP: 0.5000519860383547<br>
2 #SUP: 0.8333622131312072<br>
1 2 3 #SUP: 0.33335643690463074<br>
3 #SUP: 0.5000519860383547<br>
1 4 #SUP: 0.3333448844517001<br>
3 4 #SUP: 0.19334881331065332<br>
1 3 5 #SUP: 0.33335643690463074<br>
1 2 5 #SUP: 0.5000173262771588<br>
2 5 #SUP: 0.8333622131312072<br>
1 #SUP: 0.5000173262771588<br>
2 3 5 #SUP: 0.5000519860383547<br>
1 5 #SUP: 0.5000173262771588<br>
2 3 #SUP: 0.5000519860383547<br>
4 #SUP: 0.3333448844517001<br>
1 4 5 #SUP: 0.3333448844517001<br>
2 4 5 #SUP: 0.3333448844517001<br>
1 2 #SUP: 0.5000173262771588<br>
5 #SUP: 0.8333622131312072<br>
1 3 #SUP: 0.33335643690463074<br>
2 4 #SUP: 0.3333448844517001<br>
1 2 4 #SUP: 0.3333448844517001<br>
4 5 #SUP: 0.3333448844517001<br>
  </p>
  <p>For example, consider line 1. It indicates that the pattern {1, 2,
5} is a recent frequent itemsets with an estimated support of 50%</p>
  <p>Note that we also provide a second example named <strong><span class="Style2">MainTestEstDec_saveToMemory.java</span></strong>. This
example shows how to process a set of transactions from memory instead
of from a file and to keep the result into memory instead of saving the
result to a file. This is especially useful, if you wish to integrate
estDec into another Java program. The example also shows how to set the
decay rate.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The estDec algorithm can either take as input a stream in memory
or read transactions from a file. The <strong> input file format</strong>
of <strong>estDec </strong>is defined as follows. It is a text file.
An item is represented by a positive integer. A transaction is a line
in the text file. In each line (transaction), items are separated by a
single space. It is assumed that all items within a same transaction
(line) are sorted according to a total order (e.g. ascending order) and
that no item can appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 5</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>estDec</strong>
is defined as follows. It is a text file, where each line represents a <strong>recently
frequent itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer and it is followed
by a single space. After, all the items, the keyword "#SUP:" appears,
which is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, here are few lines
of the output file for this example. </p>
  <p>3 5 #SUP: 0.5000519860383547<br>
2 #SUP: 0.8333622131312072<br>
1 2 3 #SUP: 0.33335643690463074</p>
  <p>The output file here consists of the first line indicates that the
itemset {1, 2, 3} has an estimated support of 50 %.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>estDec</strong> is a reasonably efficient algorithm. </p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>estDec</strong> algorithm is described in this paper:</p>
  <p>Joong Hyuk Chang, Won Suk Lee: Finding recent frequent itemsets
adaptively over online data streams. KDD 2003: 487-492</p>
</blockquote>

<h3><strong><span class="centered"><a name="estdec" id="example50"> </a></span></strong>
Example 25 : Mining Recent Frequent Itemsets from a Data
Stream Using the estDec+ Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This example is not available in the release version of
SPMF.</strong></li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestEstDecPlus_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is estDec+?</p>

<blockquote>
  <p><strong>estDec+</strong> is an algorithm for <strong>mining
recent frequent itemsets </strong>from a<strong> data stream</strong>.
It is an extension of <strong>estDec </strong>proposed by Chang et
al. in 2005. The main difference with estDec is to use a compressed
tree to maintain information about recent frequent itemsets, which may
be more memory efficient in some cases but may decrease accuracy. Note
that the version of estDec+ implemented here is based on the 2014 paper
by Chang et al.</p>
  <p>Why is it useful? Because most itemset mining algorithms such as
Apriori, FPGrowth and Eclat are batch algorithms. This means that if
the input transaction database is updated, those algorithms need to be
run again from zero to update the result, which is inefficient. <strong>Stream
mining algorithms</strong> such as <strong>estDec+</strong> are
designed for discovering patterns in a stream (a potentially infinite
sequence of transactions) and for updating the results incrementally
after each new transaction. Stream mining algorithms assume that each
transaction in a database can only be read once. The <strong>estDec</strong>+
  <strong>algorithm</strong> is also interesting because it mines <strong>recent
  </strong>frequent itemsets, which means that it put more weight on
recent transactions than on older transactions when searching for
recent frequent itemsets. This allows <strong>estDec+ </strong>to
learn new trends and to forgot older trends.</p>
</blockquote>

<p>What is the input of estDec+?</p>

<blockquote>
  <p>The input of <strong>estDec</strong>+ is a stream of transactions
and a support threshold <em>minsup</em>. Each <strong>transaction</strong>
is a set of items (symbols). For example, consider the following six
transactions (t1, t2, ..., t6) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. It is
important to note that an item is not allowed to appear twice in the
same transaction and that items are assumed to be sorted by
lexicographical order in a transaction. <strong>estDec+</strong> is an
algorithm for processing a stream. This means that <strong>estDec+</strong>
is allowed to read each transaction only once because a stream is
assumed to be potentially infinite and coming at high speed.</p>
</blockquote>

<table align="center" border="1" width="404">

  <tbody>
    <tr>
      <td width="116"><strong>Transaction ID</strong></td>
      <td width="203"><strong>Items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 2, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{1, 2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{1, 2, 3, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t6</strong></td>
      <td>{2, 3, 5}</td>
    </tr>
  </tbody>
</table>

<p>What is the output of estDec+?</p>

<blockquote>
  <p><strong>estDec+</strong> produces as output the set of <strong>recent
frequent itemset</strong>s contained in the transactions that <strong>estDec</strong>+
has seen until now. It is said that <strong>estDec </strong>mines <strong>recent
  </strong>frequent itemsets because estDec utilizes a decay function
so that <strong>estDec </strong>puts more weight on recent
transactions than on older ones (frequency of an itemset). This allows <strong>estDec+
  </strong>to learn new trends and to forgot older trends.</p>
  <p>The output is a set of <strong>recent frequent itemsets</strong>.
The <strong>support count of an itemset </strong>is the number of
transactions that contains the itemset. For example, the itemset {1, 2,
4} has a support count of 1 because it only appear in t1. The <strong>support
  </strong>of an itemset is the number of transaction were the itemset
appears divided by the total number of transactions seen until now. A
frequent itemset is an itemset that has a support higher or equal to <em>minsup</em>.
  </p>
  <p>The <strong>estDec+</strong> algorithm is an approximate
algorithm. It approximate the support of itemsets and returns itemsets
that have an estimated support higher or equal to <em>minsup</em>.</p>
  <p>For example, consider the example <strong><span class="Style2">MainTestEstDecPlus_saveToFile.java</span></strong>.
This example consists of loading the transactions from a file named "<span class="Style2">contextIGB.tx</span>t" provided in the SPMF
distribution. Then, this example show how to save the result to a file.
Here is the output:</p>
  <p>2 5 #SUP: 1.0<br>
1 4 5 #SUP: 0.5<br>
1 2 3 #SUP: 0.5<br>
5 #SUP: 1.0<br>
1 2 5 #SUP: 0.5<br>
1 #SUP:0.66<br>
1 5 #SUP: 0.5555555555555556<br>
1 2 4 #SUP: 0.5<br>
4 5 #SUP: 0.5<br>
2 4 #SUP: 0.5<br>
1 4 #SUP: 0.5555555555555556<br>
1 3 #SUP: 0.5555555555555556<br>
4 #SUP: 0.5<br>
1 3 5 #SUP: 0.5<br>
2 3 #SUP:0.66<br>
1 2 #SUP: 0.5555555555555556<br>
3 4 #SUP:0.66<br>
2 #SUP: 1.0<br>
3 5 #SUP:0.66<br>
2 4 5 #SUP: 0.5<br>
3 #SUP:0.66<br>
2 3 5 #SUP:0.66<br>
  </p>
  <p>For example, consider line 1. It indicates that the pattern {1, 2,
5} is a recent frequent itemsets with an estimated support of 50%</p>
  <p>Note that we also provide a second example named <strong><span class="Style2">MainTestEstDec_saveToMemory.java</span></strong>. This
example shows how to process a set of transactions from memory instead
of from a file and to keep the result into memory instead of saving the
result to a file. This is especially useful, if you wish to integrate
estDec into another Java program. The example also shows how to set the
decay rate.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The estDec algorithm can either take as input a stream in memory
or read transactions from a file. The <strong> input file format</strong>
of <strong>estDec </strong>is defined as follows. It is a text file.
An item is represented by a positive integer. A transaction is a line
in the text file. In each line (transaction), items are separated by a
single space. It is assumed that all items within a same transaction
(line) are sorted according to a total order (e.g. ascending order) and
that no item can appear twice within the same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 5</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>estDec+</strong>
is defined as follows. It is a text file, where each line represents a <strong>recently
frequent itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer and it is followed
by a single space. After, all the items, the keyword "#SUP:" appears,
which is followed by an integer indicating the support of the itemset,
expressed as a number of transactions. For example, here are few lines
of the output file for this example. </p>
  <p>1 2 3 #SUP: 0.5<br>
5 #SUP: 1.0<br>
1 2 5 #SUP: 0.5</p>
  <p>The output file here consists of the first line indicates that the
itemset {1, 2, 3} has an estimated support of 50 %.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>estDec+</strong> is a reasonably efficient algorithm. When
minsup is high, it may use less memory than the original estDec
algorithm because the CP-Tree is generally smaller than the estTree.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>estDec</strong>+ algorithm is described in this paper:</p>
  <p>Se Jung Shin , Dae Su Lee , Won Suk Lee, âCP-tree: An adaptive
synopsis structure for compressing frequent itemsets over online data
streamsâ, Information Sciences,Volume 278, 10 September 2014, Pages
559â576</p>
</blockquote>

<h3><strong><span class="centered"><a name="uapriori" id="example23"> </a></span></strong>
Example 26 : Mining Frequent Itemsets from Uncertain
Data Using the U-Apriori Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">UApriori</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextUncertain.txt"</span></strong>, (3) set the
output file name (e.g. "<span class="Style2">output.txt</span>") (4)
set the minimum expected support to 0.10 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> UApriori </span></strong><strong><span class="Style9">contextUncertain</span></strong>.txt
output.txt 10% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextUncertain.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestUApriori.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is UApriori?</p>

<blockquote>
  <p><strong>UApriori</strong> is an algorithm for mining frequent
itemsets from a transaction database where the data is uncertain
(contains probabilities). The UApriori algorithm was proposed by Chui
et al. (2007). </p>
  <p>This algorithm can have multiple applications such as in mining
medical data or sensor data where observations may be uncertain.</p>
</blockquote>

<p>What is the input ?</p>

<blockquote>
  <p><strong>UApriori</strong> takes as input a <strong>transaction
database</strong> containing probabilities and a <strong>minimum
expected support</strong> threshold (a value between 0 and 1). A<strong>
transaction database</strong> is a set of transactions where each
transaction is a set of items. In UApriori, we assume that each item in
a transaction is annotated with an existential probability. For
example, let's consider the following transaction database, consisting
of 4 transactions (t1,t2...t5) and 5 items (1,2,3,4,5). The transaction
t1 contains item 1 with a probability of 0.5, item 2 with a probability
of 0.4, item 4 with a probability of 0.3 and item 5 with a probability
of 0.7. This database is provided in the file "<strong>contextUncertain.txt</strong>"
of the SPMF distribution:</p>
</blockquote>

<table align="center" border="1" width="200">

  <tbody>
    <tr>
      <td> <br>
      </td>
      <td><strong>1</strong></td>
      <td><strong>2</strong></td>
      <td><strong>3</strong></td>
      <td><strong>4</strong></td>
      <td><strong>5</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>0.5</td>
      <td>0.4</td>
      <td> <br>
      </td>
      <td>0.3</td>
      <td>0.7</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td> <br>
      </td>
      <td>0.5</td>
      <td>0.4</td>
      <td> <br>
      </td>
      <td>0.4</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>0.6</td>
      <td>0.5</td>
      <td> <br>
      </td>
      <td>0.1</td>
      <td>0.5</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>0.7</td>
      <td>0.4</td>
      <td>0.3</td>
      <td> <br>
      </td>
      <td>0.9</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>The output of <strong>U-Apriori </strong>is the set of frequent
itemsets. Note that the definition of a <strong>frequent itemset </strong>is
here different from the definition used by the regular Apriori
algorithm because we have to consider the existential probabilities.</p>
  <p>The <strong>expected support of an itemset in a transaction</strong>
is defined as the product of the existential probability of each item
from the itemset in this transaction. It is a value between 0 and 1.
For example, the support of itemset {1, 2} in transaction t1 is 0.5 x
0.4 = 0.2. The <strong>expected support of an itemset in a transaction
database</strong> is the sum of its support in all transactions where
it occurs. For example, the expected support of itemset {2, 3} is the
sum of its expected support in t2 and t4 : 0.5 x 0.4 + 0.4 x 0.3 =
0.32. A <strong>frequent itemset</strong> is an itemset that has an
expected support higher or equal to the minimum expected support set by
the user. For example, by running <strong>U-Apriori </strong>with a<strong>
minimum expected support of 0.10</strong>, we obtain 19 frequent
itemsets, including:</p>
</blockquote>

<table align="center" border="1" width="267">

  <tbody>
    <tr>
      <td><strong> itemsets</strong></td>
      <td><strong>expected support</strong></td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>0.19</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>0.19</td>
    </tr>
    <tr>
      <td>{1 4 5}</td>
      <td>0.14</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>0.11</td>
    </tr>
    <tr>
      <td>{1 2 5}</td>
      <td>0.54</td>
    </tr>
    <tr>
      <td>{1 5}</td>
      <td>1.28</td>
    </tr>
    <tr>
      <td>{1 3}</td>
      <td>0.21</td>
    </tr>
    <tr>
      <td>{1 4}</td>
      <td>0.21</td>
    </tr>
    <tr>
      <td>{2 3}</td>
      <td>0.32</td>
    </tr>
    <tr>
      <td>{1 2}</td>
      <td>0.78</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of<strong> UApriori </strong>is
defined as follows. It is a text file. An item is represented by a
positive integer. Each item is associated with a probability indicated
as a double value between parenthesis. A transaction is a line in the
text file. In each line (transaction), each item is immediately
followed by its probability between parenthesis and a single space. It
is assumed that all items within a same transaction (line) are sorted
according to a total order (e.g. ascending order) and that no item can
appear twice within the same line. Probabilities should be greater than
0 and not more than 1.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p># This binary context contains uncertain data.<br>
# Each line represents a transaction.<br>
# For each item there is an existential probability.<br>
1(0.5) 2(0.4) 4(0.3) 5(0.7)<br>
2(0.5) 3(0.4) 5(0.4)<br>
1(0.6) 2(0.5) 4(0.1) 5(0.5)<br>
1(0.7) 2(0.4) 3(0.3) 5(0.9)</p>
  <p>The first line represents the itemsets {1, 2, 4, 5} where items 1,
2, 4 and 5 respectively have the probabilities 0.5, 0.4, 0.3 and 0.7.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>UApriori</strong>
is defined as follows. It is a text file, where each line represents a <strong>frequent
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer and it expected support
between parenthesis, followed by a single space. After, all the items,
the keyword "#SUP:" appears, which is followed by a double value
indicating the expected support of the itemset. For example, we show
below the output file for this example. </p>
  <p>2 (0.4) Support: 1.7999999999999998<br>
3 (0.4) Support: 0.7<br>
4 (0.3) Support: 0.4<br>
5 (0.7) Support: 2.5<br>
1 (0.5) Support: 1.8<br>
1 (0.5) 2 (0.4) Support: 0.78<br>
1 (0.5) 3 (0.4) Support: 0.21<br>
2 (0.4) 5 (0.7) Support: 1.09<br>
2 (0.4) 4 (0.3) Support: 0.16999999999999998<br>
1 (0.5) 5 (0.7) Support: 1.2799999999999998<br>
3 (0.4) 5 (0.7) Support: 0.43000000000000005<br>
2 (0.4) 3 (0.4) Support: 0.32<br>
1 (0.5) 4 (0.3) Support: 0.21<br>
4 (0.3) 5 (0.7) Support: 0.26<br>
1 (0.5) 3 (0.4) 5 (0.7) Support: 0.189<br>
2 (0.4) 3 (0.4) 5 (0.7) Support: 0.188<br>
1 (0.5) 2 (0.4) 5 (0.7) Support: 0.542<br>
1 (0.5) 4 (0.3) 5 (0.7) Support: 0.135<br>
2 (0.4) 4 (0.3) 5 (0.7) Support: 0.10899999999999999 </p>
  <p>For example, the last line indicates that the itemset {2, 4, 5}
has an expected support of 0.1089999 and that items in this itemset
have an existential support of 0.4, 0.3 and 0.7 with respect to this
itemset, respectively.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>UApriori is not the most efficient algorithm for uncertain itemset
mining but it is simple and it is the first algorithm designed for this
task. </p>
</blockquote>

<p>Where can I get more information about the UApriori algorithm?</p>

<blockquote>
  <p>Here is an article describing the UApriori algorithm:</p>
  <p><em>C. Kit Chui, B. Kao, E. Hung: <a href="http://www.philippe-fournier-viger.com/spmf/uapriori.pdf" rel="nofollow">Mining Frequent Itemsets from Uncertain Data</a>. PAKDD
2007: 47-58</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="erasable" id="example30"> </a></span></strong>
Example 27 : Mining Erasable Itemsets from a Product
Database with the VME
algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">VME</span>"</strong> algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style9">contextVME</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
(e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (4)
set the threshold to 15%. <span class="Style2"> </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> VME</span></strong><strong><span class="Style9">
contextVME</span></strong>.txt output.txt 15% </span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextVME.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestVME.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is the VME algorithm?</p>

<blockquote>
  <p><strong>VME</strong> (<a href="http://www.philippe-fournier-viger.com/spmf/erasable_itemsets.pdf" rel="nofollow">Deng &amp; Xu, 2010</a>) is an algorithm for mining <strong>erasable
itemsets</strong> from a product database with profit information. </p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>VME</strong> takes as input a <strong>product database</strong>
and a threshold (a value between 0 and 100%). A product is defined as a
set of items that are used to assemble the product. Moreover each
product is annotated with a profit (a positive integer) that indicates
how much money this product generate for the company. For example,
let's consider the following product database, consisting of 6 products
and 7 items (this example is taken from the article of <a href="http://www.philippe-fournier-viger.com/spmf/erasable_itemsets.pdf" rel="nofollow">Deng &amp; Xu, 2010</a>).
Each product is annotated with the profit information. For example, the
first line indicates that the product 1 generate a total profit of 50 $
for the company and that its assembly requires parts 2, 3, 4 and 6.
This product database is provided in the file "<strong>contextVME.txt</strong>"
of the SPMF distribution<strong>.</strong>:</p>
</blockquote>

<table align="center" border="1" width="332">

  <tbody>
    <tr>
      <td width="74"> <br>
      </td>
      <td width="47"><strong>profit</strong></td>
      <td width="61"><strong>items</strong></td>
    </tr>
    <tr>
      <td><strong>product1</strong></td>
      <td>50$</td>
      <td>{2, 3, 4, 6}</td>
    </tr>
    <tr>
      <td><strong>product2</strong></td>
      <td>20$</td>
      <td>{2, 5, 7}</td>
    </tr>
    <tr>
      <td><strong>product3</strong></td>
      <td>50$</td>
      <td>{1, 2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>product4</strong></td>
      <td>800$</td>
      <td>{1, 2, 4}</td>
    </tr>
    <tr>
      <td><strong>product5</strong></td>
      <td>30$</td>
      <td>{6, 7}</td>
    </tr>
    <tr>
      <td><strong>product6</strong></td>
      <td>50$</td>
      <td>{3, 4}</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>The output is the set of <strong>erasable itemsets</strong>
generating a loss of profit lower or equal to the user-specificed
threshold. The idea is to discover item that the company could stop
manufacturing and that would minimize the amount of profit lost by
being unable to build products. </p>
  <p>To explain what is an erasable itemset more formally, it is
necessary to review some definitions An itemset is an unordered set of
distinct items. The <strong>loss of profit generated by an itemset </strong>is
defined as the sum of the product profit for all products containing an
item from this itemset. For example, the lost of profit of itemset {5,
6} is the sum of the profits of products containing 5 and/or 6: 50$ +
20 $ + 50 $ + 30 $ = 150 $. The loss of profit can also be expressed as
a percentage of the total profit of the database. For example, in this
database the total profit is 50 + 20 + 50 + 800 + 30 + 50 = 1000$.
Therefore, the lost of profit by the itemset {5, 6} could be expressed
as 15% (150 / 1000 * 100).</p>
  <p>By running <strong>VME </strong>with a<strong> threshold of 15 %</strong>,
we obtain 8 erasable itemsets (having a profit loss less or equal to
15% x 1000$ = 150 $):</p>
</blockquote>

<table align="center" border="1" width="297">

  <tbody>
    <tr>
      <td width="138"><strong>erasable itemsets</strong></td>
      <td width="143"><strong>loss of profit ("gain")</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>150</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>70</td>
    </tr>
    <tr>
      <td>{6}</td>
      <td>80</td>
    </tr>
    <tr>
      <td>{7}</td>
      <td>50</td>
    </tr>
    <tr>
      <td>{5 6}</td>
      <td>150</td>
    </tr>
    <tr>
      <td>{5 7}</td>
      <td>100</td>
    </tr>
    <tr>
      <td>{6 7}</td>
      <td>100</td>
    </tr>
    <tr>
      <td>{5 6 7}</td>
      <td>150</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>This means that if the items from one of those erasable itemsets
are not manufactured anymore, then the loss of profit will be lower or
equal to 15%.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>VME </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of two sections, as follows.</p>
  <ul>
    <li>First, the profit of the transaction is indicated by an integer
number, followed by a single space.</li>
    <li>Second, the items in the transaction are listed. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>50 2 3 4 6<br>
20 2 5 7<br>
50 1 2 3 5<br>
800 1 2 4<br>
30 6 7<br>
50 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {2, 3, 4,
6} has a profit of 50 and it contains the items 2, 3, 4 and 6. The
following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>VME </strong>is
defined as follows. It is a text file, where each line represents an <strong>erasable
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer, followed by a single
space. After, all the items, the keyword "#LOSS:" appears, which is
followed by a integer value indicating the loss of profit for that
itemset.</p>
  <p>3 #LOSS: 150<br>
5 #LOSS: 70<br>
6 #LOSS: 80<br>
7 #LOSS: 50<br>
5 6 #LOSS: 150<br>
5 7 #LOSS: 100<br>
6 7 #LOSS: 100<br>
5 6 7 #LOSS: 150 </p>
  <p>For example, the first line indicates that the itemset {3} would
generate a loss of profit of 150. The following lines follows the same
format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The VME algorithm is Apriori-based. It is not the fastest
algorithm for this problem. But it is the only one available in SPMF
because this problem is not very popular. For more efficient algorithms
for this problem, you can search for the author names. They have
proposed a few algorithms with some improvements.</p>
</blockquote>

<p>Where can I get more information about the VME algorithm?</p>

<blockquote>
  <p>Here is an article describing the VME algorithm:</p>
  <p><em> Z. Deng, X. Xu: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/erasable_itemsets.pdf">An
Efficient Algorithm for Mining Erasable Itemsets</a>. ADMA (1) 2010:
214-225</em>.</p>
</blockquote>

<h3><strong><span class="centered"><a name="itree" id="example37"> </a></span></strong>
Example 28 : Building, updating incrementally and using
an Itemset-Tree to generate targeted frequent itemsets and association
rules.</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This algorithm is not offered in the release version of
SPMF.</strong></li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestItemsetTree.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is an itemset-tree?</p>

<blockquote>
  <p>An itemset-tree is a special data structure that can be used for
performing efficient queries about itemsets and association rules in a
transaction database without having to generate all of them beforehand.</p>
  <p>An itemset-tree has the nice property of being incremental, which
means that new transactions can be added to an existing itemset tree
very efficiently without having to rebuild the tree from scratch. An
itemset-tree also has the property of being compact.</p>
</blockquote>

<p>How to use it?</p>

<blockquote>
  <p>An itemset-tree is built by inserting a set of transactions into
the tree. A <strong>transaction</strong> is simply a set of distinct
items. For example, we could insert the following 6 transactions
(t1,t2...t5) into an itemset-tree. In this example, the transaction t1
represents the set of items {1, 4}. This set of transactions is
provided in the file "<strong><span class="Style2">contextItemsetTree.txt</span></strong>"
of the SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="495">

  <tbody>
    <tr>
      <td width="156"><strong>transaction IDs</strong></td>
      <td width="323"><strong>items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 4}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{2, 5}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 3, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{1, 2, 4}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{2, 5}</td>
    </tr>
    <tr>
      <td><strong>t6</strong></td>
      <td>{2, 4}</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The result of the insertion of these six transactions is the
following itemset-tree (see the article by Kubat for more details).</p>
  <pre>{}   sup=6<br>  [2 ]   sup=3<br>    [2 5 ]   sup=2<br>    [2 4 ]   sup=1<br>  [1 ]   sup=3<br>    [1 2 ]   sup=2<br>      [1 2 4 ]   sup=1<br>      [1 2 3 4 5 ]   sup=1<br>    [1 4 ]   sup=1</pre>
  <p>The root is the empty itemset {} and the leafs are {2, 5}, {2, 4},
{1 2 4},{1 2 3 4 5} and {1, 4}.</p>
  <p>Once an itemset-tree has been created, it is possible to update it
by inserting a new transaction. For example, in this example provided
in the source code, we update the previous tree by adding a new
transaction {4, 5}. The result is this tree:</p>
  <pre>{}   sup=7<br>  [2 ]   sup=3<br>    [2 5 ]   sup=2<br>    [2 4 ]   sup=1<br>  [1 ]   sup=3<br>    [1 2 ]   sup=2<br>      [1 2 4 ]   sup=1<br>      [1 2 3 4 5 ]   sup=1<br>    [1 4 ]   sup=1<br>  [4 5 ]   sup=1</pre>
  <p>Next, it is shown how to query the tree to determine the support
of a target itemset efficiently. For example, if we execute the query
of finding the support of the itemset {2}, the support is determined to
be 5 because 2 appear in 5 transactions.</p>
  <p>After that the source code offers an example of how to use the
itemset tree to get all itemsets that subsume an itemset and to get
their support. For example, if we use the itemset {1 2} for this query
the result is:</p>
  <pre>[1 2 ]    supp:2<br>[1 2 3 ]    supp:1<br>[1 2 4 ]    supp:2<br>[1 2 5 ]    supp:1<br>[1 2 3 4 ]    supp:1<br>[1 2 3 5 ]    supp:1<br>[1 2 4 5 ]    supp:1<br>[1 2 3 4 5 ]    supp:1</pre>
  <p> Another example provided is how to use the tree to find all
itemsets that subsume an itemset such that the support is higher or
equal to a user-specified threshold named <em>minsup </em>(a positive
integer representing a number of transactions). For example, if we
execute this query with the itemset {1} and minsup =2, we get this
result: </p>
  <pre>[1 ]    supp:3<br>[1 2 ]    supp:2<br>[1 4 ]    supp:3<br>[1 2 4 ]    supp:2</pre>
  <p>Lastly, another example is how to generate all association rules
having a target itemset as antecedent and a support and confidence
respectively higher or equal to some user-specificed thresholds <em>minsup
  </em>(a positive integer representing a number of transactions) and <em>minconf
  </em>(a value between 0 and 1). For example, if the target itemset is
{1} and minconf = 0.1 and minsup = 2, the result is:</p>
  <pre>[ 1  ] ==&gt; [2  ]  sup=2  conf=0.666666666666666<br><br>[ 1  ] ==&gt; [4  ]  sup=3  conf=1.0<br><br>[ 1  ] ==&gt; [2 4  ]  sup=2  conf=0.66666666666666</pre>
</blockquote>

<p>Input and output file format</p>

<blockquote>
  <p>There is no need to use an input and output file with an itemset
tree because it is an incremental data structure that is designed for
live update and live targeted queries rather than batch processing.</p>
  <p>However, it is possible to load a transaction database in an
itemset tree. In this case, a file is loaded. The file is defined as a
text file where each line represents a transactions. Each item is
represented by an integer and it is assumed that all transactions are
sorted according to a total order and that no item can appear twice in
the same transaction. On any given line, the items of the corresponding
transaction are listed such that each item is separated from the
following item by a single space. For example, the file
"contextItemsetTree.txt" that is provided contains the following
content:</p>
  <blockquote>
    <p>1 4<br>
2 5<br>
1 2 3 4 5<br>
1 2 4<br>
2 5<br>
2 4</p>
  </blockquote>
  <p>There is a total of six transactions (six lines) in the file. The
first line represents the transaction {1, 4} (containing items 1 and
4). The second line represents the transaction {2, 5}. The third line
represents the transaction {1, 2, 3, 4, 5}. The following lines follow
the same format. </p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The itemset-tree is an efficient data structure for the case of a
database that needs to be updated frequently and where targeted queries
need to be performed. For details about the complexity in terms of
space and time, please refer to the article by Kubat et al., which
provides an extensive discussion of the complexity</p>
</blockquote>

<p>Where can I get more information about the Itemset-tree data
structure and related algorithms?</p>

<blockquote>
  <p>This article describes the itemset-tree and related algorithms for
querying it:</p>
  <p><em> Miroslav Kubat, Aladdin Hafez, Vijay V. Raghavan, Jayakrishna
R. Lekkala, Wei Kian Chen: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/ItemsetTree.pdf">Itemset
Trees for Targeted Association Querying</a>. IEEE Trans. Knowl. Data
Eng. 15(6): 1522-1534 (2003)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="meit" id="example5"> </a></span></strong>
Example 29 : Building, updating incrementally and using
a Memory Efficient Itemset-Tree to generate targeted frequent itemsets
and association rules.</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This algorithm is not offered in the release version of
SPMF.</strong></li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestMemoryEfficientItemsetTree.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is a Memory-Efficient Itemset-Tree (MEIT)?</p>

<blockquote>
  <p>An itemset-tree (IT) is a special data structure that can be used
for performing efficient queries about itemsets and association rules
in a transaction database without having to generate all of them
beforehand.</p>
  <p>An itemset-tree has the nice property of being incremental, which
means that new transactions can be added to an existing itemset tree
very efficiently without having to rebuild the tree from scratch. An
itemset-tree also has the property of being compact.</p>
  <p>The Memory-Efficient Itemset-Tree (MEIT) is a modification of the
original Itemset-Tree structure that uses about twice less memory than
the regular itemset-tree (see the paper describing MEIT for a
performance comparison). But it runs about twice slower. Therefore,
choosing between using an IT or MEIT is a trade-off between memory and
speed.</p>
</blockquote>

<p>How to use it?</p>

<blockquote>
  <p>A Memory-Efficient Itemset-Tree (MEIT) is built by inserting a set
of transactions into the tree. A <strong>transaction</strong> is
simply a set of distinct items. For example, we could insert the
following 6 transactions (t1,t2...t5) into an itemset-tree. In this
example, the transaction t1 represents the set of items {1, 4}. This
set of transactions is provided in the file "<strong><span class="Style2">contextItemsetTree.txt</span></strong>" of the SPMF
distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="495">

  <tbody>
    <tr>
      <td width="156"><strong>transaction IDs</strong></td>
      <td width="323"><strong>items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 4}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{2, 5}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 3, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{1, 2, 4}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{2, 5}</td>
    </tr>
    <tr>
      <td><strong>t6</strong></td>
      <td>{2, 4}</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The result of the insertion of these six transactions is the
following MEIT.</p>
  <pre>{}   sup=6<br>  [2 ]   sup=3<br>    [5 ]   sup=2<br>    [4 ]   sup=1<br>  [1 ]   sup=3<br>    [2 ]   sup=2<br>      [4 ]   sup=1<br>      [3 5 ]   sup=1<br>    [4 ]   sup=1</pre>
  <p>The root is the empty itemset {} and the leafs are {5}, {4},
{4},{3 5} and {4}.</p>
  <p>Once an itemset-tree has been created, it is possible to update it
by inserting a new transaction. For example, in this example provided
in the source code, we update the previous tree by adding a new
transaction {4, 5}. The result is this tree:</p>
  <pre>{}   sup=7<br>  [2 ]   sup=3<br>    [5 ]   sup=2<br>    [4 ]   sup=1<br>  [1 ]   sup=3<br>    [2 ]   sup=2<br>      [4 ]   sup=1<br>      [3 5 ]   sup=1<br>    [4 ]   sup=1<br>  [4 5 ]   sup=1</pre>
  <p>Next, it is shown how to query the tree to determine the support
of a target itemset efficiently. For example, if we execute the query
of finding the support of the itemset {2}, the support is determined to
be 5 because 2 appear in 5 transactions.</p>
  <p>After that the source code offers an example of how to use the
itemset tree to get all itemsets that subsume an itemset and to get
their support. For example, if we use the itemset {1 2} for this query
the result is:</p>
  <pre>[1 2 ]    supp:2<br>[1 2 3 ]    supp:1<br>[1 2 4 ]    supp:2<br>[1 2 5 ]    supp:1<br>[1 2 3 4 ]    supp:1<br>[1 2 3 5 ]    supp:1<br>[1 2 4 5 ]    supp:1<br>[1 2 3 4 5 ]    supp:1</pre>
  <p> Another example provided is how to use the tree to find all
itemsets that subsume an itemset such that the support is higher or
equal to a user-specified threshold named <em>minsup </em>(a positive
integer representing a number of transactions). For example, if we
execute this query with the itemset {1} and minsup =2, we get this
result: </p>
  <pre>[1 ]    supp:3<br>[1 2 ]    supp:2<br>[1 4 ]    supp:3<br>[1 2 4 ]    supp:2</pre>
  <p>Lastly, another example is how to generate all association rules
having a target itemset as antecedent and a support and confidence
respectively higher or equal to some user-specificed thresholds <em>minsup
  </em>(a positive integer representing a number of transactions) and <em>minconf
  </em>(a value between 0 and 1). For example, if the target itemset is
{1} and minconf = 0.1 and minsup = 2, the result is:</p>
  <pre>[ 1  ] ==&gt; [2  ]  sup=2  conf=0.666666666666666<br><br>[ 1  ] ==&gt; [4  ]  sup=3  conf=1.0<br><br>[ 1  ] ==&gt; [2 4  ]  sup=2  conf=0.66666666666666</pre>
</blockquote>

<p>Input and output file format</p>

<blockquote>
  <p>There is no need to use an input and output file with aa
memory-efficient itemset tree because it is an incremental data
structure that is designed for live update and live targeted queries
rather than batch processing.</p>
  <p>However, it is possible to load a transaction database in a
memory-efficient itemset tree. In this case, a file is loaded. The file
is defined as a text file where each line represents a transactions.
Each item is represented by an integer and it is assumed that all
transactions are sorted according to a total order and that no item can
appear twice in the same transaction. On any given line, the items of
the corresponding transaction are listed such that each item is
separated from the following item by a single space. For example, the
file "contextItemsetTree.txt" that is provided contains the following
content:</p>
  <blockquote>
    <p>1 4<br>
2 5<br>
1 2 3 4 5<br>
1 2 4<br>
2 5<br>
2 4</p>
  </blockquote>
  <p>There is a total of six transactions (six lines) in the file. The
first line represents the transaction {1, 4} (containing items 1 and
4). The second line represents the transaction {2, 5}. The third line
represents the transaction {1, 2, 3, 4, 5}. The following lines follow
the same format. </p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>The Memory-Efficient Itemset-Tree (MEIT) is an efficient data
structure for the case of a database that needs to be updated
frequently and where targeted queries need to be performed on itemsets
and association rules. </p>
  <p>The MEIT is a modification of the original Itemset-Tree (MEIT).
According to our experiments, the MEIT uses about twice less memory
than the IT but is about twice slower for answering queries. Therefore,
choosing between MEIT and IT is a compromise between speed and memory.</p>
</blockquote>

<p>Where can I get more information about the Itemset-tree data
structure and related algorithms?</p>

<blockquote>
  <p>This article describes the Memory-Efficient Itemset-tree:</p>
  <p> <em>Fournier-Viger, P., Mwamikazi, E., Gueniche, T., Faghihi, U.
(2013). <a href="http://www.philippe-fournier-viger.com/ADMA2013_Memory_Efficient_Itemset_Tree.pdf">Memory
Efficient Itemset Tree for Targeted Association Rule Mining</a>. Proc.
9th International Conference on Advanced Data Mining and Applications
(ADMA 2013) Part II, Springer LNAI 8347, pp. 95-106. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="msapriori" id="msapriori"> </a></span></strong>Example 30 : Mining Frequent Itemsets with Multiple Support
Thresholds Using the MSApriori Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">MSApriori</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>beta</em> = 0.4 and <em>LS</em> = 0.2 and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> MSApriori</span></strong><strong> contextIGB</strong>.txt
output.txt 0.4 0.2</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestMSApriori_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is MISApriori?</p>

<blockquote>
  <p>MISApriori is an algorithm for mining frequent itemsets by using
multiple minimum supports. It is a generalization of the Apriori
algorithm, which uses a single minimum support threshold. </p>
  <p>The idea behind MSApriori is that different minimum supports could
be used to consider the fact that some items are less frequent than
others in a dataset.</p>
</blockquote>

<p>What is the input of this algorithm?</p>

<blockquote>
  <p>The input of <strong>MSApriori</strong> is a transaction database
and two parameters named <em>beta </em>(a value between 0 and 1) and <em>LS</em>
(a value between 0 and 1). These parameters are used to determine a
minimum support for each item. </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 6 transactions
(t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For example, the
first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextIGB.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of this algorithm?</p>

<blockquote>
  <p>The output of <strong>MSApriori </strong>is the set of all
frequent itemsets contained in the database. </p>
  <p>Contrarily to the original Apriori algorithm, <strong>MSApriori</strong>
use <strong>multiple minimum supports thresholds</strong> instead of
just one. In fact, MSApriori uses a minimum support value for each
item. Because it would be time consuming to set a minimum support
threshold value for each item for a large database, the thresholds are
determined automatically by using two user-specified parameters named
beta (0 &lt;= B &lt;= 1) and LS (0 &lt;= LS &lt;= 1). </p>
  <p>The<strong> minimum support of an item <em>k</em></strong> is
then defined as the greatest value between:</p>
  <ul>
    <li>LS</li>
    <li>and B x f(<em>k</em>) where f(<em>k</em>) is the number of
transactions containing the item<em> k</em>.</li>
  </ul>
  <p>Note that if B is set to 0, there will be a single minimum support
for all items and this will be equivalent to the regular Apriori
algorithm.</p>
  <p>The <strong>support of an itemset</strong> is the number of
transactions containing the itemset divided by the total number of
transactions. An itemset is a frequent itemset if its support is higher
or equal to the smallest minimum support threshold from the minimum
support thresholds of all its items. </p>
  <p>Why MSApriori is useful? It is useful because it allows
discovering frequent itemsets containing rare items (if their minimum
support is set low).</p>
  <p>If we run <strong>MSApriori</strong> on the previous transaction
database with <em>beta = </em>0.4 and <em>LS </em>= 0.2, we obtain
the following result:</p>
  <pre>1 supp: 4<br>2 supp: 6<br>3 supp: 4<br>4 supp: 4<br>5 supp: 5<br>1 2  Support: 4<br>1 3  Support: 2<br>1 4  Support: 3<br>1 5  Support: 4<br>2 3  Support: 4<br>2 4  Support: 4<br>2 5  Support: 5<br>3 4  Support: 2<br>3 5  Support: 3<br>4 5  Support: 3<br>1 2 3  Support: 2<br>1 2 4  Support: 3<br>1 2 5  Support: 4<br>1 3 5  Support: 2<br>1 4 5  Support: 3<br>2 3 4  Support: 2<br>2 3 5  Support: 3<br>2 4 5  Support: 3<br>1 2 3 5  Support: 2<br>1 2 4 5  Support: <br></pre>
Note that here the support is expressed by an integer value which
represents the number of transactions containing the itemset. For
example, itemset {2, 3 5} has a support of 3 because it appears in
three transactions, namely t2, t4 and t5. This integer value can be
converted as a percentage by dividing by the total number of
transactions.</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>MSApriori </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. The items in the transaction are listed. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4, 5}. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>MSApriori </strong>is
defined as follows. It is a text file, where each line represents a <strong>frequent
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer, followed by a single
space. After, all the items, the keyword "#SUP:" appears, which is
followed by a integer value indicating the support of that itemset.</p>
  <p>1 #SUP: 4<br>
2 #SUP: 6<br>
3 #SUP: 4<br>
4 #SUP: 4<br>
5 #SUP: 5<br>
1 2 #SUP: 4<br>
1 3 #SUP: 2<br>
1 4 #SUP: 3<br>
1 5 #SUP: 4<br>
2 3 #SUP: 4<br>
2 4 #SUP: 4<br>
2 5 #SUP: 5<br>
3 4 #SUP: 2<br>
3 5 #SUP: 3<br>
4 5 #SUP: 3<br>
1 2 3 #SUP: 2<br>
1 2 4 #SUP: 3<br>
1 2 5 #SUP: 4<br>
1 3 5 #SUP: 2<br>
1 4 5 #SUP: 3<br>
2 3 4 #SUP: 2<br>
2 3 5 #SUP: 3<br>
2 4 5 #SUP: 3<br>
1 2 3 5 #SUP: 2<br>
1 2 4 5 #SUP: 3 </p>
  <p>For example, the first line indicates that the itemset {1} has a
support of 4 transactions. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>MSApriori</strong> is one of the first algorithm for
mining itemsets with multiple minimum support thresholds. It is not the
most efficient algorithm for this task because it is based on <strong>Apriori</strong>
and thus suffer from the same limitations. If performance is important,
it is recommend to use <strong>CFPGrowth</strong>++, which is based on
  <strong>FPGrowth</strong> and is more efficient.</p>
  <p>Note that there is one important difference between the input of
CFPGrowth++ and MSApriori in SPMF. The MISApriori works by setting the
multiple minimum supports by using the LS and BETA values. The
CFPGrowth++ implementation uses a list of minimum support values stored
in a text file instead.</p>
</blockquote>

<p>Where can I get more information about the MSApriori algorithm?</p>

<blockquote>
  <p>This article describes the <strong>MSApriori</strong> algorithm:</p>
  <p><em> B. Liu, W. Hsu, Y. Ma, "<a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/MISApriori.pdf">Mining
Association Rules with Multiple Minimum Supports</a>" Proceedings of
the ACM SIGKDD International Conference on Knowledge Discovery &amp;
Data Mining (KDD-99), August 15-18, 1999, San Diego, CA, USA.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="cfpgrowth" id="msapriori2">
</a></span></strong>Example 31 : Mining Frequent
Itemsets with Multiple Support Thresholds Using the CFPGrowth++
Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">CFPGrowth++</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextCFPGrowth</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) put "<strong>MIS.TXT</strong>" in the "MIS file name" text field
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CFPGrowth++</span></strong><strong> contextCFPGrowth</strong>.txt
output.txt MIS.txt </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextCFPgrowth.txt</span>
and <span class="Style2">MIS.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCFPGrowth_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is CFPGrowth++?</p>

<blockquote>
  <p><strong>CFPGrowth++</strong> is an algorithm for mining frequent
itemsets by using multiple minimum supports. It is an extension of the <strong>CFPGrowth</strong>
algorithm for mining frequent itemsets using multiple minimum support
thresholds. </p>
</blockquote>

<p>What is the input of this algorithm?</p>

<blockquote>
  <p>The input of<strong> CFPGrowth++ </strong>is a transaction
database and a list of minimum support thresholds indicating the
minimum support threshold for each item.</p>
  <p>A <strong>transaction database</strong> is a set of transactions,
where each transaction is a list of distinct items (symbols). For
example, let's consider the following transaction database. It consists
of 5 transactions (t1,t2...t6) and 8 items (1,2,3,4,5,6,7,8). For
instance, transaction t1 is the set of items {1, 3, 4, 6}. This
database is provided in the file "<strong>contextCFPGrowth.txt</strong>"
of the SPMF distribution<strong>.</strong>. It is important to note
that an item is not allowed to appear twice in the same transaction and
that items are assumed to be sorted by lexicographical order in a
transaction.</p>
</blockquote>

<table align="center" border="1" width="329">

  <tbody>
    <tr>
      <td width="17"><strong>Transaction ID</strong></td>
      <td width="55"><strong>items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 3, 4,6}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{1, 3, 5, 6, 7}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 3, 6, 8}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{2, 6, 7}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{2, 3}</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The list of minimum support threshold is stored in a text file
that is read as input by the algorithm. This is provided in the file "<strong>MIS.txt</strong>":</p>
</blockquote>

<table align="center" border="1" width="332">

  <tbody>
    <tr>
      <td><strong>item</strong></td>
      <td><strong>minimum support threshold</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <td>6</td>
      <td>3</td>
    </tr>
    <tr>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <td>8</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>This file indicated for example that the minimum support threshold
to be used for item 6 is 3.</p>
</blockquote>

<p>What is the output of this algorithm?</p>

<blockquote>
  <p>The output of <strong>CFPgrowth++ </strong>is the set of all
frequent itemsets contained in the database. </p>
  <p>What is a frequent itemset ? The <strong>support of an itemset</strong>
is the number of transactions containing the itemset. An itemset is a <strong>frequent
itemset</strong> if its support is higher or equal to the smallest
minimum support threshold among the minimum support thresholds of all
its items. For example, the itemset {1 2 8} is frequent because it
appears in one transactions (t3) and its support is higher than the
smallest minimum support among the minimum support of item 1, item 2
and item 8, which are respectively 1, 2 and 1.</p>
  <p>Why CFPGrowth++ is useful? It is useful because it permits setting
lower minimum support thresholds for rare items. Therefore, it allows
discovering frequent itemsets containing rare items. </p>
  <p>If we run <strong>CFPGrowth++ </strong> on the previous
transaction database with the <strong>MIS.txt</strong> file previously
described, we get the following result, where each line represents an
itemsets followed by ":" and then its absolute support.: </p>
  <pre>8:1<br>8 1:1<br>8 1 2:1 // for example, this itemset is {1, 2, 8}, and it has a support of 1.<br>8 1 2 6:1<br>8 1 2 6 3:1<br>8 1 2 3:1<br>8 1 6:1<br>8 1 6 3:1<br>8 1 3:1<br>8 2:1<br>8 2 6:1<br>8 2 6 3:1<br>8 2 3:1<br>8 6:1<br>8 6 3:1<br>8 3:1<br>1:3    // for example, this itemset is {1}, and it has a support of 3.<br>1 7:1<br>1 7 5:1  <br>1 7 5 6:1<br>1 7 5 6 3:1<br>1 7 5 3:1<br>1 7 6:1<br>1 7 6 3:1<br>1 7 3:1<br>1 5:1<br>1 5 6:1<br>1 5 6 3:1<br>1 5 3:1<br>1 2:1<br>1 2 6:1<br>1 2 6 3:1<br>1 2 3:1<br>1 6:3<br>1 6 4:1<br>1 6 4 3:1<br>1 6 3:3<br>1 4:1<br>1 4 3:1<br>1 3:3<br>7:2<br>7 6:2<br>2:3<br>2 6:2<br>2 3:2<br>6:4<br>6 3:3<br>3:4</pre>
  <strong>Note</strong>: If you are using the GUI version of SPMF the
file containing the minimum support must be located in the same folder
as the input file containing the transaction database.</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>CFPGrowth++ </strong>is
two files defined as follows. </p>
  <p>The <strong>first file</strong> (e.g. contextCFPGrowth.txt) It is
a text file containing the transactions. Each lines represents a
transaction. The items in the transaction are listed on the
corresponding line. An item is represented by a positive integer. Each
item is separated from the following item by a space. It is assumed
that items are sorted according to a total order and that no item can
appear twice in the same transaction. For example, for the previous
example, the input file is defined as follows:</p>
  <blockquote>
    <p>1 3 4 6<br>
1 3 5 6 7<br>
1 2 3 6 8<br>
2 6 7<br>
2 3</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 3, 4, 6}. The following lines follow the same format.</p>
  <p>The <strong>second file</strong> is a text file (e.g. MIS.txt)
which provides the minimum support to be used for each item. Each line
indicate the minimum support for an item and consists of two integer
values separated by a single space. The first value is the item. The
second value is the minimum support value to be used for this item. For
example, here is the file used in this example. The first line indicate
that for item "1" the minimum support to be used is 1 (one
transaction). The other lines follow the same format.</p>
  <blockquote>
    <p>1 1<br>
2 2<br>
3 3<br>
4 3<br>
5 2<br>
6 3<br>
7 2<br>
8 1 </p>
  </blockquote>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>CFPGrowth++ </strong>is
defined as follows. It is a text file, where each line represents a <strong>frequent
itemset</strong>. On each line, the items of the itemset are first
listed. Each item is represented by an integer, followed by a single
space. After, all the items, the keyword "#SUP:" appears, which is
followed by a integer value indicating the support of that itemset.</p>
  <p>8 #SUP: 1<br>
8 1 #SUP: 1<br>
8 1 2 #SUP: 1<br>
8 1 2 6 #SUP: 1<br>
8 1 2 6 3 #SUP: 1<br>
8 1 2 3 #SUP: 1<br>
8 1 6 #SUP: 1<br>
8 1 6 3 #SUP: 1<br>
8 1 3 #SUP: 1<br>
8 2 #SUP: 1<br>
8 2 6 #SUP: 1<br>
8 2 6 3 #SUP: 1<br>
8 2 3 #SUP: 1<br>
8 6 #SUP: 1<br>
8 6 3 #SUP: 1<br>
8 3 #SUP: 1<br>
1 #SUP: 3<br>
1 7 #SUP: 1<br>
1 7 5 #SUP: 1<br>
1 7 5 6 #SUP: 1<br>
1 7 5 6 3 #SUP: 1<br>
1 7 5 3 #SUP: 1<br>
1 7 6 #SUP: 1<br>
1 7 6 3 #SUP: 1<br>
1 7 3 #SUP: 1<br>
1 5 #SUP: 1<br>
1 5 6 #SUP: 1<br>
1 5 6 3 #SUP: 1<br>
1 5 3 #SUP: 1<br>
1 2 #SUP: 1<br>
1 2 6 #SUP: 1<br>
1 2 6 3 #SUP: 1<br>
1 2 3 #SUP: 1<br>
1 6 #SUP: 3<br>
1 6 4 #SUP: 1<br>
1 6 4 3 #SUP: 1<br>
1 6 3 #SUP: 3<br>
1 4 #SUP: 1<br>
1 4 3 #SUP: 1<br>
1 3 #SUP: 3<br>
7 #SUP: 2<br>
7 6 #SUP: 2<br>
2 #SUP: 3<br>
2 6 #SUP: 2<br>
2 3 #SUP: 2<br>
6 #SUP: 4<br>
6 3 #SUP: 3<br>
3 #SUP: 4 </p>
  <p>For example, the last line indicates that the itemset {4} has a
support of 4 transactions. The other lines follows the same format.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In the source code version of SPMF, there are two versions of
CFPGrowth: one that saves the result to a file (<span class="Style9">MainTestCFPGrowth_saveToFile.java</span>)
and one that saves the result to memory (<span class="Style9">MainTestCFPGrowth_saveToMemory.java</span>).
In the graphical interface and command line interface, only the version
that saves to file is offered.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>CFPGrowth++</strong> is a very efficient algorithm. It is
based on <strong>FPGrowth</strong>.</p>
  <p>SPMF also offers the MISApriori algorithm, which is less efficient
than CFPGrowth++. Note that there is one important difference between
the input of CFPGrowth++ and MSApriori in SPMF. The MISApriori
algorithm works by setting the multiple minimum supports by using some
special parameters named LS and BETA (see the example describing
MISApriori for more details). The CFPGrowth++ implementation instead
uses a list of minimum support values stored in a text file.</p>
</blockquote>

<p>Where can I get more information about the CFPGrowth++ algorithm?</p>

<blockquote>
  <p>This article describes the original CFPGrowth algorithm:</p>
  <p><em> Y.-H. Hu, Y.-L. Chen: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/CFPGrowth.pdf">Mining
association rules with multiple minimum supports: a new mining
algorithm and a support tuning mechanism</a>. Decision Support Systems
42(1): 1-24 (2006)</em></p>
  <p>This article describe CFPGrowth++, the extension of CFPGrowth that
is implemented in SPMF, which introduce a few additional optimizations.
  </p>
  <p> <em>Kiran, R. U., &amp; Reddy, P. K. (2011). <a href="http://www.edbt.org/Proceedings/2011-Uppsala/papers/edbt/a3-kiran.pdf" rel="nofollow">Novel techniques to reduce search space in multiple
minimum supports-based frequent pattern mining algorithms</a>. In
Proceedings of the 14th International Conference on Extending Database
Technology, ACM, pp. 11-20. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="twophase" id="example24"> </a></span></strong>
Example 32 : Mining High-Utility Itemsets from a
Database with Utility
Information with the Two-Phase Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Two-Phase</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Two-Phase </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTwoPhaseAlgorithm_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Two-Phase?</p>

<blockquote>
  <p><strong>Two-Phase </strong> (<a href="http://www.philippe-fournier-viger.com/spmf/two-phase-high-utility2005.pdf" rel="nofollow">Liu et al., 2005</a>)
is an algorithm for discovering<strong> high-utility itemsets </strong>in
a transaction database containing utility information.</p>
  <p><strong>High utility itemset mining</strong> has several
applications such as discovering groups of items in transactions of a
store that generate the most profit. A database containing utility
information is a database where items can have quantities and a unit
price. Although these algorithms are often presented in the context of
market basket analysis, there exist other applications.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>Two-phase</strong> takes as input a transaction database
with utility information and a <strong>minimum utility threshold</strong>
  <em>min_utility </em>(a positive integer)<em>.</em> Let's consider
the following database consisting of 5 transactions (t1,t2...t5) and 7
items (1, 2, 3, 4, 5, 6, 7). This database is provided in the text file
"<strong>DB_utility.txt</strong>" in the package ca.pfv.spmf.tests of
the SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="721">

  <tbody>
    <tr>
      <td width="56"> <br>
      </td>
      <td width="156"><strong>Items</strong></td>
      <td width="190"><strong>Transaction utility</strong></td>
      <td width="291"><strong>Item utilities for this transaction</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>3 5 1 2 4 6</td>
      <td>30</td>
      <td>1 3 5 10 6 5</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>3 5 2 4</td>
      <td>20</td>
      <td>3 3 8 6</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>3 1 4</td>
      <td>8</td>
      <td>1 5 2</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>3 5 1 7</td>
      <td>27</td>
      <td>6 6 10 5</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>3 5 2 7</td>
      <td>11</td>
      <td>2 3 4 2</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Each line of the <strong>database</strong> is:</p>
  <ul>
    <li> a set of items (the first column of the table), </li>
    <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
    <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
  </ul>
  <p>Note that the value in the second column for each line is the sum
of the values in the third column.</p>
  <p>What are real-life examples of such a database? There are several
applications in real life. One application is a customer transaction
database. Imagine that each transaction represents the items purchased
by a customer. The first customer named "<strong>t1</strong>" bought
items 3, 5, 1, 2, 4 and 6. The amount of money spent for each item is
respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount of
money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.<br>
  </p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> Two-Phase </strong>is the set of
  <strong>high utility itemset</strong>s having a utility no less than
a <em><strong>min_utility</strong></em> threshold (a positive integer)
set by the user. To explain what is a high utility itemset, it is
necessary to review some definitions. An <strong>itemset</strong> is
an unordered set of distinct items. The <strong>utility of an itemset
in a transaction </strong>is the sum of the utility of its items in
the transaction. For example, the utility of the itemset {1 4} in
transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction t3
is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>Two
Phase </strong>with a<strong> minimum utility of 30</strong>, we
obtain 8 <strong>high-utility itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>Two-phase</strong>
is defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of<strong> Two-phase </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword "#SUP:" appears, which
is followed by a double value indicating the support of the itemset.
Then, the keyword " #UTIL: " appears and is followed by the utility of
the itemset. For example, we show below the output file for this
example. </p>
  <p>2 4 #SUP: 0.4 #UTIL: 30<br>
2 5 #SUP: 0.6 #UTIL: 31<br>
1 3 5 #SUP: 0.4 #UTIL: 31<br>
2 3 4 #SUP: 0.4 #UTIL: 34<br>
2 3 5 #SUP: 0.6 #UTIL: 37<br>
2 4 5 #SUP: 0.4 #UTIL: 36<br>
2 3 4 5 #SUP: 0.4 #UTIL: 40<br>
1 2 3 4 5 6 #SUP: 0.2 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a support of 0.4 and a utility of 30. The following lines follows the
same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a much more
difficult problem than frequent itemset mining. Therefore, algorithms
for high-utility itemset mining are generally slower than frequent
itemset mining algorithms. </p>
  <p>The Two-Phase algorithm is an important algorithm because it
introduced the concept of mining <strong>high utility itemset </strong>by
using two phases by first overestimating the utility of itemsets in
phase I and then calculating their exact utility in phase II. However,
there are now some more efficient algorithms. For efficiency, it is
recommended to use a more efficient algorithm such as<strong> EFIM</strong>
that is also included in SPMF and is one of the most efficient
algorithm for this problem (see performance page of this website).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In the source code version of SPMF, there are two versions of
Two-Phase: one that saves the result to a file (<span class="Style9">MainTestTwoPhaseAlgorithm_saveToFile.java</span>)
and one that saves the result to memory (<span class="Style9">MainTestTwoPhaseAlgorithm_saveToMemory.java</span>).
In the graphical interface and command line interface, only the version
that saves to file is offered.</p>
  <p>Also note that the input format is not exactly the same as
described in the original article. But it is equivalent.</p>
</blockquote>

<p>Where can I get more information about the Two-Phase algorithm?</p>

<blockquote>
  <p>Here is an article describing the Two-Phase algorithm:</p>
  <p><em>Y. Liu, W.-K. Liao, A. N. Choudhary: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/two-phase-high-utility2005.pdf">A
Two-Phase Algorithm for Fast Discovery of High Utility Itemsets</a>.
PAKDD 2005: 689-695</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="fhm" id="example39"> </a></span></strong>
Example 33 : Mining High-Utility Itemsets from a
Database with Utility Information with the FHM Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FHM</span>"</strong> algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
(e.g. "<span class="Style2">output.txt</span>") (4) set the minimum
utility to 30 <span class="Style2"> </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FHM </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFHM.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is FHM?</p>

<blockquote>
  <p><strong>FHM </strong>(Fournier-Viger et al., ISMIS 2014) is an
algorithm for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. </p>
  <p><strong>High utility itemset mining </strong>has several
applications such as discovering groups of items in transactions of a
store that generate the most profit. A database containing utility
information is a database where items can have quantities and a unit
price. Although these algorithms are often presented in the context of
market basket analysis, there exist other applications.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>FHM</strong> takes as input a transaction database with
utility information and a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>.</em> Let's consider the following database
consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2, 3, 4, 5,
6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> FHM </strong>is the set of <strong>high
utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user. To explain what is a
high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>FHM </strong>with
a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>FHM </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>FHM </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. </p>
  <p><strong>The FHM algorithm </strong> was shown to be up to six
times faster than <strong>HUI-Miner</strong> (also included in SPMF),
especially for sparse datasets (see the performance section of the
website for a comparison). But the <strong>EFIM</strong> algorithm
(also included in SPMF) greatly outperforms FHM (see performance
section of the website).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version offered in SPMF is the original implementation of FHM.</p>
  <p>Note that the input format is not exactly the same as described in
the article. But it is equivalent.</p>
</blockquote>

<p>Where can I get more information about the FHM algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the FHM algorithm:</p>
  <p><em>Fournier-Viger, P., Wu, C.-W., Zida, S., Tseng, V. (2014) FHM:
A Faster High-Utility Itemset Mining Algorithm using Estimated Utility
Co-occurrence Pruning. Proc. 21st International Symposium on
Methodologies for Intelligent Systems (ISMIS 2014), Springer, LNAI, pp.
83-92</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="efim" id="example59"> </a></span></strong>
Example 34 : Mining High-Utility Itemsets from a
Database with Utility Information with the EFIM Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">EFIM</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> EFIM </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestEFIM_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is EFIM?</p>

<blockquote>
  <p><strong>EFIM </strong>(Zida et al., 2015) is an algorithm for
discovering<strong> high-utility itemsets </strong>in a transaction
database containing utility information. </p>
  <p><strong>High utility itemset mining </strong>has several
applications such as discovering groups of items in transactions of a
store that generate the most profit. A database containing utility
information is a database where items can have quantities and a unit
price. Although these algorithms are often presented in the context of
market basket analysis, there exist other applications.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>EFIM</strong> takes as input a transaction database with
utility information and a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>.</em> Let's consider the following database
consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2, 3, 4, 5,
6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> EFIM </strong>is the set of <strong>high
utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user. To explain what is a
high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>FHM </strong>with
a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>EFIM </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>EFIM </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. </p>
  <p><strong>The EFIM algorithm </strong> was shown to be<strong> up
to two orders of magnitude faster</strong> than the previous
state-of-the-art algorithm <strong>FHM, HUI-Miner, d2HUP, UPGrowth+ </strong>(also
included in SPMF), and consumes<strong> up to four times less memory </strong>(see
the performance section of the website for a comparison).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The implementation offered in SPMF is the original implementation
of <strong>EFIM</strong>.</p>
  <p>In the source code version of SPMF, there are two versions of
EFIM: one that saves the result to a file (<span class="Style9">MainTestEFIM_saveToFile.java</span>)
and one that saves the result to memory (<span class="Style9">MainTestEFIM_saveToMemory.java</span>).
In the graphical interface and command line interface, only the version
that saves to file is offered.</p>
  <p>Note that the input format is not exactly the same as described in
the article. But it is equivalent.</p>
</blockquote>

<p>Where can I get more information about the <strong>EFIM</strong>
algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the EFIM algorithm:</p>
  <p><em>Zida, S., Fournier-Viger, P., Lin, J. C.-W., Wu, C.-W., Tseng,
V.S. (2015). </em><a href="http://www.philippe-fournier-viger.com/spmf/MICAI2015_EFIM_High_Utility_Itemset_Mining.pdf"><em>EFIM: A
Highly Efficient Algorithm for High-Utility Itemset Mining</em></a><em>.
Proceedings of the 14th Mexican Intern. Conference on Artificial
Intelligence (MICAI 2015), Springer LNAI, to appear.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="huiminer" id="example40"> </a></span></strong>
Example 35 : Mining High-Utility Itemsets from a
Database with Utility Information with the HUI-Miner Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HUI-Miner</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> HUI-Miner </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestHUIMiner.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is HUI-Miner?</p>

<blockquote>
  <p><strong>HUI-Miner </strong> (Liu &amp; Qu, CIKM 2012) is an
algorithm for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. </p>
  <p><strong>High utility itemset mining </strong>has several
applications such as discovering groups of items in transactions of a
store that generate the most profit. A database containing utility
information is a database where items can have quantities and a unit
price. Although these algorithms are often presented in the context of
market basket analysis, there exist other applications.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>HUI-Miner</strong> takes as input a transaction database
with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> HUI-Miner </strong>is the set of
  <strong>high utility itemset</strong>s having a utility no less than
a <em><strong>min_utility</strong></em> threshold (a positive integer)
set by the user. To explain what is a high utility itemset, it is
necessary to review some definitions. An <strong>itemset</strong> is
an unordered set of distinct items. The <strong>utility of an itemset
in a transaction </strong>is the sum of the utility of its items in
the transaction. For example, the utility of the itemset {1 4} in
transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction t3
is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>HUI-Miner
  </strong>with a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>HUI-Miner </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>HUI-Miner </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. The <strong>HUI-Miner </strong>algorithm is reported as
one of the most efficient algorithm for <strong>high utility itemset
mining</strong>. However, recently <strong>the FHM algorithm </strong>(also
included in SPMF) was shown to be up to six times faster than
HUI-Miner, especially for sparse datasets (see the performance section
of the website for a comparison). More recently, the <strong>EFIM</strong>
algorithm (2015) was proposed and was shown to outperform FHM (2014),
HUI-Miner (2012), HUP-Miner (2014). All these algorithms are offered in
SPMF (see "performance" page of this website).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version implemented here contains all the optimizations
described in the paper proposing HUI-Miner. Note that the input format
is not exactly the same as described in the original article. But it is
equivalent.</p>
</blockquote>

<p>Where can I get more information about the HUI-Miner algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the HUI-Miner
algorithm:</p>
  <p><em>M. Liu, J.-F. Qu: Mining high utility itemsets without
candidate generation. CIKM 2012, 55-64</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="hupminer" id="example57"> </a></span></strong>
Example 36 : Mining High-Utility Itemsets from a
Database with Utility Information with the HUP-Miner Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HUP-Miner</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> HUP-Miner </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 2 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestHUPMiner.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is HUP-Miner?</p>

<blockquote>
  <p><strong>HUP-Miner </strong> (Krishnamoorthy, 2014) is an
extension of the <strong>HUI-Miner </strong>algorithm (Liu &amp; Qu,
CIKM 2012) for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. It introduces the
idea of partitioning the database and another pruning strategy named
LA-prune. A drawback of HUP-Miner is that the user needs to set an
additional parameter, which is the number of partitions. Moreover,
according to our experiments, HUP-Miner is faster than HUI-Miner but
slower than FHM.</p>
  <p><strong>High utility itemset mining </strong>has several
applications such as discovering groups of items in transactions of a
store that generate the most profit. A database containing utility
information is a database where items can have quantities and a unit
price. Although these algorithms are often presented in the context of
market basket analysis, there exist other applications.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>HUP-Miner</strong> takes as input a transaction database
with utility information, a minimum utility threshold <em>min_utility </em>(a
positive integer) and a number of partitions <em>k</em>.</p>
  <p>Note that the parameter <em>k</em> determines how much partitions
HUP-Miner uses internally, which influence the performance of HUP-Miner
but has no effect on the output of the algorithm. A typical value for <em>k
  </em>could be 10. However, the optimal value for <em>k</em> may be
found empirically for each dataset.</p>
  <p>Let's consider the following database consisting of 5 transactions
(t1,t2...t5) and 7 items (1, 2, 3, 4, 5, 6, 7). This database is
provided in the text file "<strong>DB_utility.txt</strong>" in the
package <strong>ca.pfv.spmf.tests </strong>of the SPMF distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> HUP-Miner </strong>is the set of
  <strong>high utility itemset</strong>s having a utility no less than
a <em><strong>min_utility</strong></em> threshold (a positive integer)
set by the user. To explain what is a high utility itemset, it is
necessary to review some definitions. An <strong>itemset</strong> is
an unordered set of distinct items. The <strong>utility of an itemset
in a transaction </strong>is the sum of the utility of its items in
the transaction. For example, the utility of the itemset {1 4} in
transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction t3
is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>HUP-Miner
  </strong>with a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
  <p> </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>HUP-Miner </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>HUP-Miner </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. The <strong>HUI-Miner </strong>algorithm was reported as
one of the most efficient algorithm for <strong>high utility itemset
mining</strong>. HUP-Miner is an extension of HUI-Miner, just like FHM.
These two latter are faster than HUI-Miner. However, HUP-Miner
introduce a new parameter which is the number of partitions. In our
experiment, FHM is faster than HUP-Miner. More recently, the <strong>EFIM</strong>
algorithm (2015) was proposed and was shown to outperform HUP-Miner,
and other recent algorithms such as FHM (2014), HUI-Miner (2012),
HUP-Miner (2014). All these algorithms are offered in SPMF (see
"performance" page of this website).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version implemented here contains all the optimizations
described in the paper proposing HUP-Miner. Note that the input format
is not exactly the same as described in the original article. But it is
equivalent.</p>
</blockquote>

<p>Where can I get more information about the HUP-Miner algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the HUP-Miner
algorithm:</p>
  <p><em>Krishnamoorthy, S. (2014). Pruning Strategies for Mining
High-Utility Itemsets. Expert Systems with Applications. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="upgrowth" id="example48"> </a></span></strong>
Example 37 : Mining High-Utility Itemsets from a
Database with Utility Information with the UP-Growth / UPGrowth+
Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">UPGrowth</span>"</strong> or <strong>"<span class="Style9">UPGrowth+</span>"</strong> algorithm<strong>, </strong>
(2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
(e.g. "<span class="Style2">output.txt</span>") (4) set the minimum
utility to 30 <span class="Style2"> </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> UPGrowth </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestUPGrowth.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is UPGrowth?</p>

<blockquote>
  <p><strong>UP-Growth </strong> (Tseng et al., KDD 2010) is an
algorithm for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. <strong>UP-Growth+
  </strong> (Tseng et al., KDD 2012) is an improved version. </p>
  <p>Those two algorithms are important algorithms because they
introduce some interesting ideas. However, recently some more efficient
algorithms have been proposed such as <strong>FHM</strong> (2014) and <strong>HUI-Miner</strong>
(2012). These latter algorithms were shown to be more than 100 times
faster than UP-Growth+ in some cases, and are also offered in SPMF.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>UP-Growth</strong> takes as input a transaction database
with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> UP-Growth </strong>is the set of
  <strong>high utility itemset</strong>s having a utility no less than
a <em><strong>min_utility</strong></em> threshold (a positive integer)
set by the user. To explain what is a high utility itemset, it is
necessary to review some definitions. An <strong>itemset</strong> is
an unordered set of distinct items. The <strong>utility of an itemset
in a transaction </strong>is the sum of the utility of its items in
the transaction. For example, the utility of the itemset {1 4} in
transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction t3
is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>UP-Growth
  </strong>with a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>UP-Growth </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>UP-Growth </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. The <strong>UP-Growth (2010) </strong>algorithm was the
fastest algorithm for <strong>high-utility itemset mining</strong> in
2010. However, more efficient algorithm have been proposed. The <strong>HUI-Miner
(2012) </strong>was shown to be up to 100 times faster than UPGrowth,
and more recently <strong>the FHM algorithm (2014)</strong> was shown
to be up to six times faster than HUI-Miner. More recently, the <strong>EFIM</strong>
algorithm (2015) was proposed and was shown to outperform UPGrowth+ and
other recent algorithms such as FHM (2014), HUI-Miner (2012), HUP-Miner
(2014). All these algorithms are offered in SPMF (see "performance"
page of this website).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version implemented here contains all the optimizations
described in the paper proposing UP-Growth (strategies DGU, DGN, DLU
and DLN). Note that the input format is not exactly the same as
described in the original article. But it is equivalent.</p>
</blockquote>

<p>Where can I get more information about the UP-Growth algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the UP-Growth
algorithm:</p>
  <p><em>V S. Tseng, C.-W. Wu, B.-E. Shie, P. S. Yu: UP-Growth: an
efficient algorithm for high utility itemset mining. KDD 2010: 253-262</em></p>
  <p><em>V. S. Tseng, B.-E. Shie, C.-W. Wu, and P. S. Yu. Efficient
algorithms for mining high utility itemsets from transactional
databases. IEEE Transactions on Knowledge and Data Engineering, 2012,
doi: 10.1109/TKDE.2012.59.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="ihup" id="example45"> </a></span></strong>
Example 38 : Mining High-Utility Itemsets from a
Database with Utility Information with the IHUP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">IHUP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> IHUP </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestIHUP.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is IHUP?</p>

<blockquote>
  <p><strong>IHUP </strong> (Ahmed et al., TKDE 2009) is an algorithm
for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. </p>
  <p>Note that the original IHUP algorithm is designed to be
incremental. In this implementation of IHUP can only be run in batch
mode.</p>
  <p>Also note that more efficient algorithm have been recently
proposed such as FHM (2014) and HUI-Miner (2012). These latter
algorithms outperforms IHUP by more than an order of magnitude, and are
also offered in SPMF.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>IHUP</strong> takes as input a transaction database with
utility information and a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>.</em> Let's consider the following database
consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2, 3, 4, 5,
6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> IHUP </strong>is the set of <strong>high
utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user. To explain what is a
high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>IHUP </strong>with
a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
      <td>20 % (1 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>IHUP </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>IHUP</strong>
is defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <strong>IHUP (2009) </strong>algorithm
was the fastest algorithm for high-utility itemset mining in 2009.
However, more efficient algorithm have been recently proposed. <strong>UPGrowth
  </strong>(2010) is an improved version of IHUP. The <strong>HUI-Miner
(2012) </strong>algorithm outperforms UPGrowth (2009) by more than an
order of magnitude, and more recently <strong>the FHM algorithm (2014)</strong>
was shown to be up to six times faster than HUI-Miner. More recently,
the <strong>EFIM</strong> algorithm (2015) was proposed and was shown
to outperform IHUP, and other recent algorithms such as FHM (2014),
HUI-Miner (2012), HUP-Miner (2014). All these algorithms are offered in
SPMF (see "performance" page of this website).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version of IHUP implemented here is designed to be run in
batch mode rather than as an incremental algorithm. Besides, note that
the input format is not exactly the same as described in the original
article. But it is equivalent.</p>
</blockquote>

<p>Where can I get more information about the IHUP algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the IHUP algorithm:</p>
  <p><em>C. F. Ahmed, S. K. Tanbeer, B.-S. Jeong, Y.-K. Lee: Efficient
Tree Structures for High Utility Pattern Mining in Incremental
Databases. IEEE Trans. Knowl. Data Eng. 21(12): 1708-1721 (2009)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="d2hup" id="example58"> </a></span></strong>
Example 39 : Mining High-Utility Itemsets from a
Database with Utility Information with the d2HUP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">D2HUP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> D2HUP </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestD2HUP.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>d2HUP</strong>?</p>

<blockquote>
  <p><strong>d2HUP </strong> (Liu et al., ICMD 2012) is an algorithm
for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. </p>
  <p>It is an algorithm that was shown to be more efficient than
UPGrowth and Two-Phase. But in the paper describing d2HUP, the
performance was not compared with some recent algorithms such as FHM
(2014), HUI-Miner (2012), HUP-Miner (2014).</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>d2HUP</strong> takes as input a transaction database with
utility information and a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>.</em> Let's consider the following database
consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2, 3, 4, 5,
6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> d2HUP </strong>is the set of <strong>high
utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user. To explain what is a
high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>d2HUP </strong>with
a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="226">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
    </tr>
    <tr>
      <td>{1 2 3 4 5 6}</td>
      <td>30</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>d2HUP </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>d2HUP</strong>
is defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <strong>d2HUP (2012) </strong>algorithm
was proposed in 2012 to discover high-utility itemsets without
maintaining candidates. A similar idea to avoid candidates was proposed
in HUI-Miner (2012) at about the same time. In this implementation, we
have implemented d2HUP with all the proposed optimizations. In the
paper describing <strong>d2HUP</strong>, this latter was shown to be
more efficient than UPGrowth and Two-Phase. Recently, the <strong>EFIM</strong>
algorithm was proposed (also offered in SPMF). EFIM is shown to
outperform d2HUP, and other recent algorithms such as FHM (2014),
HUI-Miner (2012), HUP-Miner (2014).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>Note that the input format is not exactly the same as described in
the original article. But it is equivalent.</p>
  <p>We have implemented the CAUL structure using pseudo-projections as
suggested in the paper. Also, the </p>
</blockquote>

<p>Where can I get more information about the d2HUP algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the d2HUP
algorithm:</p>
  <p><em>Liu, J., Wang, K., Fung, B. (2012). Direct discovery of high
utility itemsets without candidate generation. Proceedings of the 2012
IEEE 12th International Conference on Data Mining. IEEE Computer
Society, 2012.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="fhmplus" id="example49"> </a></span></strong> Example 40 : Mining High-Utility Itemsets from a Transaction Database with Utility Information while considering Length Constraints, using the FHM+ algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style9">FHM+</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
    (e.g. "<span class="Style2">output.txt</span>") (4) set the minimum
    utility to 30, (5) set the minimum length to 2, (6) set the maximum length to 3, <span class="Style2"> </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FHM+ </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
      output.txt 30 2 3  </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    the file <strong><span class="Style2">"MainTestFHMPlus.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is FHM+?</p>
<blockquote>
  <p><strong>FHM+ </strong>(Fournier-Viger et al., IEA AIE 2016) is an
    algorithm for discovering<strong> high-utility itemsets </strong>in a
    transaction database containing utility information. It extends the FHM algorithm by letting the user specify length constraints to find only patterns having a minimum and maximum size (length), and  use novel optimizations to mine patterns with these constraints efficiently.  Using constraints on the length of itemsets is useful because it not only reduce the number of patterns found but also can make the algorithm more than 10 times faster using the novel optimization called Length Upper-Bound Reduction.</p>
  <p><strong>High utility itemset mining </strong>has several
    applications such as discovering groups of items in transactions of a
    store that generate the most profit. A database containing utility
    information is a database where items can have quantities and a unit
    price. Although these algorithms are often presented in the context of
    market basket analysis, there exist other applications.</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
  <p><strong>FHM+</strong> takes as input a transaction database with
    utility information, a minimum utility threshold <em>min_utility </em>(a
    positive integer)<em>, a minimum pattern length </em>(a positive number), and a 	<em>maximum pattern length</em> (a positive integer)<em>.</em> Let's consider the following database
    consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2, 3, 4, 5,
    6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
    in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
    distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"><br></td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
        transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
        generated by this item for this transaction)(the third column of the
        table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
      sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
      several applications in real life. One application is a customer
      transaction database. Imagine that each transaction represents the
      items purchased by a customer. The first customer named "<strong>t1</strong>"
      bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
      item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
      of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p><em> </em>The output of<strong> FHM+ </strong>is the set of <strong>high
    utility itemset</strong>s having a utility no less than the <em><strong>min_utility</strong></em> threshold (a positive integer), and containing a number of items that is no less than the minimum pattern length and no greater the maximum pattern length, set by the user. To explain what is a
    high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong> is an unordered set of distinct items. The <strong>utility of an
      itemset in a transaction </strong>is the sum of the utility of its
    items in the transaction. For example, the utility of the itemset {1 4}
    in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
    t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong> is the sum of its utility in all transactions where it appears. For
    example, the utility of {1 4} in the database is the utility of {1 4}
    in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong> is an itemset such that its utility is no
    less than <em>min_utility</em> For example, if we run <strong>FHM+ </strong>with
    a<strong> minimum utility of 30</strong>, a minimum length of 2 items, and a maximum length of 3 items, we obtain 6 <strong>high-utility
      itemsets</strong> respecting these constraints</p>
</blockquote>
<table align="center" border="1" width="411">
  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
  </tbody>
</table>
<blockquote>
  <p>If the database is a transaction database from a store, we could
    interpret these results as all the groups of items bought together that
  generated a profit of 30 $ or more, and that contain at least 2 items, and no more than 3 items.. </p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> of <strong>FHM+ </strong>is
    defined as follows. It is a text file. Each lines represents a
    transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
      item is represented by a positive integer. Each item is separated from
      the next item by a single space. It is assumed that all items within a
      same transaction (line) are sorted according to a total order (e.g.
      ascending order) and that no item can appear twice within the same
      transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
      transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
      each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
      3 5 2 4:20:3 3 8 6<br>
      3 1 4:8:1 5 2<br>
      3 5 1 7:27:6 6 10 5<br>
      3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
    2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
    respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
    transaction. The following lines follow the same format.</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> of <strong>FHM+ </strong>is
    defined as follows. It is a text file, where each line represents a <strong>high
      utility itemset</strong>. On each line, the items of the itemset are
    first listed. Each item is represented by an integer, followed by a
    single space. After, all the items, the keyword " #UTIL: " appears and
    is followed by the utility of the itemset. For example, we show below
    the output file for this example. </p>
  <p>1 3 5 #UTIL: 31<br>
    2 4 #UTIL: 30<br>
    2 5 #UTIL: 31<br>
    2 3 4 #UTIL: 34<br>
    2 3 5 #UTIL: 37<br>
  2 4 5 #UTIL: 36</p>
  <p>For example, the first line indicates that the itemset {2, 4} has
    a utility of 30. The following lines follows the same format.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
    problem than frequent itemset mining. Therefore, high-utility itemset
    mining algorithms are generally slower than frequent itemset mining
    algorithms. </p>
  <p><strong>The FHM algorithm </strong> was shown to be up to six
    times faster than <strong>HUI-Miner</strong> (also included in SPMF),
    especially for sparse datasets (see the performance section of the
    website for a comparison). The <strong>FHM+ algorithm</strong> is an optimized version of FHM for efficiently discovering high utility itemsets whe length constraints are used. It can be more than 10 times faster than FHM when length constraints are applied, thanks to a novel technique called Length Upper-bound Reduction.</p>
</blockquote>
<p>Implementation details</p>
<blockquote>
  <p>The version offered in SPMF is the original implementation of FHM+.</p>
  <p>Note that the input format is not exactly the same as described in
    the article. But it is equivalent.</p>
</blockquote>
<p>Where can I get more information about the FHM+ algorithm?</p>
<blockquote>
  <p>This is the reference of the article describing the FHM+ algorithm:</p>
  <p><em> Fournier-Viger, P., Lin, C.W., Duong, Q.-H., Dam, T.-L. (2016). FHM+: Faster High-Utility Itemset Mining using Length Upper-Bound Reduction . Proc. 29th Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2016), Springer LNAI, to appear</em></p>
</blockquote>
<h3><strong><span class="centered"><a name="fchm" id="example68"> </a></span></strong> Example 41 : Mining Correlated High-Utility Itemsets in a
  Database with Utility Information with the FCHM Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style9">FCHM</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
    (4) set the minimum utility to 30 and the minbond parameter to 0.5 <span class="Style2"> </span>and
    (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FHM </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
      output.txt 30 0.5 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    the file <strong><span class="Style2">"MainTestFCHM.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is <strong>FCHM</strong>?</p>
<blockquote>
  <p><strong>FCHM </strong> (Fournier-Viger et al., 2016)) is an algorithm
    for discovering<strong> correlated high-utility itemsets </strong>in a
    transaction databases containing utility information. </p>
  <p>A limitation of traditional high utility itemset mining algorithms is that they may find many itemsets having a high utility but containing items that are weakly correlated (as shown in the FCHM paper). The FCHM addresses this issue by combining the idea of correlated pattern with high-utility pattern, to find high-utility itemsets where items are highly correlated. FCHM uses the bond measure to evaluate whether an itemset is a correlated itemset.</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
  <p><strong>FCHM</strong> takes as input a transaction database with
    utility information, a minimum utility threshold <em>min_utility </em>(a
    positive integer)<em>, </em>and a <em>minbond</em> threshold (a double number<em> in the </em>[0,1] interval)<em>.</em> Let's consider the following database
    consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2, 3, 4, 5,
    6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
    in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
    distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"><br></td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
        transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
        generated by this item for this transaction)(the third column of the
        table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
      sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
      several applications in real life. One application is a customer
      transaction database. Imagine that each transaction represents the
      items purchased by a customer. The first customer named "<strong>t1</strong>"
      bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
      item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
      of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p><em> </em>The output of<strong> FCHM </strong>is the set of <strong>correlated high
    utility itemset</strong>s having a <strong>utility</strong> no less than a <em><strong>min_utility</strong></em> threshold (a positive integer) set by the user, and a <em><strong>bond</strong></em> no less than a 	<strong><em>minbond</em></strong> threshold also set by the user. </p>
  <p>To explain what is a
    correlated high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong> is an unordered set of distinct items. The <strong>utility of an
      itemset in a transaction </strong>is the sum of the utility of its
    items in the transaction. For example, the utility of the itemset {1 4}
    in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
    t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong> is the sum of its utility in all transactions where it appears. For
    example, the utility of {1 4} in the database is the utility of {1 4}
    in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong> is an itemset such that its utility is no
    less than <em>min_utility.</em></p>
  <p>A<strong> correlated itemset</strong> is an itemset such that its <em>bond </em>is no less than a <em>minbond</em> threshold set by the user<em>.</em> The <em>bond </em>of an itemsets is the number of transactions
containing the itemset divided by the number of transactions containing
any of its items. The bond is a value in the [0,1] interval. A high
value means a highly correlated itemset. Note that single items have by
default a bond of 1. A<strong> correlated high-utility itemset</strong>  is a high-utility itemset that is also a correlated itemset.</p>
  <p> For example, if we run <strong>FHM </strong>with
    a<strong> minimum utility of 30 </strong>and<strong> minbond = 0.5</strong>, we obtain 3 <strong>correlated high-utility
      itemsets</strong>:</p>
</blockquote>
<table align="center" border="1" width="329">
  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>bond</strong></td>
      <td width="97"><strong>utility</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>0.5</td>
      <td>30</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>0.75</td>
      <td>31</td>
    </tr>
    <tr>
      <td>{2 5 3}</td>
      <td>0.6</td>
      <td>37</td>
    </tr>
  </tbody>
</table>
<blockquote>
  <p>If the database is a transaction database from a store, we could
    interpret these results as all the groups of items bought together that
  generated a profit of 30 $ or more, and containing items that are correlated (are likely to be bought together). </p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> of <strong>FCHM </strong>is
    defined as follows. It is a text file. Each lines represents a
    transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
      item is represented by a positive integer. Each item is separated from
      the next item by a single space. It is assumed that all items within a
      same transaction (line) are sorted according to a total order (e.g.
      ascending order) and that no item can appear twice within the same
      transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
      transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
      each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
      3 5 2 4:20:3 3 8 6<br>
      3 1 4:8:1 5 2<br>
      3 5 1 7:27:6 6 10 5<br>
      3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
    2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
    respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
    transaction. The following lines follow the same format.</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> of <strong>FCHM</strong> is defined as follows. It is a text file, where each line represents a <strong>correlated high
    utility itemset</strong>. On each line, the items of the itemset are
    first listed. Each item is represented by an integer, followed by a
    single space. After, all the items, the keyword " #UTIL: " appears and
    is followed by the utility of the itemset. Then, there is a single space, followed by the keyword "#BOND: ", followed by the bond of the itemset. For example, we show below
    the output file for this example. </p>
  <p>4 2 #UTIL: 30 #BOND: 0.5 <br>
  2 5 #UTIL: 31 #BOND: 0.75 <br>
  2 5 3 #UTIL: 37 #BOND: 0.6 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
    a utility of 30 and a bond of 0.5. The following lines follows the same format.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
    problem than frequent itemset mining. Therefore, <strong>high-utility
      itemset mining</strong> algorithms are generally slower than frequent
    itemset mining algorithms. The FCHM algorithm is the first algorithm for mining correlated high-utility itemsets using the bond measure. It extends FHM, one of the fastest algorithm for high-utility itemsets mining.</p>
</blockquote>
<p>Implementation details</p>
<blockquote>
  <p>Note that the input format is not exactly the same as described in
    the original article. But it is equivalent.</p>
</blockquote>
<p>Where can I get more information about the FCHM algorithm?</p>
<blockquote>
  <p>This is the reference of the article describing the FCHM
    algorithm:</p>
  <p><em>Fournier-Viger, P., Lin, C. W., Dinh, T., Le, H. B. (2016). <a href="http://www.philippe-fournier-viger.com/spmf/HAIS2016_Correlated_high_utility_itemsets_13pages.pdf">Mining Correlated High-Utility Itemsets Using the Bond Measure</a>. Proc. 11 th International Conference on Hybrid Artificial Intelligence Systems (HAIS 2016), Springer LNAI, 14 pages, to appear.</em></p>
</blockquote>
<h3><strong><span class="centered"><a name="fhmfreq" id="example67"> </a></span></strong>
  Example 42 : Mining Frequent High-Utility Itemsets from
  a Database with Utility Information with the FHMFreq Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FHMFreq</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30, (5) set the minimum support to 0.4, <span class="Style2"> </span>and (6) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FHM </span></strong><strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 0.4</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFHMFreq.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>FHMFreq</strong>?</p>

<blockquote>
  <p><strong>FHM </strong>(Fournier-Viger et al., ISMIS 2014) is an
algorithm for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. <strong>FHMFreq </strong>is
a simple extension of FHM for discovering <strong>frequent
high-utility itemsets </strong>(it combines frequent itemset mining
with high-utility itemset mining)<strong>.</strong></p>
  <p><strong>High utility itemset mining </strong>has several
applications such as discovering groups of items in transactions of a
store that generate the most profit. A database containing utility
information is a database where items can have quantities and a unit
price. Although these algorithms are often presented in the context of
market basket analysis, there exist other applications.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>FHMFreq</strong> takes as input a transaction database
with utility information, a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>, </em>and a minimum support threshold <em>minsup</em>
(a percentage value represented as a double in [0,1])<em>.</em> Let's
consider the following database consisting of 5 transactions
(t1,t2...t5) and 7 items (1, 2, 3, 4, 5, 6, 7). This database is
provided in the text file "<strong>DB_utility.txt</strong>" in the
package <strong>ca.pfv.spmf.tests </strong>of the SPMF distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> FHMFreq </strong>is the set of <strong>frequent
high utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user, and a support no less
than the <em><strong>minsup</strong></em> threshold also set by the
user. To explain what is a high utility itemset, it is necessary to
review some definitions. An <strong>itemset</strong> is an unordered
set of distinct items. The <strong>utility of an itemset in a
transaction </strong>is the sum of the utility of its items in the
transaction. For example, the utility of the itemset {1 4} in
transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction t3
is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. The
support of an itemset is the number of transactions containing the
itemset. For example, the support of itemset {1 4} is 2 transactions
because it appears in transactions t1 and t3. The support of an itemset
can also expressed as a percentage. For example, the support of itemset
{1 4} is said to be 40% (or 0.4) because it appears in 2 out of five
transactions in the database.</p>
  <p>A <strong>frequent high utility itemset</strong> is an itemset
such that its utility is no less than <em>min_utility</em> and that
its support is no less than the <em>minsup</em> threshold. For
example, if we run <strong>FHMFreq </strong>with a<strong> minimum
utility of 30 </strong>and a <strong>minimum support</strong> of 40
%, we obtain 7 <strong>high-utility itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="411">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="179"><strong> support</strong></td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>30</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{1 3 5}</td>
      <td>31</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>34</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 5}</td>
      <td>37</td>
      <td>60 % (3 transactions)</td>
    </tr>
    <tr>
      <td>{2 4 5}</td>
      <td>36</td>
      <td>40 % (2 transactions)</td>
    </tr>
    <tr>
      <td>{2 3 4 5}</td>
      <td>40</td>
      <td>40 % (2 transactions)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more, and appear in at least 2
transactions. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>FHMFreq </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>FHMFreq </strong>is
defined as follows. It is a text file, where each line represents a <strong>frequent
high utility itemset</strong>. On each line, the items of the itemset
are first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. After, the keyword " #SUP: "
appears and is followed by the support of the itemset. For example, we
show below the output file for this example. </p>
  <p>4 2 #UTIL: 30 #SUP: 2<br>
4 2 5 #UTIL: 36 #SUP: 2<br>
4 2 5 3 #UTIL: 40 #SUP: 2<br>
4 2 3 #UTIL: 34 #SUP: 2<br>
2 5 #UTIL: 31 #SUP: 3<br>
2 5 3 #UTIL: 37 #SUP: 3<br>
1 5 3 #UTIL: 31 #SUP: 2 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30 and a support of two transactions. The following lines
follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. </p>
  <p><strong>The <strong>FHMFreq</strong> </strong>algorithm here
described is a simple extension of the <strong>FHM</strong> algorithm
to add the minsup threshold as parmeter.</p>
  <p><strong>For high-utility itemset mining, the FHM algorithm </strong>
was shown to be up to six times faster than <strong>HUI-Miner</strong>
(also included in SPMF), especially for sparse datasets (see the
performance section of the website for a comparison). But the <strong>EFIM</strong>
algorithm (also included in SPMF) greatly outperforms FHM (see
performance section of the website). </p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version of FHMFreq offered in SPMF extends the original
implementation of FHM.</p>
  <p>Note that the input format is not exactly the same as described in
the article. But it is equivalent.</p>
</blockquote>

<p>Where can I get more information about the FHMFreq algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the FHM algorithm:</p>
  <p><em>Fournier-Viger, P., Wu, C.-W., Zida, S., Tseng, V. (2014) FHM:
A Faster High-Utility Itemset Mining Algorithm using Estimated Utility
Co-occurrence Pruning. Proc. 21st International Symposium on
Methodologies for Intelligent Systems (ISMIS 2014), Springer, LNAI, pp.
83-92</em></p>
  <p>The <em>FHMFreq</em> algorithm is a simple extension of that
algorithm.</p>
</blockquote>

<p> </p>

<h3><strong><span class="centered"><a name="fhn" id="example53"> </a></span></strong>
Example 43 : Mining High-Utility Itemsets from a
Database with Positive or Negative Unit Profit using the FHN Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FHN</span>"</strong> algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style9">DB_NegativeUtility.txt</span><span class="Style2">"</span></strong>, (3) set the output file name (e.g. "<span class="Style2">output.txt</span>") (4) set the minimum utility to 30 <span class="Style2"> </span>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FHN </span></strong><strong><span class="Style9">DB_NegativeUtility.txt
    </span></strong>output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_NegativeUtility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFHN_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>FHN</strong>?</p>

<blockquote>
  <p><strong>FHN </strong> (Fournier-Viger et al, 2014) is an
algorithm for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information. It is an extension
of the <strong>FHM</strong> algorithm designed for mining patterns in
a transaction database where items may have negative unit profit values.</p>
  <p>Items with negative values are interesting in real-life scenarios.
Often in a retail store, items may be sold at a loss. If traditional
high utility itemset mining algorithms such as Two-Phase, IHUP,
UPGrowth, HUI-Miner and FHM are appied on such database, it was
demonstrated that they may not discover the correct restults. To
address this issue, algorithms such as HUINIV-Mine and FHN were
proposed. At the time where FHN was proposed (2014), FHN is the
state-of-the-art algorithm for mining high-tility itemsets with both
positive and negative unit profit values.</p>
  <p>This is the original implementation of FHN.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>FHN</strong> takes as input a transaction database with
utility information and a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>.</em> Let's consider the following database
consisting of 10 transactions (t1,t2...t10) and 5 items (1, 2, 3, 4,
5). This database is provided in the text file "<strong>DB_NegativeUtility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>1 4 5</td>
        <td>27</td>
        <td>5 12 10</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>2 3 4</td>
        <td>36</td>
        <td>-3 -4 36</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>1 4</td>
        <td>45</td>
        <td>15 30</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>1 5</td>
        <td>15</td>
        <td>5 10</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>2 3 4</td>
        <td>36</td>
        <td>-3 -4 36</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>2 3 5</td>
        <td>20</td>
        <td>-3 -2 20</td>
      </tr>
      <tr>
        <td><strong>t7</strong></td>
        <td>1</td>
        <td>10</td>
        <td>10</td>
      </tr>
      <tr>
        <td><strong>t8</strong></td>
        <td>1 4</td>
        <td>21</td>
        <td>15 6</td>
      </tr>
      <tr>
        <td><strong>t9</strong></td>
        <td>2 3 4</td>
        <td>24</td>
        <td>-3 -2 24</td>
      </tr>
      <tr>
        <td><strong>t10</strong></td>
        <td>1 5</td>
        <td>15</td>
        <td>5 10</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is: </p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 1, 4 and 5. The amount of profit generated by the sale of
each of these item is respectively 5 $, 12 $ and 10 $. The total amount
of money spent in this transaction is 5 + 12 + 10 = 27 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> FHN </strong>is the set of <strong>high
utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user. To explain what is a
high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 12 = 17 and the utility of {1 4} in
transaction t3 is 15 + 30 = 45. The<strong> utility of an itemset in a
database</strong> is the sum of its utility in all transactions where
it appears. For example, the utility of {1 4} in the database is the
utility of {1 4} in t1 plus the utility of {1 4} in t3, plus the
utility of {1 4} in t8, for a total of 17 + 45 + 21 = 83. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>FHN </strong>with
a<strong> minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="226">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility ($)</strong></td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>50</td>
    </tr>
    <tr>
      <td>{1 5}</td>
      <td>45</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>55</td>
    </tr>
    <tr>
      <td>{1 4}</td>
      <td>83</td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>144</td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>87</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>77</td>
    </tr>
    <tr>
      <td>{3 4}</td>
      <td>86</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>FHN </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>1 4 5:27:5 12 10<br>
2 3 4:36:-3 -4 36<br>
1 4:45:15 30<br>
1 5:15:5 10<br>
2 3 4:36:-3 -4 36<br>
2 3 5:20:-3 -2 20<br>
1:10:10<br>
1 4:21:15 6<br>
2 3 4:24:-3 -2 24<br>
1 5:15:5 10 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {1, 4, 5}
has a total utility of 27 and that items 1, 4and 5 respectively have a
utility of 5, 12 and 10 in this transaction. The following lines follow
the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>FHN</strong>
is defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>5 #UTIL: 50<br>
5 1 #UTIL: 45<br>
1 #UTIL: 55<br>
1 4 #UTIL: 83<br>
4 #UTIL: 144<br>
4 2 #UTIL: 87<br>
4 2 3 #UTIL: 77<br>
4 3 #UTIL: 86<br>
  </p>
  <p> For example, the second line indicates that the itemset {1, 5}
has a utility of 45. The other lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <strong>FHN (2014) </strong>algorithm
is up to 100 times faster than HUINIV-Mine, the previous
state-of-the-art algorithm for high-utility itemset mining with
negative unit profit.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version of <strong>FHN</strong> in SPMF is the original
implementation.</p>
</blockquote>

<p>Where can I get more information about the FHN algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <strong>FHN</strong>
algorithm:</p>
  <p><em>Fournier-Viger, P. (2014). <a href="http://www.philippe-fournier-viger.com/FHN.pdf">FHN: Efficient
Mining of High-Utility Itemsets with Negative Unit Profits</a><a href="http://www.philippe-fournier-viger.com/ADMA2014_FHN_high_utility_itemsets_negative_profit.pdf">.</a>
Proc.
10th International Conference on Advanced Data Mining and Applications
(ADMA 2014), Springer LNCS 8933, pp. 16-29.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="huiniv" id="example54"> </a></span></strong>
Example 44 : Mining High-Utility Itemsets from a
Database with Positive or Negative Unit Profit using the HUINIV-Mine
Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HUINIV-Mine</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_NegativeUtility.txt</span><span class="Style2">"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> HUINIV-Mine</span></strong> <strong><span class="Style9">DB_NegativeUtility.txt </span></strong>output.txt 30 </span>
in a folder containing <span class="Style2">spmf.jar</span> and the
example input file <span class="Style2">DB_NegativeUtility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestHUINIVMine_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>HUINIV-Mine</strong>?</p>

<blockquote>
  <p><strong>HUINIV-Mine </strong> is an algorithm for discovering<strong>
high-utility itemsets </strong>in a transaction database containing
utility information. It is an extension of the <strong>Two-Phase</strong>
algorithm designed for mining patterns in a transaction database where
items may have negative unit profit values.</p>
  <p>Items with negative values are interesting in real-life scenarios.
Often in a retail store, items may be sold at a loss. If traditional
high utility itemset mining algorithms such as Two-Phase, IHUP,
UPGrowth, HUI-Miner and FHM are appied on such database, it was
demonstrated that they may not discover the correct restults. To
address this issue, the HUINIV-Mine algorithm was proposed. However,
faster algorithms now exists, such as <strong>FHN</strong>, also
offered in SPMF.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>HUINIV-Mine</strong> takes as input a transaction database
with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 10 transactions (t1,t2...t10) and 5 items (1, 2,
3, 4, 5). This database is provided in the text file "<strong>DB_NegativeUtility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>1 4 5</td>
        <td>27</td>
        <td>5 12 10</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>2 3 4</td>
        <td>36</td>
        <td>-3 -4 36</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>1 4</td>
        <td>45</td>
        <td>15 30</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>1 5</td>
        <td>15</td>
        <td>5 10</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>2 3 4</td>
        <td>36</td>
        <td>-3 -4 36</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>2 3 5</td>
        <td>20</td>
        <td>-3 -2 20</td>
      </tr>
      <tr>
        <td><strong>t7</strong></td>
        <td>1</td>
        <td>10</td>
        <td>10</td>
      </tr>
      <tr>
        <td><strong>t8</strong></td>
        <td>1 4</td>
        <td>21</td>
        <td>15 6</td>
      </tr>
      <tr>
        <td><strong>t9</strong></td>
        <td>2 3 4</td>
        <td>24</td>
        <td>-3 -2 24</td>
      </tr>
      <tr>
        <td><strong>t10</strong></td>
        <td>1 5</td>
        <td>15</td>
        <td>5 10</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is: </p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 1, 4 and 5. The amount of profit generated by the sale of
each of these item is respectively 5 $, 12 $ and 10 $. The total amount
of money spent in this transaction is 5 + 12 + 10 = 27 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> HUINIV-Mine</strong>is the set of
  <strong>high utility itemset</strong>s having a utility no less than
a <em><strong>min_utility</strong></em> threshold (a positive integer)
set by the user. To explain what is a high utility itemset, it is
necessary to review some definitions. An <strong>itemset</strong> is
an unordered set of distinct items. The <strong>utility of an itemset
in a transaction </strong>is the sum of the utility of its items in
the transaction. For example, the utility of the itemset {1 4} in
transaction t1 is 5 + 12 = 17 and the utility of {1 4} in transaction
t3 is 15 + 30 = 45. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, the utility of {1 4} in the database is the utility of {1 4}
in t1 plus the utility of {1 4} in t3, plus the utility of {1 4} in t8,
for a total of 17 + 45 + 21 = 83. A<strong> high utility itemset</strong>
is an itemset such that its utility is no less than <em>min_utility</em>
For example, if we run<strong> HUINIV-Mine </strong>with a<strong>
minimum utility of 30</strong>, we obtain 8 <strong>high-utility
itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="226">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility ($)</strong></td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>50</td>
    </tr>
    <tr>
      <td>{1 5}</td>
      <td>45</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>55</td>
    </tr>
    <tr>
      <td>{1 4}</td>
      <td>83</td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>144</td>
    </tr>
    <tr>
      <td>{2 4}</td>
      <td>87</td>
    </tr>
    <tr>
      <td>{2 3 4}</td>
      <td>77</td>
    </tr>
    <tr>
      <td>{3 4}</td>
      <td>86</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of<strong> HUINIV-Mine </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>1 4 5:27:5 12 10<br>
2 3 4:36:-3 -4 36<br>
1 4:45:15 30<br>
1 5:15:5 10<br>
2 3 4:36:-3 -4 36<br>
2 3 5:20:-3 -2 20<br>
1:10:10<br>
1 4:21:15 6<br>
2 3 4:24:-3 -2 24<br>
1 5:15:5 10 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {1, 4, 5}
has a total utility of 27 and that items 1, 4and 5 respectively have a
utility of 5, 12 and 10 in this transaction. The following lines follow
the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of<strong> HUINIV-Mine </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file for this example. </p>
  <p>5 #UTIL: 50<br>
5 1 #UTIL: 45<br>
1 #UTIL: 55<br>
1 4 #UTIL: 83<br>
4 #UTIL: 144<br>
4 2 #UTIL: 87<br>
4 2 3 #UTIL: 77<br>
4 3 #UTIL: 86<br>
  </p>
  <p> For example, the second line indicates that the itemset {1, 5}
has a utility of 45. The other lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <strong>HUINIV-Mine </strong>is the
first algorithm for high-utility itemset mining with negative unit
profit. However, faster algorithms have now been proposed such as FHN
(2014), also offered in SPMF.</p>
</blockquote>

<p>Where can I get more information about the HUINIV-Mine algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <strong>HUINIV-Mine</strong>
algorithm:</p>
  <p><em>Chu, Chun-Jung, Vincent S. Tseng, and Tyne Liang. "An
efficient algorithm for mining high utility itemsets with negative item
values in large databases." Applied Mathematics and Computation 215.2
(2009): 767-778.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="foshu" id="example60"> </a></span></strong>
Example 45 : Mining On-Shelf High-Utility Itemsets from
a Transaction Database using the FOSHU Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FOSHU</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_FOSHU.txt</span><span class="Style2">"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility ratio to 0.8 <span class="Style2"></span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FOSHU </span></strong><strong><span class="Style9">DB_FOSHU.txt
    </span></strong>output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_FOSHU.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFOSHU_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>FOSHU</strong>?</p>

<blockquote>
  <p><strong>FOSHU </strong> (Fournier-Viger et al, 2015) is an
algorithm for discovering<strong> high-utility itemsets </strong>in a
transaction database containing utility information and information
about the <strong>time periods where items</strong> are sold. The task
of<strong> on-shelf high-utility itemset mining </strong>is an
extension of the<strong> task of high utility </strong>i<strong>temset
mining</strong>.</p>
  <p>The FOSHU algorithm for on-shelf-high-utility itemset mining is
interesting because it addresses two limitations of high-utility
itemset mining algorithms. First, most algorithms cannot handle
databases where items may have negative unit profit/weight. But such
items often occur in real-life transaction databases. For example, it
is common that a retail store will sell items at a loss to stimulate
the sale of other related items or simply to attract customers to their
retail location. If classical HUIM algorithms are applied on database
containing items with negative unit profit, they can generate an
incomplete set of high-utility itemsets. Second, most algorithms
consider that items have the same shelf time, i.e. that all item are on
sale for the same time period. However, in real-life some items are
only sold during some short time period (e.g. the summer). Algorithms
ignoring the shelf time of items have a bias toward items having more
shelf time since they have more chance to generate a high profit.</p>
  <p>FOSHU is the state-of-the-art algorithm for on-shelf high-utility
itemset mining. It was shown to outperform TS-HOUN by up to three
orders of magnitude in terms of execution time.</p>
  <p>This is the original implementation of FOSHU.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>FOSHU</strong> takes as input a transaction database with
information about the utility of items and their shelf time time, and a
minimum utility threshold <em>min_utility ratio </em>(a positive
double value in the [0,1] interval)<em>.</em> For example, let's
consider the following database consisting of 5 transactions (t1,t2,
..., t5) and 7 items (1, 2, 3, 4, 5, 6, 7). This database is provided
in the text file "<strong>DB_FOSHU.txt</strong>" in the package <strong>ca.pfv.spmf.tests
  </strong>of the SPMF distribution<strong>.</strong></p>
  <table align="center" border="1" width="1018">
    <tbody>
      <tr>
        <td width="56"><strong>Transaction</strong></td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
        <td width="291"><strong>Time period</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>1 3 4</td>
        <td>3</td>
        <td>-5 1 2</td>
        <td>0</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>1 3 5 7</td>
        <td>17</td>
        <td>-10 6 6 5</td>
        <td>0</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>1 2 3 4 5 6</td>
        <td>25</td>
        <td>-5 4 1 12 3 5</td>
        <td>1</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>2 3 4 5</td>
        <td>20</td>
        <td>8 3 6 3</td>
        <td>1</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>2 3 5 7</td>
        <td>11</td>
        <td>4 2 3 2</td>
        <td>2</td>
      </tr>
    </tbody>
  </table>
  <p>Each line of the <strong>database</strong> represents a
transaction and contains the following information: </p>
  <ul>
    <li> a set of items (the second column of the table), </li>
    <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the third column of the table),</li>
    <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the fourth column of the
table).</li>
    <li>the time period where this transaction occurred (the fifth
column). </li>
  </ul>
  <p>Note that the value in the third column for each line is the sum
of the values in the second column. Moreoever, nNote that utility
values may be positive or negative integers. Time periods are values
numbered 0,1,2,3..., which may represent for example periods such as
"summer", "fall", "winter" and "spring".</p>
  <p>What are real-life examples of such a database? There are several
applications in real life. The main application is for customer
transaction databases. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 1, 3 and 4. The amount of profit generated by the sale of
each of these item is respectively -5 $, 1 $ and 2 $. The total amount
of money spent in this transaction is -5 + 1 + 2 = 3 $. This
transaction was done during time period "0", which may for example
represents the summer.</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> FOSHU </strong>algorithm is the
set of <strong>on-shelf high utility itemset</strong>s having a
relative utility no less than the<strong> <em>min_utility_ratio</em></strong>
threshold set by the user. To explain what is an on-shelf high utility
itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1,
3, 4} in transaction t1 is -5 + 1 + 2 = 3, and the utility of {1, 3, 4}
in transaction t3 is -5 + 1 + 12 = 7. The<strong> utility of an itemset
in a database</strong> is the sum of its utility in all transactions
where it appears. For example, the utility of {1, 3, 4} in the database
is the utility of {1, 3, 4} in t1 plus the utility of {1, 3, 4} in t3,
for a total of 3 + 7 = 10. The <strong>relative utility of an itemset </strong>is
the utility of that itemset divided by the sum of the transaction
utilities for the time period where the itemset was sold. For example,
itemset {1, 3, 4} was sold in time periods "0" and "1". The total
utility of time period "0" and "1" is 3+17+25+20 = 65. Thus, the
relative utility of {1, 3, 4} is 10 / 65 = 15.3 %. The relative utility
can be interpreted as the percentage of the profit generated by a given
itemset during the time period when it was sold.</p>
  <p>A<strong> on-shelf high utility itemset</strong> is an itemset
such that its relative utility is no less than <em>min_utility_ratio.</em>
For example, if we run <strong>FOSHU </strong>with a<strong> minimum
utility of 0.8 (which means 80 %)</strong>, we obtain 4 <strong>on-shelf
high-utility itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="329">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility ($)</strong></td>
      <td width="97"><strong>relative utility (%)</strong></td>
    </tr>
    <tr>
      <td>{2, 5, 7}</td>
      <td>9 $</td>
      <td>81 %</td>
    </tr>
    <tr>
      <td>{2, 3, 5, 7}</td>
      <td>11 $</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>{2, 4, 5}</td>
      <td>36 $</td>
      <td>80 %</td>
    </tr>
    <tr>
      <td>{2, 3, 4, 5}</td>
      <td>40 $</td>
      <td>88 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated at least a 80 % of the profit during the time period when
they were sold. <br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>FOSHU </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
    <li>Fourth, the symbol ":" appears and is followed by a positive
integer such as 0,1,2.... indicating the time period of the transaction</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>1 3 4:3:-5 1 2:0<br>
1 3 5 7:17:-10 6 6 5:0<br>
1 2 3 4 5 6:25:-5 4 1 12 3 5:1<br>
2 3 4 5:20:8 3 6 3:1<br>
2 3 5 7:11:4 2 3 2:2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {1,3, 4}
has a total utility of 3 and that items 1, 3 and 4 respectively have a
utility of -5, 1 and 2 in this transaction. The following lines follow
the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>FOSHU</strong>
is defined as follows. It is a text file, where each line represents a <strong>on-shelf
high utility itemset</strong>. On each line, the items of the itemset
are first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. Then, the keyword "#RUTIL:"
appears followed by the relative utility of this itemset. For example,
we show below the output file for this example. </p>
  <p>7 2 5 #UTIL: 9 #RUTIL: 0.8181818181818182<br>
7 2 5 3 #UTIL: 11 #RUTIL: 1.0<br>
4 2 5 #UTIL: 36 #RUTIL: 0.8<br>
4 2 5 3 #UTIL: 40 #RUTIL: 0.8888888888888888</p>
  <p> For example, the second line indicates that the itemset {2, 3, 5,
7} has a utility of 11 $ and a relative utility of 100 %. The other
lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>On-shelf high utility itemset mining </strong>is a more
difficult problem than frequent itemset mining. Therefore, <strong>on-shelf
high-utility itemset mining</strong> algorithms are generally slower
than frequent itemset mining algorithms. The <strong>FOSHU (2015) </strong>algorithm
is up to 1000 times faster than TS-HOUN, the previous state-of-the-art
algorithm for on-shelf high-utility itemset mining.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version of <strong>FOSHU </strong>offered in SPMF is the
original implementation.</p>
</blockquote>

<p>Where can I get more information about the FOSHU algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <strong>FOSHU</strong>
algorithm:</p>
  <p><em>Fournier-Viger, P., Zida, S. (2015). <a href="http://www.philippe-fournier-viger.com/FOSHU_on-shelf-high-utility-itemset-mining.pdf">FOSHU:
Faster On-Shelf High Utility Itemset Miningâ with or without negative
unit profit.</a> Proc. 30th Symposium on Applied Computing (ACM
SAC 2015). ACM Press, pp. 857-864.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="tshoun" id="example61"> </a></span></strong>
Example 46 : Mining On-Shelf High-Utility Itemsets from
a Transaction Database using the TS-HOUN Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">TS-HOUN</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_FOSHU.txt</span><span class="Style2">"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility ratio to 0.8 <span class="Style2"></span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TS-HOUN </span></strong><strong><span class="Style9">DB_FOSHU.txt
    </span></strong>output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_FOSHU.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTSHOUN_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>TS-HOUN</strong>?</p>

<blockquote>
  <p><strong>TS-HOUN </strong> (Lan et al, 2014) is an algorithm for
discovering<strong> high-utility itemsets </strong>in a transaction
database containing utility information and information about the <strong>time
periods where items</strong> are sold. The task of<strong> on-shelf
high-utility itemset mining </strong>is an extension of the<strong>
task of high utility </strong>i<strong>temset mining</strong>.</p>
  <p>The TS-HOUN algorithm for on-shelf-high-utility itemset mining is
interesting because it addresses two limitations of high-utility
itemset mining algorithms. First, most algorithms cannot handle
databases where items may have negative unit profit/weight. But such
items often occur in real-life transaction databases. For example, it
is common that a retail store will sell items at a loss to stimulate
the sale of other related items or simply to attract customers to their
retail location. If classical HUIM algorithms are applied on database
containing items with negative unit profit, they can generate an
incomplete set of high-utility itemsets. Second, most algorithms
consider that items have the same shelf time, i.e. that all item are on
sale for the same time period. However, in real-life some items are
only sold during some short time period (e.g. the summer). Algorithms
ignoring the shelf time of items have a bias toward items having more
shelf time since they have more chance to generate a high profit.</p>
  <p>TS-HOUN is the first algorithm for on-shelf high utility itemset
mining with both positive and negative profit values. However, it was
outperformed by FOSHU (also offered in SPMF). FOSHU was shown to
outperform TS-HOUN by up to three orders of magnitude in terms of
execution time (see "Performance" section of this website for more
details).</p>
  <p>This is the original implementation of FOSHU.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>TS-HOUN</strong> takes as input a transaction database
with information about the utility of items and their shelf time time,
and a minimum utility threshold <em>min_utility ratio </em>(a
positive double value in the [0,1] interval)<em>.</em> For example,
let's consider the following database consisting of 5 transactions
(t1,t2, ..., t5) and 7 items (1, 2, 3, 4, 5, 6, 7). This database is
provided in the text file "<strong>DB_FOSHU.txt</strong>" in the
package <strong>ca.pfv.spmf.tests </strong>of the SPMF distribution<strong>.</strong></p>
  <table align="center" border="1" width="1018">
    <tbody>
      <tr>
        <td width="56"><strong>Transaction</strong></td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
        <td width="291"><strong>Time period</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>1 3 4</td>
        <td>3</td>
        <td>-5 1 2</td>
        <td>0</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>1 3 5 7</td>
        <td>17</td>
        <td>-10 6 6 5</td>
        <td>0</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>1 2 3 4 5 6</td>
        <td>25</td>
        <td>-5 4 1 12 3 5</td>
        <td>1</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>2 3 4 5</td>
        <td>20</td>
        <td>8 3 6 3</td>
        <td>1</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>2 3 5 7</td>
        <td>11</td>
        <td>4 2 3 2</td>
        <td>2</td>
      </tr>
    </tbody>
  </table>
  <p>Each line of the <strong>database</strong> represents a
transaction and contains the following information: </p>
  <ul>
    <li> a set of items (the second column of the table), </li>
    <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the third column of the table),</li>
    <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the fourth column of the
table).</li>
    <li>the time period where this transaction occurred (the fifth
column). </li>
  </ul>
  <p>Note that the value in the third column for each line is the sum
of the values in the second column. Moreoever, nNote that utility
values may be positive or negative integers. Time periods are values
numbered 0,1,2,3..., which may represent for example periods such as
"summer", "fall", "winter" and "spring".</p>
  <p>What are real-life examples of such a database? There are several
applications in real life. The main application is for customer
transaction databases. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 1, 3 and 4. The amount of profit generated by the sale of
each of these item is respectively -5 $, 1 $ and 2 $. The total amount
of money spent in this transaction is -5 + 1 + 2 = 3 $. This
transaction was done during time period "0", which may for example
represents the summer.</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of <strong>TS-HOUN </strong>algorithm is
the set of <strong>on-shelf high utility itemset</strong>s having a
relative utility no less than the<strong> <em>min_utility_ratio</em></strong>
threshold set by the user. To explain what is an on-shelf high utility
itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1,
3, 4} in transaction t1 is -5 + 1 + 2 = 3, and the utility of {1, 3, 4}
in transaction t3 is -5 + 1 + 12 = 7. The<strong> utility of an itemset
in a database</strong> is the sum of its utility in all transactions
where it appears. For example, the utility of {1, 3, 4} in the database
is the utility of {1, 3, 4} in t1 plus the utility of {1, 3, 4} in t3,
for a total of 3 + 7 = 10. The <strong>relative utility of an itemset </strong>is
the utility of that itemset divided by the sum of the transaction
utilities for the time period where the itemset was sold. For example,
itemset {1, 3, 4} was sold in time periods "0" and "1". The total
utility of time period "0" and "1" is 3+17+25+20 = 65. Thus, the
relative utility of {1, 3, 4} is 10 / 65 = 15.3 %. The relative utility
can be interpreted as the percentage of the profit generated by a given
itemset during the time period when it was sold.</p>
  <p>A<strong> on-shelf high utility itemset</strong> is an itemset
such that its relative utility is no less than <em>min_utility_ratio.</em>
For example, if we run<strong> TS-HOUN </strong>with a<strong> minimum
utility of 0.8 (which means 80 %)</strong>, we obtain 4 <strong>on-shelf
high-utility itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="329">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility ($)</strong></td>
      <td width="97"><strong>relative utility (%)</strong></td>
    </tr>
    <tr>
      <td>{2, 5, 7}</td>
      <td>9 $</td>
      <td>81 %</td>
    </tr>
    <tr>
      <td>{2, 3, 5, 7}</td>
      <td>11 $</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>{2, 4, 5}</td>
      <td>36 $</td>
      <td>80 %</td>
    </tr>
    <tr>
      <td>{2, 3, 4, 5}</td>
      <td>40 $</td>
      <td>88 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated at least a 80 % of the profit during the time period when
they were sold. <br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of<strong> TS-HOUN </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
    <li>Fourth, the symbol ":" appears and is followed by a positive
integer such as 0,1,2.... indicating the time period of the transaction</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>1 3 4:3:-5 1 2:0<br>
1 3 5 7:17:-10 6 6 5:0<br>
1 2 3 4 5 6:25:-5 4 1 12 3 5:1<br>
2 3 4 5:20:8 3 6 3:1<br>
2 3 5 7:11:4 2 3 2:2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {1,3, 4}
has a total utility of 3 and that items 1, 3 and 4 respectively have a
utility of -5, 1 and 2 in this transaction. The following lines follow
the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of<strong> TS-HOUN </strong>is
defined as follows. It is a text file, where each line represents a <strong>on-shelf
high utility itemset</strong>. On each line, the items of the itemset
are first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. Then, the keyword "#RUTIL:"
appears followed by the relative utility of this itemset. For example,
we show below the output file for this example. </p>
  <p>7 2 5 #UTIL: 9 #RUTIL:0.8181818181818182<br>
7 2 5 3 #UTIL: 11 #RUTIL:1.0<br>
4 2 5 #UTIL: 36 #RUTIL:0.8<br>
4 2 5 3 #UTIL: 40 #RUTIL:0.8888888888888888</p>
  <p> For example, the second line indicates that the itemset {2, 3, 5,
7} has a utility of 11 $ and a relative utility of 100 %. The other
lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>On-shelf high utility itemset mining </strong>is a more
difficult problem than frequent itemset mining. Therefore, <strong>on-shelf
high-utility itemset mining</strong> algorithms are generally slower
than frequent itemset mining algorithms.TS-HOUN (2014) is the first
algorithm for on-shelf high utility itemset mining with both positive
and negative profit values. However, it was outperformed by FOSHU
(2015) (also offered in SPMF). FOSHU was shown to outperform TS-HOUN by
up to three orders of magnitude in terms of execution time (see
"Performance" section of this website for more details).</p>
</blockquote>

<p>Where can I get more information about the TS-HOUN algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <strong>TS-HOUN</strong>
algorithm:</p>
  <p><em>G.-C. Lan, T.-P. Hong, J.-P. Huang and V.S. Tseng. On-shelf
utility mining with negative item values. In Expert Systems with
Applications. 41:3450â3459, 2014.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="eihi" id="example62"> </a></span></strong>
Example 47 : Incremental High-Utility Itemset Mining in
a Database with Utility Information with the EIHI Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This algorithm is not offered in the graphical user
interface of SPMF.</strong></li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestEIHI.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>EIHI</strong>?</p>

<blockquote>
  <p><strong>EIHI </strong> (Fournier-Viger et al., 2015) is an
algorithm for maintaining<strong> high-utility itemsets </strong>in a
transaction database containing utility information that is updated
incrementally by inserting new transactions. This task called "<strong>incremental
high-utility itemset mining" </strong>is a generalization of the<strong>
task of high utility </strong>i<strong>temset mining</strong>, where
the database is not assumed to be static.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>EIHI</strong> takes as input a transaction database with
utility information and a minimum utility threshold <em>min_utility </em>(a
positive integer)<em>.</em> Let's consider the following database
consisting of 4 transactions (t1,t2...t4) and 7 items (1, 2, 3, 4, 5,
6, 7). This database is provided in the text file "<strong>DB_incremental1.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <blockquote>
    <table align="center" border="1" width="721">
      <tbody>
        <tr>
          <td width="56"> <br>
          </td>
          <td width="156"><strong>Items</strong></td>
          <td width="190"><strong>Transaction utility</strong></td>
          <td width="291"><strong>Item utilities for this transaction</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>3 5 1 2 4 6</td>
          <td>30</td>
          <td>1 3 5 10 6 5</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>3 5 2 4</td>
          <td>20</td>
          <td>3 3 8 6</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>3 1 4</td>
          <td>8</td>
          <td>1 5 2</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>3 5 1 7</td>
          <td>27</td>
          <td>6 6 10 5</td>
        </tr>
      </tbody>
    </table>
  </blockquote>
  <p>Each line of the <strong>database</strong> is:</p>
  <ul>
    <li> a set of items (the first column of the table), </li>
    <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
    <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
  </ul>
  <p>Note that the value in the second column for each line is the sum
of the values in the third column.</p>
  <p>What are real-life examples of such a database? There are several
applications in real life. One application is a customer transaction
database. Imagine that each transaction represents the items purchased
by a customer. The first customer named "<strong>t1</strong>" bought
items 3, 5, 1, 2, 4 and 6. The amount of money spent for each item is
respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount of
money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  <p>The EIHI algorithm is an <strong>incremental algorithm</strong>,
which means that it can efficiently update the result when new
transactions are inserted into the database. In this example, we will
consider that a new transaction is inserted into the database, as
follows: </p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <p>This transaction is provided in the file "<strong>DB_incremental2.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> EIHI </strong>is the set of <strong>high
utility itemset</strong>s having a utility no less than a <em><strong>min_utility</strong></em>
threshold (a positive integer) set by the user. To explain what is a
high utility itemset, it is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, consider the initial database containing transactions t1, t2,
t3 and t4. In this database, the utility of {1 4} is the utility of {1
4} in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>EIHI </strong>with
a<strong> minimum utility of 30 </strong>on the initial database
containing t1,t2,t3 and t4, we obtain <strong>6</strong> <strong>high-utility
itemsets</strong>:</p>
  <table align="center" border="1" width="226">
    <tbody>
      <tr>
        <td width="113"><strong> itemsets</strong></td>
        <td width="97"><strong>utility</strong></td>
      </tr>
      <tr>
        <td>{2 4}</td>
        <td>30</td>
      </tr>
      <tr>
        <td>{1 3 5}</td>
        <td>31</td>
      </tr>
      <tr>
        <td>{2 3 4}</td>
        <td>34</td>
      </tr>
      <tr>
        <td>{2 4 5}</td>
        <td>36</td>
      </tr>
      <tr>
        <td>{2 3 4 5}</td>
        <td>40</td>
      </tr>
      <tr>
        <td>{1 2 3 4 5 6}</td>
        <td>30</td>
      </tr>
    </tbody>
  </table>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more.</p>
  <p>Now, EIHI is an incremental. It is designed to update the set of
high-utility itemsets when new transactions are inserted. For example,
consider that transaction t5 is now inserted. The results is thus
updated as follows, where <strong>8 high-utility itemsets are found:</strong></p>
  <table align="center" border="1" width="273">
    <tbody>
      <tr>
        <td width="144"><strong> itemsets</strong></td>
        <td width="113"><strong>utility</strong></td>
      </tr>
      <tr>
        <td>{2 4}</td>
        <td>30</td>
      </tr>
      <tr>
        <td><strong>{2 5}</strong></td>
        <td><strong>31</strong></td>
      </tr>
      <tr>
        <td>{1 3 5}</td>
        <td>31</td>
      </tr>
      <tr>
        <td>{2 3 4}</td>
        <td>34</td>
      </tr>
      <tr>
        <td><strong>{2 3 5}</strong></td>
        <td><strong>37</strong></td>
      </tr>
      <tr>
        <td>{2 4 5}</td>
        <td>36</td>
      </tr>
      <tr>
        <td>{2 3 4 5}</td>
        <td>40</td>
      </tr>
      <tr>
        <td>{1 2 3 4 5 6}</td>
        <td>30</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>EIHI </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file <strong>"DB_incremental1.txt"</strong>
is defined as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5</p>
  </blockquote>
  <p>And the input file <strong>"DB_incremental2.txt"</strong> is
defined as follows:</p>
  <blockquote>
    <p> 3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line of the file<strong> "DB_incremental1.txt"</strong>.
It means that the transaction {3, 5, 1, 2, 4, 6} has a total utility of
30 and that items 3, 5, 1, 2, 4 and 6 respectively have a utility of 1,
3, 5, 10, 6 and 5 in this transaction. The following lines follow the
same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>EIHI </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file after all transactions have been processed from both
files. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. </p>
  <p><strong>The EIHI algorithm </strong> was shown to be up to 100
times faster than <strong>HUI-LIST-INS</strong> (also included in
SPMF), the previous state-of-the-art algorithm for maintaining
high-utility itemsets in transactions databases where transaction
insertions are performed.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The version offered in SPMF is the original implementation of <strong>EIHI</strong>.</p>
  <p>Note that the input format is not exactly the same as described in
the article. But it is equivalent.</p>
  <p>Note also, that a file "<strong>MainTestEIHI_Xruns.java</strong>"
is provided in the package "<strong>ca.pfv.spmf.tests</strong>". This
file can be used to run experiments such as those provided in the
article proposing EIHI where a different number of updates is varied on
some datasets. This example uses a single file as input and divide it
into several parts. Then, the algorithm is incrementally run by
processing each part of the file one after the other.</p>
</blockquote>

<p>Where can I get more information about the <strong>EIHI</strong>
algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <strong>EIHI</strong>
algorithm:</p>
  <p><em>Fournier-Viger, P., Lin, J. C.-W., Gueniche, T., Barhate, P.
(2015). Efficient Incremental High Utility Itemset Mining. Proc. 5th
ASE International Conference on Big Data (BigData 2015), to appear.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="eihi" id="example63"> </a></span></strong>
Example 48 : Incremental High-Utility Itemset Mining in
a Database with Utility Information with the HUI-LIST-INS Algorithm </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>This algorithm is not offered in the graphical user
interface of SPMF.</strong></li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestHUI_LIST_INS.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>HUI-LIST-INS</strong>?</p>

<blockquote>
  <p><strong>HUI-LIST-INS </strong> (Lin et al., 2014) is an algorithm
for maintaining<strong> high-utility itemsets </strong>in a
transaction database containing utility information that is updated
incrementally by inserting new transactions. This task called "<strong>incremental
high-utility itemset mining" </strong>is a generalization of the<strong>
task of high utility </strong>i<strong>temset mining</strong>, where
the database is not assumed to be static.</p>
  <p>Note that the faster algorithm <strong>EIHI</strong> is also
offered in SPMF.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>HUI-LIST-INS</strong> takes as input a transaction
database with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 4 transactions (t1,t2...t4) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_incremental1.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <blockquote>
    <table align="center" border="1" width="721">
      <tbody>
        <tr>
          <td width="56"> <br>
          </td>
          <td width="156"><strong>Items</strong></td>
          <td width="190"><strong>Transaction utility</strong></td>
          <td width="291"><strong>Item utilities for this transaction</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>3 5 1 2 4 6</td>
          <td>30</td>
          <td>1 3 5 10 6 5</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>3 5 2 4</td>
          <td>20</td>
          <td>3 3 8 6</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>3 1 4</td>
          <td>8</td>
          <td>1 5 2</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>3 5 1 7</td>
          <td>27</td>
          <td>6 6 10 5</td>
        </tr>
      </tbody>
    </table>
  </blockquote>
  <p>Each line of the <strong>database</strong> is:</p>
  <ul>
    <li> a set of items (the first column of the table), </li>
    <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
    <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
  </ul>
  <p>Note that the value in the second column for each line is the sum
of the values in the third column.</p>
  <p>What are real-life examples of such a database? There are several
applications in real life. One application is a customer transaction
database. Imagine that each transaction represents the items purchased
by a customer. The first customer named "<strong>t1</strong>" bought
items 3, 5, 1, 2, 4 and 6. The amount of money spent for each item is
respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount of
money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  <p>The HUI-LIST-INS algorithm is an <strong>incremental algorithm</strong>,
which means that it can efficiently update the result when new
transactions are inserted into the database. In this example, we will
consider that a new transaction is inserted into the database, as
follows: </p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <p>This transaction is provided in the file "<strong>DB_incremental2.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> HUI-LIST-INS </strong>is the set
of <strong>high utility itemset</strong>s having a utility no less
than a <em><strong>min_utility</strong></em> threshold (a positive
integer) set by the user. To explain what is a high utility itemset, it
is necessary to review some definitions. An <strong>itemset</strong>
is an unordered set of distinct items. The <strong>utility of an
itemset in a transaction </strong>is the sum of the utility of its
items in the transaction. For example, the utility of the itemset {1 4}
in transaction t1 is 5 + 6 = 11 and the utility of {1 4} in transaction
t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong>
is the sum of its utility in all transactions where it appears. For
example, consider the initial database containing transactions t1, t2,
t3 and t4. In this database, the utility of {1 4} is the utility of {1
4} in t1 plus the utility of {1 4} in t3, for a total of 11 + 7 = 18. A<strong>
high utility itemset</strong> is an itemset such that its utility is no
less than <em>min_utility</em> For example, if we run <strong>HUI-LIST-INS
  </strong>with a<strong> minimum utility of 30 </strong>on the
initial database containing t1,t2,t3 and t4, we obtain <strong>6</strong>
  <strong>high-utility itemsets</strong>:</p>
  <table align="center" border="1" width="226">
    <tbody>
      <tr>
        <td width="113"><strong> itemsets</strong></td>
        <td width="97"><strong>utility</strong></td>
      </tr>
      <tr>
        <td>{2 4}</td>
        <td>30</td>
      </tr>
      <tr>
        <td>{1 3 5}</td>
        <td>31</td>
      </tr>
      <tr>
        <td>{2 3 4}</td>
        <td>34</td>
      </tr>
      <tr>
        <td>{2 4 5}</td>
        <td>36</td>
      </tr>
      <tr>
        <td>{2 3 4 5}</td>
        <td>40</td>
      </tr>
      <tr>
        <td>{1 2 3 4 5 6}</td>
        <td>30</td>
      </tr>
    </tbody>
  </table>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more.</p>
  <p>Now, HUI-LIST-INS is an incremental. It is designed to update the
set of high-utility itemsets when new transactions are inserted. For
example, consider that transaction t5 is now inserted. The results is
thus updated as follows, where <strong>8 high-utility itemsets are
found:</strong></p>
  <table align="center" border="1" width="273">
    <tbody>
      <tr>
        <td width="144"><strong> itemsets</strong></td>
        <td width="113"><strong>utility</strong></td>
      </tr>
      <tr>
        <td>{2 4}</td>
        <td>30</td>
      </tr>
      <tr>
        <td><strong>{2 5}</strong></td>
        <td><strong>31</strong></td>
      </tr>
      <tr>
        <td>{1 3 5}</td>
        <td>31</td>
      </tr>
      <tr>
        <td>{2 3 4}</td>
        <td>34</td>
      </tr>
      <tr>
        <td><strong>{2 3 5}</strong></td>
        <td><strong>37</strong></td>
      </tr>
      <tr>
        <td>{2 4 5}</td>
        <td>36</td>
      </tr>
      <tr>
        <td>{2 3 4 5}</td>
        <td>40</td>
      </tr>
      <tr>
        <td>{1 2 3 4 5 6}</td>
        <td>30</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>HUI-LIST-INS </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file <strong>"DB_incremental1.txt"</strong>
is defined as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5</p>
  </blockquote>
  <p>And the input file <strong>"DB_incremental2.txt"</strong> is
defined as follows:</p>
  <blockquote>
    <p> 3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line of the file<strong> "DB_incremental1.txt"</strong>.
It means that the transaction {3, 5, 1, 2, 4, 6} has a total utility of
30 and that items 3, 5, 1, 2, 4 and 6 respectively have a utility of 1,
3, 5, 10, 6 and 5 in this transaction. The following lines follow the
same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>HUI-LIST-INS </strong>is
defined as follows. It is a text file, where each line represents a <strong>high
utility itemset</strong>. On each line, the items of the itemset are
first listed. Each item is represented by an integer, followed by a
single space. After, all the items, the keyword " #UTIL: " appears and
is followed by the utility of the itemset. For example, we show below
the output file after all transactions have been processed from both
files. </p>
  <p>2 4 #UTIL: 30<br>
2 5 #UTIL: 31<br>
1 3 5 #UTIL: 31<br>
2 3 4 #UTIL: 34<br>
2 3 5 #UTIL: 37<br>
2 4 5 #UTIL: 36<br>
2 3 4 5 #UTIL: 40<br>
1 2 3 4 5 6 #UTIL: 30 </p>
  <p>For example, the first line indicates that the itemset {2, 4} has
a utility of 30. The following lines follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining</strong> is a more difficult
problem than frequent itemset mining. Therefore, high-utility itemset
mining algorithms are generally slower than frequent itemset mining
algorithms. </p>
  <p><strong>The EFIM algorithm </strong> was shown to be up to 100
times faster than <strong>HUI-LIST-INS</strong> (also included in
SPMF) for maintaining high-utility itemsets in transactions databases
where transaction insertions are performed.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>Note that the input format is not exactly the same as described in
the article. But it is equivalent.</p>
  <p>Note also, that a file "<strong>MainTestHUI_LIST_INS_Xruns.java</strong>"
is provided in the package "<strong>ca.pfv.spmf.tests</strong>". This
file can be used to run experiments such as those provided in the
article proposing HUI-LIST-INS where a different number of updates is
varied on some datasets. This example uses a single file as input and
divide it into several parts. Then, the algorithm is incrementally run
by processing each part of the file one after the other.</p>
</blockquote>

<p>Where can I get more information about the <strong>HUI-LIST-INS</strong>
algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <strong>HUI-LIST-INS</strong>
algorithm:</p>
  <p><em>J. C.-W. Lin, W. Gan, T.P. Hong, J. S. Pan, Incrementally
Updating High-Utility Itemsets with Transaction Insertion. In: Proc.
10th Intern. Conference on Advanced Data Mining and Applications (ADMA
2014), Springer (2014)</em></p>
</blockquote>

<h3><a name="efimclosed" id="chuiminer2"> </a>Example 49 : Mining Closed High-Utility Itemsets from
a transaction database with utility information using the CHUI-Miner
Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style9">EFIM-Closed</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
    (4) set the minimum utility to 30 <span class="Style2"> </span>and
    (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> EFIM-Closed</span></strong> <strong><span class="Style9">DB_utility</span></strong>.txt
      output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    the file <strong><span class="Style2">"MainTestEFIM_Closed_saveToFile.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is <span style="font-weight: bold;">EFIM-Closed</span><strong></strong>?</p>
<blockquote>
  <p><strong>EFIM-Closed </strong> (Fournier-Viger et al., 2016) is an
    algorithm for discovering<strong> closed high-utility itemsets </strong>in
    a transaction database containing utility information. </p>
  <p>There has been many work on the design of algorithms for high-utility itemset mining. However, a  limitation of many<span style="font-weight: bold;"> high-utility
      itemset mining </span>algorithms is that they output too many itemsets. As a result, it may be inconvenient for a user to analyze the result of traditional high utility itemset mining algorithms. As a solution,  algorithms have been designed to discover
    only the high-utility
    itemsets that are <span style="font-weight: bold;">closed</span>. The
    concept of <span style="font-weight: bold;">closed itemset</span> was
    previously introduced in<span style="font-weight: bold;"> frequent
      itemset mining</span>. An
    itemset is closed if it has no subset having the same support
    (frequency) in the database. In terms of application to transaction
    databases, the concept of closed itemset can be understood as any
    itemset
    that is the largest set of items bought by a given set of
    customers. For more details, you may look at the paper about EFIM-Closed. It
    provides more details about the motivation for mining closed
    high-utility itemsets. Other popular alternative algorithms for closed high-utility itemsets mining are <strong>CHUI-Miner</strong> (2015, also offered in SPMF), and <strong>CHUD</strong> (2011,2013, currently not offered in SPMF).</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
  <p><strong>EFIM-Closed </strong> takes as input a transaction database
    with utility information and a minimum utility threshold <em>min_utility </em>(a positive integer)<em>.</em> Let's consider the following
    database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
    3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
    in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
    distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"><br></td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
        transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
        generated by this item for this transaction)(the third column of the
        table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
      sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
      several applications in real life. One application is a customer
      transaction database. Imagine that each transaction represents the
      items purchased by a customer. The first customer named "<strong>t1</strong>"
      bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
      item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
      of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p><em> </em>The output of<strong> EFIM-Closed </strong>is the set
    of <strong>closed high utility itemset</strong>s having a utility no
    less than a <em><strong>min_utility</strong></em> threshold (a
    positive integer) set by the user. To explain what is a <span style="font-weight: bold;">closed high utility itemset</span>, it is
    necessary to review some definitions. </p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
    items. The <strong>utility of an itemset in a transaction </strong>is
    the sum of the utility of its items in the transaction. For example,
    the utility of the itemset {1 4} in transaction t1 is 5 + 6 = 11 and
    the utility of {1 4} in transaction t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong> is the sum of its utility
    in all transactions where it appears. For example, the utility of {1 4}
    in the database is the utility of {1 4} in t1 plus the utility of {1 4}
    in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong> is an itemset such that its utility is no less than <em>min_utility.</em></p>
  <p>To explain what is a <span style="font-weight: bold;">closed
    itemset</span> it is necessary
    to review a few definitions. </p>
  <p> The <strong>support of an itemset</strong> is the number of
    transactions that contain the itemset. For example, the itemset {1, 3,
    5}
    has a support of 2 because it appears in three transactions from the
    database (t1 and t4). A <span style="font-weight: bold;">closed</span> is an itemset X
    such that there does not exist an itemset Y strictly included in X that
    has the same support. For example, itemset {1, 3, 5} is a closed
    itemset.</p>
  <p>A <strong>closed high utility itemset (CHUI)</strong> is a
    high-utility itemset that is a closed itemset.</p>
  <p> For example, if we run <span style="font-weight: bold;">EFIM-Closed </span><strong> </strong>with a<strong> minimum utility of 30</strong> we obtain 4 <span style="font-weight: bold;">closed high-utility itemsets</span><strong></strong>:</p>
</blockquote>
<table align="center" border="1" width="349">
  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="117"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1, 2, 3, 4, 5, 6}</td>
      <td>30</td>
      <td>1 transaction</td>
    </tr>
    <tr>
      <td>{2, 3, 4, 5}</td>
      <td>40</td>
      <td>2 transactions</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>37</td>
      <td>3 transactions</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>31</td>
      <td>2 transactions</td>
    </tr>
  </tbody>
</table>
<blockquote>
  <p>If the database is a transaction database from a store, we could
    interpret these results as all the groups of items bought together that
    generated a profit of 30 $ or more, and that are maximal sets of items
    in common for a group of customers. <br>
  </p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> of <span style="font-weight: bold;">EFIM-Closed </span><strong> </strong>is
    defined as follows. It is a text file. Each lines represents a
    transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
      item is represented by a positive integer. Each item is separated from
      the next item by a single space. It is assumed that all items within a
      same transaction (line) are sorted according to a total order (e.g.
      ascending order) and that no item can appear twice within the same
      transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
      transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
      each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
      3 5 2 4:20:3 3 8 6<br>
      3 1 4:8:1 5 2<br>
      3 5 1 7:27:6 6 10 5<br>
      3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
    2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
    respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
    transaction. The following lines follow the same format.</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> of <span style="font-weight: bold;">EFIM-Closed</span><strong></strong> is defined as follows. It is a text file, where each line represents a <strong>closed
    high
    utility itemset</strong><span style="font-weight: bold;">s</span>. On
    each line, the items of the
    itemset are first listed. Each item is represented by an integer,
    followed by a single space. After, all the items, the keyword
    "#SUPPORT:" appears and is followed by the support of the itemset.
    Then, the keyword #UTIL: " appears and is followed by the utility of
    the itemset. For example, we show below the output file for this
    example. </p>
  <p>6 4 2 1 5 3 #SUP: 1 #UTIL: 30<br>
    4 3 2 5 #SUP: 2 #UTIL: 40<br>
    2 5 3 #SUP: 3 #UTIL: 37<br>
    1 3 5 #SUP: 2 #UTIL: 31</p>
  <p>For example, the third line indicates that the itemset {2, 3, 5}
    has
    a support of 3 transactions and a utility of 37$. The other lines
    follows the same format.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
    problem than frequent itemset mining. Therefore, <strong>high-utility
      itemset mining</strong> algorithms are generally slower than frequent
    itemset mining algorithms. The <span style="font-weight: bold;">EFIM-Closed </span><strong> </strong>algorithm
    was proposed in 2016 to discover only the high-utility itemsets that
    are closed itemsets. It is generally faster to mine closed high-utility itemsets than discovering all
    high-utility itemsets. Thus, this algorithm can in some cases
    outperform algorithms
    such as <span style="font-weight: bold;">FHM</span> and <span style="font-weight: bold;">HUI-Miner,</span> who discover all
    high-utility itemsets. The <span style="font-weight: bold;">EFIM-Closed </span>algorithm was shown to outperform the original algorithm for mining closed high-utility itemsets named <span style="font-weight: bold;">CHUD</span> algorithm (published in the
    proceedings of the ICDM 2011 conference).<br>
  </p>
</blockquote>
<p>Implementation details</p>
<blockquote>
  <p>This is an implementation of<span style="font-weight: bold;"> EFIM-Closed</span>,
    implemented by P. Fournier-Viger. This is an alternative implementation
    that was not used in the paper. The main differences with the
    implementation in the paper is that this implementation (1) does not
    calculate utility-unit arrays (see the paper) and (2) adds the EUCP
    optimizations introduced in the FHM algorithm.</p>
  <p>In the source code version of SPMF, there are two examples of
    using<span style="font-weight: bold;"> EFIM-Closed</span> in the
    package <span style="font-weight: bold;">ca.pfv.spmf.tests</span>. The
    first one is <span style="font-weight: bold;">MainTestEFIM_Closed_saveToFile</span>,
    which saves the result to an output file. The second one is <span style="font-weight: bold;">MainTestEFIM_Closed_saveToMemory</span>,
    which saves the result to memory.</p>
</blockquote>
<p>Where can I get more information about the <span style="font-weight: bold;">EFIM_Closed </span>algorithm?</p>
<blockquote>
  <p>This is the reference of the article describing the <span style="font-weight: bold;">EFIM_Closed </span> algorithm:</p>
  <p><em>Fournier-Viger, P., Zida, S. Lin, C.W., Wu, C.-W., Tseng, V. S. (2016). EFIM-Closed: Fast and Memory Efficient Discovery of Closed High-Utility Itemsets. Proc. 12th Intern. Conference on Machine Learning and Data Mining (MLDM 2016). Springer, LNAI, 15 pages, to appear</em></p>
</blockquote>
<h3><a name="chuiminer" id="chuiminer"> </a>Example 50  : Mining Closed High-Utility Itemsets from
  a transaction database with utility information using the CHUI-Miner
  Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">CHUI-Miner</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 30 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CHUI-Miner</span></strong> <strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCHUIMiner_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <span style="font-weight: bold;">CHUI</span><strong>-Miner</strong>?</p>

<blockquote>
  <p><strong>CHUI-Miner </strong> (Wu et al., 2014) is an
algorithm for discovering<strong> closed high-utility itemsets </strong>in
a transaction database containing utility information. </p>
  <p>There has been many work on the topic of
high-utility itemset mining. A limitation of many<span style="font-weight: bold;"> high-utility
itemset mining </span>algorithms is that they generate too much
itemsets as output. The CHUI-Miner algorithm was designed to discover
only the high-utility
itemsets that are <span style="font-weight: bold;">closed</span>. The
concept of <span style="font-weight: bold;">closed itemset</span> was
previously introduced in<span style="font-weight: bold;"> frequent
itemset mining</span>. An
itemset is closed if it has no subset having the same support
(frequency) in the database. In terms of application to transaction
database, the concept of closed itemset can be understood as any
itemset
that is the largest set of items bought in common by a given set of
customers. For more details, see the paper by Wu et al. (2015). It
provides a more details about the motivation for mining closed
high-utility itemsets.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>CHUI-Miner</strong> takes as input a transaction database
with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> CHUI-Miner </strong>is the set
of <strong>closed high utility itemset</strong>s having a utility no
less than a <em><strong>min_utility</strong></em> threshold (a
positive integer) set by the user. To explain what is a <span style="font-weight: bold;">closed high utility itemset</span>, it is
necessary to review some definitions. </p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
items. The <strong>utility of an itemset in a transaction </strong>is
the sum of the utility of its items in the transaction. For example,
the utility of the itemset {1 4} in transaction t1 is 5 + 6 = 11 and
the utility of {1 4} in transaction t3 is 5 + 2 = 7. The<strong>
utility of an itemset in a database</strong> is the sum of its utility
in all transactions where it appears. For example, the utility of {1 4}
in the database is the utility of {1 4} in t1 plus the utility of {1 4}
in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong>
is an itemset such that its utility is no less than <em>min_utility.</em></p>
  <p>To explain what is a <span style="font-weight: bold;">closed
itemset</span> it is necessary
to review a few definitions. </p>
  <p> The <strong>support of an itemset</strong> is the number of
transactions that contain the itemset. For example, the itemset {1, 3,
5}
has a support of 2 because it appears in three transactions from the
database (t1 and t4). A <span style="font-weight: bold;">closed</span>
is an itemset X
such that there does not exist an itemset Y strictly included in X that
has the same support. For example, itemset {1, 3, 5} is a closed
itemset.</p>
  <p>A <strong>closed high utility itemset (CHUI)</strong> is a
high-utility itemset that is a closed itemset.</p>
  <p> For example, if we run <span style="font-weight: bold;">CHUI</span><strong>-Miner
  </strong>with a<strong>
minimum utility of 30</strong> we obtain 4 <span style="font-weight: bold;">closed high-utility itemsets</span><strong></strong>:</p>
</blockquote>

<table align="center" border="1" width="349">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="117"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{1, 2, 3, 4, 5, 6}</td>
      <td>30</td>
      <td>1 transaction</td>
    </tr>
    <tr>
      <td>{2, 3, 4, 5}</td>
      <td>40</td>
      <td>2 transactions</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>37</td>
      <td>3 transactions</td>
    </tr>
    <tr>
      <td>{1, 3, 5}</td>
      <td>31</td>
      <td>2 transactions</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 30 $ or more, and that are maximal sets of items
in common for a group of customers. <br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <span style="font-weight: bold;">CHUI</span><strong>-Miner </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <span style="font-weight: bold;">CHUI</span><strong>-Miner</strong>
is defined as follows. It is a text file, where each line represents a <strong>closed
high
utility itemset</strong><span style="font-weight: bold;">s</span>. On
each line, the items of the
itemset are first listed. Each item is represented by an integer,
followed by a single space. After, all the items, the keyword
"#SUPPORT:" appears and is followed by the support of the itemset.
Then, the keyword #UTIL: " appears and is followed by the utility of
the itemset. For example, we show below the output file for this
example. </p>
  <p>6 4 2 1 5 3 #SUP: 1 #UTIL: 30<br>
4 3 2 5 #SUP: 2 #UTIL: 40<br>
2 5 3 #SUP: 3 #UTIL: 37<br>
1 3 5 #SUP: 2 #UTIL: 31</p>
  <p>For example, the third line indicates that the itemset {2, 3, 5}
has
a support of 3 transactions and a utility of 37$. The other lines
follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <span style="font-weight: bold;">CHUI</span><strong>-Miner
  </strong>algorithm
was proposed in 2015 to discover only the high-utility itemsets that
are closed itemset. It is generally faster than discovering all
high-utility itemsets. Thus, this algorithm can in some cases
outperform algorithms
such as <span style="font-weight: bold;">FHM</span> and <span style="font-weight: bold;">HUI-Miner,</span> who discover all
high-utility itemsets. The <span style="font-weight: bold;">CHUI-Miner
  </span>algorithm is an improved version of the <span style="font-weight: bold;">CHUD</span> algorithm published in the
proceedings of the ICDM 2011 conference.<br>
  </p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>This is an implementation of <span style="font-weight: bold;">CHUI-Miner</span>,
implemented by P. Fournier-Viger. This is an alternative implementation
that was not used in the paper. The main differences with the
implementation in the paper is that this implementation (1) does not
calculate utility-unit arrays (see the paper) and (2) adds the EUCP
optimizations introduced in the FHM algorithm.</p>
  <p>In the source code version of SPMF, there are two examples of
using <span style="font-weight: bold;">CHUI-Miner</span> in the
package <span style="font-weight: bold;">ca.pfv.spmf.tests</span>. The
first one is <span style="font-weight: bold;">MainTestCHUIMiner_saveToFile</span>,
which saves the result to an output file. The second one is <span style="font-weight: bold;">MainTestCHUIMiner_saveToMemory</span>,
which saves the result to memory.</p>
</blockquote>

<p>Where can I get more information about the <span style="font-weight: bold;">CHUI-Miner </span>algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <span style="font-weight: bold;">CHUI-Miner</span>
algorithm:</p>
  <p><em>Wu, C.W., Fournier-Viger, P., Gu, J.-Y., Tseng, V.S. (2015).
Mining Closed+ High Utility Itemsets without Candidate Generation.
Proc. 2015 Conference on Technologies and Applications of Artificial
Intelligence (TAAI 2015), pp. 187-194.</em></p>
</blockquote>

<h3><a name="ghuiminer" id="ghuiminer"> </a>Example 51: Mining Generators of High-Utility Itemsets from
a transaction database with utility information using the GHUI-Miner
Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">GHUI-Miner</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 20 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> GHUI-Miner</span></strong> <strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGHUIMiner_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <span style="font-weight: bold;">GHUI-</span><strong>Miner</strong>?</p>

<blockquote>
  <p><strong>GHUI-Miner </strong> (Fournier-Viger et al., 2014) is an
algorithm for discovering<strong> generators of high-utility itemsets </strong>in
a transaction database containing utility information. </p>
  <p>There has been quite a huge amount of work on the topic of
high-utility itemset mining. A limitation of several high-utility
itemset mining algorithms is that they generate too much results. The <span style="font-weight: bold;">GHUI-Miner </span>algorithm was designed
to discover only the<span style="font-weight: bold;"> generators </span>of<span style="font-weight: bold;"> high-utility
itemsets</span>. The concept of
generator was previously introduced in frequent itemset mining. An
itemset is a <span style="font-weight: bold;">generator</span> if it
has no subset having the same support
(frequency) in the database. An itemset is <span style="font-weight: bold;">closed</span> if it has no superset having
the same support
(frequency) in the database. In terms of application to transaction
database, the concept of generator can be understood as any itemset
that is the smallest set of items bought in common by a given set of
customers, while a closed itemset is the maximal set of items.
Generators have shown to be more useful than closed or
maximal itemsets in the field of pattern mining for various tasks such
as classification. The <span style="font-weight: bold;">GHUI-Miner</span>
algorithm discovers all <span style="font-weight: bold;">generators of
high-utility itemsets</span>, that is generators that (1) are
high-utility itemsets or (2) have a superset that is a high-utility
itemset and has the same support.<br>
  </p>
  <p>For more details, see the paper by Fournier-Viger
(2014). It provides a lot of details about the motivation for mining
generators of high-utility itemsets.</p>
  <p>This is the original implementation of <span style="font-weight: bold;">GHUI-Miner</span>.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>GHUI-Miner</strong> takes as input a transaction database
with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> GHUI-Miner </strong>is the set
of <strong>generators of high utility itemset</strong>s having a
utility no
less than a <em><strong>min_utility</strong></em> threshold (a
positive integer) set by the user. To explain what is a high utility
generator itemsets, it is necessary to review some definitions. </p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
items. The <strong>utility of an itemset in a transaction </strong>is
the sum of the utility of its items in the transaction. For example,
the utility of the itemset {1 4} in transaction t1 is 5 + 6 = 11 and
the utility of {1 4} in transaction t3 is 5 + 2 = 7. The<strong>
utility of an itemset in a database</strong> is the sum of its utility
in all transactions where it appears. For example, the utility of {1 4}
in the database is the utility of {1 4} in t1 plus the utility of {1 4}
in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong>
is an itemset such that its utility is no less than <em>min_utility.</em></p>
  <p>To explain what is a <strong>generator</strong>, it is necessary
to review a few definitions. </p>
  <p> The <strong>support of an itemset</strong> is the number of
transactions that contain the itemset. For example, the itemset {1, 5}
has a support of 2 because it appears in three transactions from the
database (t1 and t4). A <strong>generator</strong> is an itemset X
such that there does not exist an itemset Y strictly included in X that
has the same support. For example, itemset {1, 5} is a generator.</p>
  <p>A<span style="font-weight: bold;"> generator of high</span><strong>-utility
itemsets (HUG)</strong> is a generator itemset that (1) is a
high-utility itemsets or (2) has a superset that is a high-utility
itemset and has the same support.</p>
  <p> For example, if we run <span style="font-weight: bold;">GHUI</span><strong>-Miner
  </strong>with a<strong>
minimum utility of 30,</strong> we obtain 7 <strong>generator of
high-utility itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="349">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="117"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>22</td>
      <td>3 transactions</td>
    </tr>
    <tr>
      <td>{2, 4}</td>
      <td>30</td>
      <td>2 transactions</td>
    </tr>
    <tr>
      <td>{1 5}</td>
      <td>24</td>
      <td>2 transactions</td>
    </tr>
    <tr>
      <td style="vertical-align: top;">{6}<br>
      </td>
      <td style="vertical-align: top;">5<br>
      </td>
      <td style="vertical-align: top;">1 transaction<br>
      </td>
    </tr>
    <tr>
      <td style="vertical-align: top;">{4, 5}<br>
      </td>
      <td style="vertical-align: top;">18<br>
      </td>
      <td style="vertical-align: top;">2 transactions</td>
    </tr>
    <tr>
      <td style="vertical-align: top;">{1, 4, 5}<br>
      </td>
      <td style="vertical-align: top;">20<br>
      </td>
      <td style="vertical-align: top;">1 transaction</td>
    </tr>
    <tr>
      <td style="vertical-align: top;">{1, 2}<br>
      </td>
      <td style="vertical-align: top;">15<br>
      </td>
      <td style="vertical-align: top;">1 transaction<br>
      </td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a retail store, we
could
interpret each itemset found as the smallest set of items common to a
group of customers that has bought a given high-utility itemset. <br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <span style="font-weight: bold;">GHUI</span><strong>-Miner </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <span style="font-weight: bold;">GHUI</span><strong>-Miner</strong>
is defined as follows. It is a text file, where each line represents a <strong>generator
of high-utility itemset</strong>. On each line, the items of the
itemset are first listed. Each item is represented by an integer,
followed by a single space. After, all the items, the keyword
"#SUPPORT:" appears and is followed by the support of the itemset.
Then, the keyword #UTIL: " appears and is followed by the utility of
the itemset. For example, we show below the output file for this
example. </p>
  <p>6 #SUP: 1 #UTIL: 5<br>
2 #SUP: 3 #UTIL: 22<br>
4 2 #SUP: 2 #UTIL: 30<br>
4 5 #SUP: 2 #UTIL: 18<br>
4 1 5 #SUP: 1 #UTIL: 20<br>
2 1 #SUP: 1 #UTIL: 15<br>
1 5 #SUP: 2 #UTIL: 24</p>
  <p>For example, the third line indicates that the itemset {2, 4} has
a support of 2 transactions and a utility of 30$. The other lines
follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <span style="font-weight: bold;">GHUI</span><strong>-Miner
  </strong>algorithm
was proposed in 2014 to discover only the high-utility itemsets that
are generators. It is generally faster than discovering all
high-utility itemsets. Thus, this algorithm can outperform algorithms
such as FHM and HUI-Miner, who discover all high-utility itemsets. This
implementation of <span style="font-weight: bold;">GHUI-Miner</span>
relies on the <span style="font-weight: bold;">CHUI-Miner</span>
algorithm for discovering closed high-utility itemsets (a necessary
step to find GHUIs efficiently)<br>
  </p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>This is the original implementation of <span style="font-weight: bold;">GHUI-Miner</span>.</p>
</blockquote>

<p>Where can I get more information about the <span style="font-weight: bold;">GHUI-Miner</span> algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the <span style="font-weight: bold;">GHUI-Miner</span>
algorithm:</p>
  <p><em>Fournier-Viger, P., Wu, C.W., Tseng, V.S. (2014). Novel
Concise Representations of High Utility Itemsets using Generator
Patterns. Proc. 10th Intern. Conference on Advanced Data Mining and
Applications (ADMA 2014), Springer LNCS 8933, pp. 30-43.</em></p>
  <p>Note that in this article, another algorithm called HUG-Miner is
also proposed. It is a different algorithm, which is also offered in
SPMF.</p>
</blockquote>

<h3><strong><span class="centered"><a name="hugminer" id="example64"> </a></span></strong>
Example 52 : Mining High-Utility Generator Itemsets from
a transaction database with utility information using the HUG-Miner
Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HUG-Miner</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 20 <span class="Style2"> </span>and
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> HUG-Miner</span></strong> <strong><span class="Style9">DB_utility</span></strong>.txt
output.txt 20 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTest_HUGMINER_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>HUG-Miner</strong>?</p>

<blockquote>
  <p><strong>HUG-Miner </strong> (Fournier-Viger et al., 2014) is an
algorithm for discovering<strong> high-utility generator itemsets </strong>in
a transaction database containing utility information. </p>
  <p>There has been quite a huge amount of work on the topic of
high-utility itemset mining. A limitation of several high-utility
itemset mining algorithms is that they generate too much results. The
HUG-Miner algorithm was designed to discover only the high-utility
itemsets that are <strong>generators</strong>. The concept of
generator was previously introduced in frequent itemset mining. An
itemset is a generator if it has no subset having the same support
(frequency) in the database. In terms of application to transaction
database, the concept of generator can be understood as any itemset
that is the smallest set of items bought in common by a given set of
customers. Generators have shown to be more useful than closed or
maximal itemsets in the field of pattern mining for various tasks such
as classification. For more details, see the paper by Fournier-Viger
(2014). It provides a lot of details about the motivation for mining
High-utility genrator itemsets.</p>
  <p>This is the original implementation of HUG-Miner.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>HUG-Miner</strong> takes as input a transaction database
with utility information and a minimum utility threshold <em>min_utility
  </em>(a positive integer)<em>.</em> Let's consider the following
database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
generated by this item for this transaction)(the third column of the
table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> HUG-Miner </strong>is the set of
  <strong>high utility generator itemset</strong>s having a utility no
less than a <em><strong>min_utility</strong></em> threshold (a
positive integer) set by the user. To explain what is a high utility
generator itemsets, it is necessary to review some definitions. </p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
items. The <strong>utility of an itemset in a transaction </strong>is
the sum of the utility of its items in the transaction. For example,
the utility of the itemset {1 4} in transaction t1 is 5 + 6 = 11 and
the utility of {1 4} in transaction t3 is 5 + 2 = 7. The<strong>
utility of an itemset in a database</strong> is the sum of its utility
in all transactions where it appears. For example, the utility of {1 4}
in the database is the utility of {1 4} in t1 plus the utility of {1 4}
in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong>
is an itemset such that its utility is no less than <em>min_utility.</em></p>
  <p>To explain what is a <strong>generator</strong>, it is necessary
to review a few definitions. </p>
  <p> The <strong>support of an itemset</strong> is the number of
transactions that contain the itemset. For example, the itemset {1, 5}
has a support of 2 because it appears in three transactions from the
database (t1 and t4). A <strong>generator</strong> is an itemset X
such that there does not exist an itemset Y strictly included in X that
has the same support. For example, itemset {1, 5} is a generator.</p>
  <p>A <strong>high utility generator itemsets (HUG)</strong> is a
high-utility itemset that is a generator.</p>
  <p> For example, if we run <strong>HUG-Miner </strong>with a<strong>
minimum utility of 20</strong>, we obtain 4 <strong>high-utility
generator itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="349">

  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
      <td width="117"><strong>support</strong></td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>22</td>
      <td>5 transactions</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>20</td>
      <td>5 transactions</td>
    </tr>
    <tr>
      <td>{2, 4}</td>
      <td>30</td>
      <td>2 transactions</td>
    </tr>
    <tr>
      <td>{1 5}</td>
      <td>24</td>
      <td>2 transactions</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as all the groups of items bought together that
generated a profit of 20 $ or more, and that are minimal sets of items
in common for a group of customers. <br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>HUG-Miner </strong>is
defined as follows. It is a text file. Each lines represents a
transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
3 5 2 4:20:3 3 8 6<br>
3 1 4:8:1 5 2<br>
3 5 1 7:27:6 6 10 5<br>
3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
transaction. The following lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>HUG-Miner</strong>
is defined as follows. It is a text file, where each line represents a <strong>high
utility generator itemset</strong>. On each line, the items of the
itemset are first listed. Each item is represented by an integer,
followed by a single space. After, all the items, the keyword
"#SUPPORT:" appears and is followed by the support of the itemset.
Then, the keyword #UTIL: " appears and is followed by the utility of
the itemset. For example, we show below the output file for this
example. </p>
  <p>2 #SUP: 5 #UTIL: 22<br>
1 #SUP: 5 #UTIL: 20<br>
4 2 #SUP: 2 #UTIL: 30<br>
1 5 #SUP: 2 #UTIL: 24</p>
  <p>For example, the third line indicates that the itemset {2, 4} has
a support of 2 transactions and a utility of 30$. The following lines
follows the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
problem than frequent itemset mining. Therefore, <strong>high-utility
itemset mining</strong> algorithms are generally slower than frequent
itemset mining algorithms. The <strong>HUG-Miner </strong>algorithm
was proposed in 2014 to discover only the high-utility itemsets that
are generators. It is generally faster than discovering all
high-utility itemsets. Thus, this algorithm can outperform algorithms
such as FHM and HUI-Miner, who discover all high-utility itemsets.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>This is the original implementation of HUG-Miner.</p>
</blockquote>

<p>Where can I get more information about the HUG-Miner algorithm?</p>

<blockquote>
  <p>This is the reference of the article describing the HUG-Miner
algorithm:</p>
  <p><em>Fournier-Viger, P., Wu, C.W., Tseng, V.S. (2014). Novel
Concise Representations of High Utility Itemsets using Generator
Patterns. Proc. 10th Intern. Conference on Advanced Data Mining and
Applications (ADMA 2014), Springer LNCS 8933, pp. 30-43.</em></p>
  <p>Note that in this article, another algorithm called GHUI-Miner is
also proposed. It is a different algorithm, which is also offered in
SPMF.</p>
</blockquote>

<h3><strong><span class="centered"><a name="husrm" id="example65"> </a></span></strong>
Example 53 : Mining High-Utility Sequential Rules from a
Sequence Database with utility information using the HUSRM Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HUSRM</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DataBase_HUSRM</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
(4) set the minimum utility to 20, (5) set the minimum confidence to
0.7, (6) set the minimum antecedent size to 4, (7) set the maximum
consequent size to 4, <span class="Style2"> </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> HUSRM</span></strong> <strong><span class="Style9">DataBase_HUSRM</span></strong>.txt
output.txt 40 </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2"><strong><span class="Style9">DataBase_HUSRM</span></strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTest_HUSRM_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is <strong>HUSRM</strong>?</p>

<blockquote>
  <p><strong>HUSRM </strong> (Zida et al, 2015) is the first algorithm
for discovering<strong> high-utility sequential rules </strong>in a
sequence database containing utility information. </p>
  <p>An typical example of a sequence database with utility information
is a database of customer transactions containing sequences of
transactions performed by customers, where each transaction is a set of
items annotated with the profit generated by the sale of items. The
goal of high-utility sequential rule mining is to find rules of the
form A -&gt; B, meaning that if a customer buy items A, he will then
buy items B with a high confidence, and this rule generate a high
profit. Although, this algorithm is designed for the scenario of
sequence of transactions, the task is general and could be applied to
other types of data such as sequences of webpages visited by user on a
website, where the sale profit is replaced by the time spent on
webpages.</p>
  <p>This is the original implementation of HUSRM. </p>
  <p>Note that the problem of high-utility sequential rule mining is
similar to high-utility sequential pattern mining. However, a key
advantage of high-utility sequential rule mining is that discovered
rules provide information about the probability that if some customers
buy some items A, they will then buy other items B. High-utility
sequential patterns do not consider the confidence that a pattern will
be followed.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>HUSRM</strong> takes as input a sequence database with
utility information, a minimum utility threshold <em>min_utility </em>(a
positive integer), a minimum confidence threshold (a double value in
the [0,1] interval, a maximum antecedent size (a positive integer) and
a maximm consequent size (a positive nteger). </p>
  <p>Let's consider the following sequence database consisting of 4
sequences of transactions (s1,s2, s3, s4) and 7 items (1, 2, 3, 4, 5,
6, 7). This database is provided in the text file "<strong>DataBase_HUSRM.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong> </p>
  <table align="center" border="1" width="595">
    <tbody>
      <tr>
        <td width="63"> <br>
        </td>
        <td width="350"><strong>Sequence</strong></td>
        <td width="160"><strong>Sequence utility</strong></td>
      </tr>
      <tr>
        <td><strong>s1</strong></td>
        <td>{1[1],2[4]},{3[10]},{6[9]},{7[2]},{5[1]}</td>
        <td>27</td>
      </tr>
      <tr>
        <td><strong>s2</strong></td>
        <td>{1[1],4[12]},{3[20]},{2[4]},{5[1],7[2]}</td>
        <td>40</td>
      </tr>
      <tr>
        <td><strong>s3</strong></td>
        <td>{1[1]},{2[4]},{6[9]},{5[1]}</td>
        <td>15</td>
      </tr>
      <tr>
        <td><strong>s4</strong></td>
        <td>{1[3],2[4],3[5]},{6[3],7[1]}</td>
        <td>16</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is a sequence:</p>
    <ul>
      <li>each sequence is an ordered list of transactions, such that
transactions are enclosed by {} in this Example </li>
      <li>each transaction contains a set of items represented by
integers </li>
      <li>each item is annotated with a utility value (e.g. sale
profit), indicated between squared brackets</li>
[ ].
      <li>the sum of the utilities (e.g. profit) of all items in the
sequence is also indicated (the "sequence utility" column)</li>
    </ul>
    <p>What are real-life examples of such a database? A typical
example is a database containing sequences of customer transactions.
Imagine that each sequence represents the transactions made by a
customer. The first customer named "<strong>s1</strong>" bought items 1
and 2, and those items respectively generated a profit of 1$ and 4$.
Then, the customer bought item 3 for 10$. Then, the customer bought
item 6 for 9 $. Then, the customer bought items 7 for 2$. Then the
customer bought item 5 for 1 $.</p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><em> </em>The output of<strong> HUSRM </strong>is the set of <strong>high
utility sequential rules </strong>meeting the criteria specified by
the user</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The <strong>support</strong>
of a rule X==&gt;Y is the number of sequences that contains XâªY divided
by the number of sequences in the database. The <strong>confidence</strong>
of a rule is the number of sequences that contains XâªY, divided by the
number of sequences that contains X. For example, the rule {1,2,3}
==&gt; {7} has a support of 3/5 because it appears in 3 out of 4
sequences in the database.</p>
  <p>The <strong>utility (profit) of a rule </strong>is the total
utility (profit) generated by the rule in the sequences where it
appears. For example, the rule {1,2,3} ==&gt; {7} appears in sequences
s1,s2, and s4. In s1, the profit generated by that rule is 1$ + 4$ +
10$ + 2 $ = 17$. In s2, the profit generated by that rule is 1$ + 20$ +
4 + 2 $ = 27$. In s4, the profit generated by that rule is 3$ + 4$ + 5$
+ 1$ =13$. Thus, the total utility of that rule in the database is 17$
+ 27 $ + 13$ = 57 $</p>
  <p><strong>The HUSRM algorithm returns all high-utility sequential
rules,  that is each rule that meet the four following criteria:</strong></p>
  <ul>
    <li>the utility of the rule in the database is no less than a
minimum utility threshold set by the user,</li>
    <li>the confidence of the rule in the database is no less than a
minimum confidence threshold set by the user,</li>
    <li>the number of items in the antecedent (left side) of the rule
contains no more than a maximum number of items specified by the user,</li>
    <li>the number of items in the consequent (right side) of the rule
contains no more than a maximum number of items specified by the user,</li>
  </ul>
  <p> For example, if we run <strong>HUSRM </strong>with a<strong>
minimum utility of 40 </strong>and<strong> minconf = 0.70 (70 %)</strong>,
and a<strong> maximum antecedent and consequent size of 4 items</strong>,
we obtain 7 <strong>high-utility sequential rules</strong>:</p>
</blockquote>

<table align="center" border="1" width="486">

  <tbody>
    <tr>
      <td width="135"><strong> rule</strong></td>
      <td width="90"><strong>confidence</strong></td>
      <td width="86"><strong>utility</strong></td>
      <td width="147"><strong>support</strong></td>
    </tr>
    <tr>
      <td>1,4 ==&gt; 2,3,5,7</td>
      <td>100 %</td>
      <td>40</td>
      <td>1 sequence(s)</td>
    </tr>
    <tr>
      <td>1,3,4 ==&gt; 2,5,7</td>
      <td>100 %</td>
      <td>40</td>
      <td>1 sequence(s)</td>
    </tr>
    <tr>
      <td>1,2,3,4 ==&gt; 5,7</td>
      <td>100 %</td>
      <td>40</td>
      <td>1 sequence(s)</td>
    </tr>
    <tr>
      <td>1,2,3 ==&gt; 7</td>
      <td>100 %</td>
      <td>57</td>
      <td>3 sequence(s)</td>
    </tr>
    <tr>
      <td>1,3 ==&gt; 7 </td>
      <td>100 %</td>
      <td>45</td>
      <td>3 sequence(s)</td>
    </tr>
    <tr>
      <td>2,3 ==&gt; 7</td>
      <td>100 %</td>
      <td>52</td>
      <td>3 sequence(s)</td>
    </tr>
    <tr>
      <td>3 ==&gt; 7</td>
      <td>100 %</td>
      <td>40</td>
      <td>3 sequence(s)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If the database is a transaction database from a store, we could
interpret these results as rules representing the purchasing behavior
of customers, such that these rules have a high confidence and generate
a high profit. For example, the rule {1,3} -&gt; {7} means that all
customers buying the items 1 and 3 always buy the item 7 thereafter
(since the confidence is 100%) and that this rule has generated a
profit of 57 $ and appear in three sequences.<br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> of <strong>HUSRM </strong>is
defined as follows. It is a text file. </p>
  <ul>
    <li>Each lines represents a sequence of transactions. </li>
    <li>Each transaction is separated by the keyword -1.</li>
    <li>A transaction is a list of items (positive integers) separated
by single spaces and where each item is annotated with a generated sale
profit indicated between square brackets [ ]. The sale profit is a
positive integer.</li>
    <li>In a transaction, it is assumed that items are sorted according
to some order (eg. alphabetical order).</li>
    <li>Each sequence ends by the keyword "-2". Then, it is followed by
the keyword "SUtility:" followed by the sum of the utility (profit) of
all items in that sequence.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <blockquote>
    <p>1[1] 2[4] -1 3[10] -1 6[9] -1 7[2] -1 5[1] -1 -2 SUtility:27<br>
1[1] 4[12] -1 3[20] -1 2[4] -1 5[1] 7[2] -1 -2 SUtility:40<br>
1[1] -1 2[4] -1 6[9] -1 5[1] -1 -2 SUtility:15<br>
1[3] 2[4] 3[5] -1 6[3] 7[1] -1 -2 SUtility:16</p>
  </blockquote>
  <p>For examle, consider the first line. It means that the first
customer nbought items 1 and 2, and those items respectively generated
a profit of 1$ and 4$. Then, the customer bought item 3 for 10$. Then,
the customer bought item 6 for 9 $. Then, the customer bought items 7
for 2$. Then the customer bought item 5 for 1 $. Thus, this customer
has made 5 transaction. The total utility (profit) generated by that
sequence of transaction is 1$ + 4$ + 10$ + 9$ + 2$ + 1$ = 27 $.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> of <strong>HUSRM</strong>
is defined as follows. It is a text file, where each line represents a <strong>high
utility sequential rule</strong>. On each line, the items of the left
side of the rule (antecedent) are first listed. Each item is
represented by an integer, followed by a ",". After, the keyword "
==&gt;" appears. It is followed by the items in the right side of the
rule (consequent), each separated by ",". Then, the keyword "#SUP"
appears followed by the support of the rule. Then, the keyword "#CONF"
appears followed by the confidence of the rule. Then, the keyword
"#UTIL" appears followed by the utility of the rule.</p>
  <p>1,4 ==&gt; 2,3,5,7 #SUP: 1.0 #CONF: 1.0 #UTIL: 40.0<br>
1,3,4 ==&gt; 2,5,7 #SUP: 1.0 #CONF: 1.0 #UTIL: 40.0<br>
1,2,3,4 ==&gt; 5,7 #SUP: 1.0 #CONF: 1.0 #UTIL: 40.0<br>
1,2,3 ==&gt; 7 #SUP: 3.0 #CONF: 1.0 #UTIL: 57.0<br>
1,3 ==&gt; 7 #SUP: 3.0 #CONF: 1.0 #UTIL: 45.0<br>
2,3 ==&gt; 7 #SUP: 3.0 #CONF: 1.0 #UTIL: 52.0<br>
3 ==&gt; 7 #SUP: 3.0 #CONF: 1.0 #UTIL: 40.0 </p>
  <p>For example, the fourth line indicates that all customers buying
the items 1, 2 and 3 will then buy item 7 with a confidence of 100%,
and that this rule has generated a profit of 57 $ and appear in three
sequences.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>High utility sequential rulemining </strong>is a more
difficult problem than sequential rule mining and sequential pattern
mining. Therefore, <strong>high-utility sequential rule mining</strong>
algorithms are generally slower than those types of algorithms. The <strong>HUSRM
  </strong>algorithm is the first algorithm for high-utility sequential
rule mining.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>This is the original implementation of HUSRM.</p>
</blockquote>

<p>Where can I get more information about the HUSRM algorithm?</p>

<blockquote>
  <p>This is the article describing the HUSRM algorithm:</p>
  <p><em>Zida, S., Fournier-Viger, P., Wu, C.-W., Lin, J. C. W., Tseng,
V.S., (2015). <a href="http://www.philippe-fournier-viger.com/HUSRM_high_utility_sequential_rules.pdf">Efficient
Mining of High Utility Sequential Rules</a>. Proc. 11th Intern.
Conference on Machine Learning and Data Mining (MLDM 2015). Springer,
LNAI 9166, pp. 157-171.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="minfhm" id="example66"> </a></span></strong> Example 54 : Mining Minimal High-Utility Itemsets from
a transaction database with utility information using the MinFHM Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style9">MinFHM</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DB_utility</span><span class="Style2">.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
    (4) set the minimum utility to 30 <span class="Style2"> </span>and
    (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> MinFHM</span></strong> <strong><span class="Style9">DB_utility</span></strong>.txt
      output.txt 30 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">DB_utility.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    the file <strong><span class="Style2">"MainTestMinFHM.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is <strong>MinFHM</strong>?</p>
<blockquote>
  <p><strong>MinFHM </strong> (Fournier-Viger et al., 2016) is an
    algorithm for discovering<strong> minimal high-utility  itemsets </strong>in
    a transaction database containing utility information. </p>
  <p>There has been quite a huge amount of work on the topic of
    high-utility itemset mining in recent years. High-utility itemset mining consists of finding sets of items that yield a high profit in a database of customer transaactions where the purchase quantities of items in transactions is indicated and each item has a unit profit. Several algorithms have been proposed for high-utility itemset mining. However, they may find a huge number of patterns. These patterns are often very long and often represent rare cases, as in real-life, few customers exactly buy the same large set of items. For marketing purpose, a retailer may  be more interested in finding the smallest sets of items that generate a high profit, since it is easier to co-promote a small set of items targeted at  many customers rather than a large set of items targeted at few customers.  The
    MinFHM algorithm was designed to address this issues by discovering only the <strong>high-utility
    itemsets that are minimal</strong>.</p>
  <p>A high-utility itemset is said to be <strong>minimal</strong> if it has no subset that is also a high-utility itemset. In terms of application to transaction
    database, the concept of minimal high-utility itemsets can be understood as the smallest sets of items that yield a high profit.. The concept of minimal high-utility itemset can also be understood as the opposite of the concept of maximal high-utility itemset proposed in other work.</p>
  <p>This is the original implementation of MinFHM.</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
  <p><strong>MinFHM</strong> takes as input a transaction database
    with utility information and a minimum utility threshold <em>min_utility </em>(a positive integer)<em>.</em> Let's consider the following
    database consisting of 5 transactions (t1,t2...t5) and 7 items (1, 2,
    3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utility.txt</strong>"
    in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
    distribution<strong>.</strong></p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"><br></td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>3 5 1 2 4 6</td>
        <td>30</td>
        <td>1 3 5 10 6 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>3 5 2 4</td>
        <td>20</td>
        <td>3 3 8 6</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>3 1 4</td>
        <td>8</td>
        <td>1 5 2</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>3 5 1 7</td>
        <td>27</td>
        <td>6 6 10 5</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>3 5 2 7</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
        transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. profit
        generated by this item for this transaction)(the third column of the
        table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
      sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
      several applications in real life. One application is a customer
      transaction database. Imagine that each transaction represents the
      items purchased by a customer. The first customer named "<strong>t1</strong>"
      bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
      item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
      of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
  </blockquote>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p><em> </em>The output of<strong> MinFHM </strong>is the set of <strong>minimal high utility  itemset</strong>s having a utility no
    less than a <em><strong>min_utility</strong></em> threshold (a
    positive integer) set by the user. To explain what is a minimal high-utility itemsets, it is necessary to review some definitions. </p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
    items. The <strong>utility of an itemset in a transaction </strong>is
    the sum of the utility of its items in the transaction. For example,
    the utility of the itemset {1 4} in transaction t1 is 5 + 6 = 11 and
    the utility of {1 4} in transaction t3 is 5 + 2 = 7. The<strong> utility of an itemset in a database</strong> is the sum of its utility
    in all transactions where it appears. For example, the utility of {1 4}
    in the database is the utility of {1 4} in t1 plus the utility of {1 4}
    in t3, for a total of 11 + 7 = 18. A<strong> high utility itemset</strong> is an itemset such that its utility is no less than <em>min_utility.</em>  </p>
  <p>A minimal <strong>high utility itemsets (MinHUI)</strong> is a
    high-utility itemset that is has no subset that is a high-utility itemset</p>
  <p> For example, if we run <strong>MinFHM </strong>with a<strong> minimum utility of 30</strong>, we obtain 2 <strong>minimal high-utility
    itemsets</strong>:</p>
</blockquote>
<table align="center" border="1" width="226">
  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
    </tr>
    <tr>
      <td>{2, 4}</td>
      <td>30</td>
    </tr>
    <tr>
      <td>{2 5}</td>
      <td>31</td>
    </tr>
  </tbody>
</table>
<blockquote>
  <p>If the database is a transaction database from a store, we could
    interpret these results as all the smallest groups of items bought together that
    generated a profit of 30 $ or more (that are minimal). <br>
  </p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> of <strong>MinFHM </strong>is
    defined as follows. It is a text file. Each lines represents a
    transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
      item is represented by a positive integer. Each item is separated from
      the next item by a single space. It is assumed that all items within a
      same transaction (line) are sorted according to a total order (e.g.
      ascending order) and that no item can appear twice within the same
      transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
      transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
      each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
      3 5 2 4:20:3 3 8 6<br>
      3 1 4:8:1 5 2<br>
      3 5 1 7:27:6 6 10 5<br>
      3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
    2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
    respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
    transaction. The following lines follow the same format.</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> of <strong>MinFHM</strong> is defined as follows. It is a text file, where each line represents a <strong>high
    utility generator itemset</strong>. On each line, the items of the
    itemset are first listed. Each item is represented by an integer,
    followed by a single space. After, all the items, the keyword
    "#SUPPORT:" appears and is followed by the support of the itemset.
    Then, the keyword #UTIL: " appears and is followed by the utility of
    the itemset. For example, we show below the output file for this
    example. </p>
  <p>2 #SUP: 5 #UTIL: 22<br>
    1 #SUP: 5 #UTIL: 20<br>
    4 2 #SUP: 2 #UTIL: 30<br>
    1 5 #SUP: 2 #UTIL: 24</p>
  <p>For example, the third line indicates that the itemset {2, 4} has
    a support of 2 transactions and a utility of 30$. The following lines
    follows the same format.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p><strong>High utility itemset mining </strong>is a more difficult
    problem than frequent itemset mining. Therefore, <strong>high-utility
      itemset mining</strong> algorithms are generally slower than frequent
    itemset mining algorithms. The <strong>MinFHM </strong>algorithm
    was proposed in 2016 to discover only the  high-utility itemsets that are minimal. It was found that MinFHM can be orders of magitude faster than  algorithms such as FHM for mining all high-utility itemsets.</p>
</blockquote>
<p>Implementation details</p>
<blockquote>
  <p>This is the original implementation of the MinFHM algorithm</p>
</blockquote>
<p>Where can I get more information about the MinFHM algorithm?</p>
<blockquote>
  <p>This is the reference of the article describing the MinFHM 
    algorithm:</p>
  <p><em>Fournier-Viger, P., Lin, C.W., Wu, C.-W., Tseng, V. S., Faghihi, U. (2016). Mining Minimal High-Utility Itemsets. Proc. 27th International Conference on Database and Expert Systems Applications (DEXA 2016). Springer, LNCS, 13 pages, to appear</em></p>
</blockquote>
<h3><strong><span class="centered"><a name="skymine" id="example70"> </a></span></strong> Example 55 : Mining Skyline High-Utility Itemsets in
a transaction database with utility information using the SkyMine Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style9">SkyMine</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">SkyMineTransaction</span><span class="Style2">.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>"), (4) set the name of the second input file to "<span class="Style2">SkyMineItemUtilities.txt</span>" <span class="Style2"> </span>and
    (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SkyMine</span></strong> <strong><span class="Style9">SkyMineTransaction</span></strong>.txt
      output.txt SkyMineItemUtilities.txt  </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input files<span class="Style3"><strong> SkyMineTransaction</strong>.txt</span> and <span class="Style3"><strong>SkyMineItemUtilities</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    the file <strong><span class="Style2">"MainTestSkyMine_saveToFile"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is <strong>SkyMine</strong>?</p>
<blockquote>
  <p><strong>SkyMine </strong> (Goyal et al, 2015) is an
    algorithm for discovering<strong> skyline high-utility  itemsets </strong>in
    a transaction database containing utility information.  </p>
  <p>This is the original implementation of SkyMine.</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
 <strong>SkyMine</strong> takes as input a transaction database with purchase quantities, a table indicating the utility of items, and a minimum utility threshold <em>min_utility </em>(a positive integer)<em>.</em> Let's consider the following
    database consisting of 5 transactions (t1,t2...t5) and 9 items (1, 2,
    3, 4, 5, 6, 7, 8, 9). This database is provided in the text file "<strong>SkyMineTransaction.txt</strong>"
    in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
    distribution<strong>.</strong>
  <table align="center" border="1" width="525">
    <tbody>
      <tr>
        <td width="56"><br></td>
        <td width="156"><strong>Items</strong></td>
        <td width="291"><strong>Item purchase quantities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>1 3 4 8</td>
        <td>1 1 1 1</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>1 3 5 7</td>
        <td>2 6 2 5</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>1 2 3 4 5 6</td>
        <td>1 2 1 6 1 5</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>2 3 5 7</td>
        <td>2 2 1 2</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>1 3 4 9</td>
        <td>1 1 1 1</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a set of items (the first column of the table), </li>
      <li>the purchase quantities of these items in this
        transaction (the second column of the table),</li></ul>
    <p>For example, the second line of the database indicates that in the second transaction, the items 1, 3, 5, and 7 were purchased respectively with quantities of 2, 6, 2, and 5.</p>
    <p>Moreover, another table must be provided to indicate  the unit profit of each item (how much profit is generated by the sale of one unit of each item). For example, consider the utility table provided in the file "<strong>SkyMineItemUtilities.txt</strong> (below). The first line indicates that each unit sold of item 1 yield a profit of 5$.</p>
    <table align="center" border="1" width="463">
      <tbody>
        <tr>
          <td width="156"><strong>Item</strong></td>
          <td width="291"><strong>Utility (unit profit)</strong></td>
        </tr>
        <tr>
          <td>1</td>
          <td>5</td>
        </tr>
        <tr>
          <td>2</td>
          <td>2</td>
        </tr>
        <tr>
          <td>3</td>
          <td>1</td>
        </tr>
        <tr>
          <td>4</td>
          <td>2</td>
        </tr>
        <tr>
          <td>5</td>
          <td>3</td>
        </tr>
        <tr>
          <td>6</td>
          <td>1</td>
        </tr>
        <tr>
          <td>7</td>
          <td>1</td>
        </tr>
        <tr>
          <td>8</td>
          <td>1</td>
        </tr>
        <tr>
          <td>9</td>
          <td>25</td>
        </tr>
      </tbody>
    </table>
    <p>What are real-life examples of such a database? There are
      several applications in real life. One application is a customer
      transaction database. Imagine that each transaction represents the
      items purchased by a customer. The first customer named "<strong>t1</strong>"
      bought items 1, 3, 4 and 8. The purchase quantities of each item is respectively 1, 1, 1, and 1. The total amount
    of money spent in this transaction is (1*5)+(3*1)+(4*2)+(8*1)= 24 $.</p>
  </blockquote>
  <blockquote>&nbsp;
  </blockquote>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p><em> </em>The output of<strong> SkyMine </strong>is the set of <strong>skyline high utility  itemset</strong>s. To explain what is a skyline high-utility itemsets, it is necessary to review some definitions. </p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
    items. The utility of an item in a transaction is the product of its purchase quantity in the transaction by its unit profit. For example, the utility of item 3 in transaction t2 is (6*1)- 6 $. The <strong>utility of an itemset in a transaction </strong>is
    the sum of the utility of its items in the transaction. For example,
    the utility of the itemset {5 7} in transaction t2 is (2*3)+(5*1)=12$ and
    the utility of {5, 7} in transaction t4 is (1*3)+(2*1)=5. The<strong> utility of an itemset in a database</strong> is the sum of its utility
    in all transactions where it appears. For example, the utility of {5 7}
    in the database is the utility of {5 7} in t4 plus the utility of {5 7}
    in t5, for a total of 12 + 5= 17. The utility of an itemset X is denoted as u(X). Thus u({5 7})= 17$</p>
  <p>The <strong>support of an itemset</strong> is the number of transactions that contains the itemset. For example, the support of the itemset {5 7} 	 is sup({5 7}) = 2 transactions because it appears in transactions t4 and t5.</p>
  <p>An itemset X  is said to be<strong> dominating </strong>another itemset Y, if and only if, sup(X) â¥ sup(Y ) and  u(X) &gt; u(Y ), or, sup(X) &gt; sup(Y ) and u(X) â¥ u(Y ).</p>
  <p>A<strong> skyline high utility itemset</strong> is an itemset that is not dominated by another itemset in the transaction database.<em></em>  </p>
  <p> For example, if we run <strong>SkyMine</strong>, we obtain 3 <strong>skyline high-utility
    itemsets</strong>:</p>
</blockquote>
<table align="center" border="1" width="226">
  <tbody>
    <tr>
      <td width="113"><strong> itemsets</strong></td>
      <td width="97"><strong>utility</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>14</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>34</td>
    </tr>
    <tr>
      <td>{2, 3, 4, 5}</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
<blockquote>
  <p>If the database is a transaction database from a store, we could
    interpret these results as all the itemsets that are dominating the other itemsets in terms of selling frequencies and utilty.<br>
  </p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> of the transaction file of <strong>Skymine </strong>is
    defined as follows. It is a text file. Each lines represents a
  transaction. Each transaction is a list of items separated by single spaces. Each item is a positive integer followed by ":" and its purchase quantity in the transaction. Note that it is assume that items on each line are ordered according to some total order such as the alphabetical order. For example, for the previous example, the input file SkyMineTransactions.txt  is defined as follows:</p>
  <p>    1:1 3:1 4:1 8:1 <br>
    1:2 3:6 5:2 7:5 <br>
    1:1 2:2 3:1 4:6 5:1 6:5 <br>
    2:4 3:3 4:3 5:1 <br>
    2:2 3:2 5:1 7:2 <br>
    1:1 3:1 4:1 9:1 </p>
  <p>For example, the second line indicates that the items 1, 3, 5 and 7 respectively have a purchase quantity of 2, 6, 2 and 5 in that transaction.</p>
  <p>The input format of the second file, indicating the utility (unit profit) of each item, is defined as follows. Each line is an item, followed by a space, followed by the unit profit of the item. For example, consider the content of the file "SkyMineItemUtilities.txt", shown below. The first line indicates that the item 1 has a unit profit of 5$. The other lines follow the same format.</p>
  <blockquote>
    <p>1 5
      
      
      <br>
      2 2
      
      
      <br>
      3 1
      
      
      <br>
      4 2
      
      
      <br>
      5 3
      
      
      <br>
      6 1
      
      
      <br>
      7 1
      
      
      <br>
      8 1
      
      
      <br>
      9 25
      
      </p>
  </blockquote>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> of <strong>SkyMine</strong> is defined as follows. It is a text file, where each line represents a <strong>skyline high
    utility  itemset</strong>. On each line, the items of the
    itemset are first listed. Each item is represented by an integer,
    followed by a single space. Then, the keyword #UTIL: " appears and is followed by the utility of
    the itemset. For example, we show below the output file for this
    example. </p>
  <p>3 #UTIL: 14<br>
1 3 #UTIL: 34<br>
2 3 4 5 #UTIL: 40</p>
  <p>For example, the third line indicates that the itemset {2, 3, 4, 5} has
    a utility of 40$. The other lines
    follows the same format.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p><strong>SkyMine </strong>is the original algorithm for mining Skyline high-utility itemets.</p>
</blockquote>
<p>Where can I get more information about the algorithm?</p>
<blockquote>
  <p>This is the reference of the article describing the algorithm:</p>
  <p><em> Goyal, V., Sureka, A., &amp; Patel, D. (2015). Efficient Skyline Itemsets Mining.   In Proceedings of the Eighth International C* Conference on Computer Science &amp; Software   Engineering (pp. 119-124). ACM.</em></p>
</blockquote>
<h3><strong><span class="centered"><a name="uspan" id="example69"> </a></span></strong> Example 56 : Mining High-Utility Sequential Patterns from a
  Sequence Database with utility information using the USPAN Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style9">USPAN</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style9">DataBase_HUSRM</span><span class="Style2">.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
    (4) set the minimum utility to 35, (5) set the maximum
    pattern length to 4, <span class="Style2"> </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> USpan</span></strong> <strong><span class="Style9">DataBase_HUSRM</span></strong>.txt
      output.txt 35 4 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2"><strong><span class="Style9">DataBase_HUSRM</span></strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    the file <strong><span class="Style2">"MainTestUSpan.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is <strong>USpan</strong>?</p>
<blockquote>
  <p><strong>USpan </strong> (Zida et al, 2012) is a famous algorithm
    for discovering<strong> high-utility sequential patterns </strong>in a
    sequence database containing utility information. </p>
  <p>An typical example of a sequence database with utility information
    is a database of customer transactions containing sequences of
    transactions performed by customers, where each transaction is a set of
    items annotated with the profit generated by the sale of items. The
    goal of high-utility sequential rule mining is to find patterns of the form A, B, C, meaning that several customers have bought items A, followed by buying item B, followed by buying item C, and that this pattern generated a high
    profit. Although, this algorithm is designed for the scenario of
    sequence of transactions, the task is general and could be applied to
    other types of data such as sequences of webpages visited by user on a
    website, where the sale profit is replaced by the time spent on
    webpages.</p>
  <p>A limitation of the problem of high-utility sequential pattern mining is that patterns are only found based on the profit that they generate but there is no measure of the confidence that these patterns will be followed. For example, a pattern A,B,C may have a high utility but most customers may still buy items A,B without buying C. The alternative that proposed a solution to this problem is high-utility sequential rule mining, which discover rules of the form A -&gt; BC with a confidence (conditional probability). The algorithm HUSRM also offered in SPMF finds the high-utility sequential rules.</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
  <p><strong>USPAN</strong> takes as input a sequence database with
    utility information, a minimum utility threshold <em>min_utility </em>(a
    positive integer), an optionally, a maximum pattern length parameter (a positive integer) indicating the maximum number of items that a pattern should contani. </p>
  <p>Let's consider the following sequence database consisting of 4
    sequences of transactions (s1,s2, s3, s4) and 7 items (1, 2, 3, 4, 5,
    6, 7). This database is provided in the text file "<strong>DataBase_HUSRM.txt</strong>"
    in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
    distribution<strong>.</strong></p>
  <table align="center" border="1" width="595">
    <tbody>
      <tr>
        <td width="63"><br></td>
        <td width="350"><strong>Sequence</strong></td>
        <td width="160"><strong>Sequence utility</strong></td>
      </tr>
      <tr>
        <td><strong>s1</strong></td>
        <td>{1[1],2[4]},{3[10]},{6[9]},{7[2]},{5[1]}</td>
        <td>27</td>
      </tr>
      <tr>
        <td><strong>s2</strong></td>
        <td>{1[1],4[12]},{3[20]},{2[4]},{5[1],7[2]}</td>
        <td>40</td>
      </tr>
      <tr>
        <td><strong>s3</strong></td>
        <td>{1[1]},{2[4]},{6[9]},{5[1]}</td>
        <td>15</td>
      </tr>
      <tr>
        <td><strong>s4</strong></td>
        <td>{1[3],2[4],3[5]},{6[3],7[1]}</td>
        <td>16</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is a sequence:</p>
    <ul>
      <li>each sequence is an ordered list of transactions, such that
        transactions are enclosed by {} in this Example </li>
      <li>each transaction contains a set of items represented by
        integers </li>
      <li>each item is annotated with a utility value (e.g. sale
        profit), indicated between squared brackets</li>
      [ ].
      <li>the sum of the utilities (e.g. profit) of all items in the
        sequence is also indicated (the "sequence utility" column)</li>
    </ul>
    <p>Note that this representation of the input database is not exactly the same as in the paper about USpan. However, it is equivalent.</p>
    <p>What are real-life examples of such a database? A typical
      example is a database containing sequences of customer transactions.
      Imagine that each sequence represents the transactions made by a
      customer. The first customer named "<strong>s1</strong>" bought items 1
      and 2, and those items respectively generated a profit of 1$ and 4$.
      Then, the customer bought item 3 for 10$. Then, the customer bought
      item 6 for 9 $. Then, the customer bought items 7 for 2$. Then the
      customer bought item 5 for 1 $.</p>
  </blockquote>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p><em> </em>The output of<strong> USPAN </strong>is the set of <strong>high
    utility sequential patterns </strong>meeting the criteria specified by
    the user</p>
  <p>A <strong>sequential pattern</strong> is a sequence of itemsets  X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong> (sets of items). A sequential pattern is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1<em> is a subset of </em>Yi1, X2 <em>s a subset of </em> Yi2, ...
Xk <em>s a subset of </em> Yik.  </p>
  <p>The <strong>utility (profit) of a sequential pattern </strong>is the sum of the maximum 
    utility (profit) generated by the pattern in each sequences where it
    appears. For example, the rule (3)(7) appears in sequences
    s1,s2, and s4. In s1, the profit generated by that patern is 10 + 2 = 12 $. In s2, the profit generated by that pattern is 20 + 2 = 22 $. In s4, the profit generated by that pattern is 5+1 = 6$. Thus, the total utility of that rule in the database is 12 + 22 + 6 = 40 $.</p>
  <p><strong>The USPAN algorithm returns all high-utility sequential
    patterns,  such that each pattern  the two following criteria:</strong></p>
  <ul>
    <li>the utility of the rule in the database is no less than a
      minimum utility threshold set by the user,</li>
    <li>the confidence of the rule in the database is no less than a
      minimum confidence threshold set by the user,</li>
    <li>the number of items in the antecedent (left side) of the rule
      contains no more than a maximum number of items specified by the user,</li>
    <li>the number of items in the consequent (right side) of the rule
      contains no more than a maximum number of items specified by the user,</li>
  </ul>
  <p> For example, if we run<strong> USPAN</strong>with a<strong> minimum utility of 40 </strong>and<strong> a maximum pattern length of 4 items</strong>,
    we obtain 9 <strong>high-utility sequential patterns</strong>:</p>
</blockquote>
<table align="center" border="1" width="237">
  <tbody>
    <tr>
      <td width="135"><strong> rule</strong></td>
      <td width="86"><strong>utility</strong></td>
    </tr>
    <tr>
      <td>(1, 4), (3) (2)</td>
      <td>37</td>
    </tr>
    <tr>
      <td>(1, 4) (3) (7)</td>
      <td>35</td>
    </tr>
    <tr>
      <td>(1) (3) (7)</td>
      <td>36</td>
    </tr>
    <tr>
      <td>(3)</td>
      <td>35</td>
    </tr>
    <tr>
      <td>(3) (7)</td>
      <td>40</td>
    </tr>
    <tr>
      <td>(4) (3) (2)</td>
      <td>36</td>
    </tr>
    <tr>
      <td>(4) (3) (2) (5)</td>
      <td>37</td>
    </tr>
    <tr>
      <td>(4) (3) (2) (7)</td>
      <td>38</td>
    </tr>
    <tr>
      <td>(4) (3) (2) (5, 7)</td>
      <td>35</td>
    </tr>
  </tbody>
</table>
<blockquote>
  <p>If the database is a transaction database from a store, we could
    interpret these results as rules representing the purchasing behavior
    of customers, such that these rules have a high confidence and generate
    a high profit. For example, the rule {1,3} -&gt; {7} means that all
    customers buying the items 1 and 3 always buy the item 7 thereafter
    (since the confidence is 100%) and that this rule has generated a
  profit of 57 $ and appear in three sequences.<br>
  </p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> of <strong>USPAN </strong>is
    defined as follows. It is a text file. </p>
  <ul>
    <li>Each lines represents a sequence of transactions. </li>
    <li>Each transaction is separated by the keyword -1.</li>
    <li>A transaction is a list of items (positive integers) separated
      by single spaces and where each item is annotated with a generated sale
      profit indicated between square brackets [ ]. The sale profit is a
      positive integer.</li>
    <li>In a transaction, it is assumed that items are sorted according
      to some order (eg. alphabetical order).</li>
    <li>Each sequence ends by the keyword "-2". Then, it is followed by
      the keyword "SUtility:" followed by the sum of the utility (profit) of
      all items in that sequence.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <blockquote>
    <p>1[1] 2[4] -1 3[10] -1 6[9] -1 7[2] -1 5[1] -1 -2 SUtility:27<br>
      1[1] 4[12] -1 3[20] -1 2[4] -1 5[1] 7[2] -1 -2 SUtility:40<br>
      1[1] -1 2[4] -1 6[9] -1 5[1] -1 -2 SUtility:15<br>
      1[3] 2[4] 3[5] -1 6[3] 7[1] -1 -2 SUtility:16</p>
  </blockquote>
  <p>For examle, consider the first line. It means that the first
    customer nbought items 1 and 2, and those items respectively generated
    a profit of 1$ and 4$. Then, the customer bought item 3 for 10$. Then,
    the customer bought item 6 for 9 $. Then, the customer bought items 7
    for 2$. Then the customer bought item 5 for 1 $. Thus, this customer
    has made 5 transaction. The total utility (profit) generated by that
    sequence of transaction is 1$ + 4$ + 10$ + 9$ + 2$ + 1$ = 27 $.</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> of <strong>USPAN</strong> is defined as follows. It is a text file, where each line represents a <strong>high
    utility sequential pattern</strong>. Each line, first indicate the sequential patterns which is a list of itemsets. Each itemset is represented by a list of positive integers. Each itemset is separated by a -1. Then, the keyword "#UTIL"
    appears followed by the utility of the sequential pattern.</p>
  <p>1 4 -1 3 -1 2 -1 #UTIL: 37<br>
    1 4 -1 3 -1 7 -1 #UTIL: 35<br>
    1 -1 3 -1 7 -1 #UTIL: 36<br>
    3 -1 #UTIL: 35<br>
    3 -1 7 -1 #UTIL: 40<br>
    4 -1 3 -1 2 -1 #UTIL: 36<br>
    4 -1 3 -1 2 -1 5 -1 #UTIL: 37<br>
    4 -1 3 -1 2 -1 7 -1 #UTIL: 38<br>
    4 -1 3 -1 5 7 -1 #UTIL: 35  </p>
  <p>For example, the first line represents the pattern of buying items 1 and 4 together, then buying item 3, then buying item 2. This pattern has a total utility of 37, meaning that it generated a 37 $ profit. The other lines follow the same format.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p><strong>High utility sequential pattern mining </strong>is a more
    difficult problem than sequential pattern mining. Therefore, <strong>high-utility sequential pattern mining</strong> algorithms are generally slower than sequential pattern mining algorithm. For this reason, it is wise to use the optional maximum pattern length constraint when using USpan, to reduce the number of patterns found, and thus the size of the search space.</p>
  <p>It is also worth noting that in the USpan paper they do not compare the performance of their algorithm with previous algorithms for high-utility sequential pattern mining.</p>
</blockquote>
<p>Where can I get more information about the USPAN algorithm?</p>
<blockquote>
  <p>This is the article describing the USPAN algorithm:</p>
  <p><em>Yin, Junfu, Zhigang Zheng, and Longbing Cao. "USpan: an efficient algorithm for     mining high utility sequential patterns." Proceedings of the 18th ACM SIGKDD     international conference on Knowledge discovery and data mining. ACM, 2012.</em></p>
</blockquote>
<h3><strong><span class="centered"><a name="allassociationrules" id="example18"> </a></span></strong> Example 57 :
  Mining All Association Rules </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">FPGrowth_association_rules</span>"
    </strong>algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set minsup = 50 %, minconf= 60% (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FPGrowth_association_rules</span></strong><strong>
contextIGB</strong>.txt output.txt 50% 60%</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
this file <strong><span class="Style2">"MainTestAllAssociationRules_FPGrowth_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>It is an algorithm for discovering all association rules in a
transaction database, following the two steps approach proposed by
Agrawal &amp; Srikant (1993). The first step is to discover frequent
itemsets. The second step is to generate rules by using the frequent
itemsets. The main difference with Agrawal &amp; Srikant in this
implementation is that FPGrowth is used to generate frequent itemsets
instead of Apriori because FPGrowth is more efficient.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a transaction database (aka binary context) and two
thresholds named <em>minsup</em> (a value between 0 and 1) and <em>minconf
  </em>(a value between 0 and 1)<em>.</em></p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of distinct items. For
example, consider the following transaction database. It contains 6
transactions (t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For
example, the first transaction represents the set of items 1, 2, 4 and
5. This database is provided as the file <strong>contextIGB.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output of an association rule mining algorithm is a set of
association rules respecting the user-specified <em>minsup</em> and <em>minconf</em>
thresholds. To explain how this algorithm works, it is necessary to
review some definitions. An<strong> association rule</strong> X==&gt;Y
is a relationship between two itemsets (sets of items) X and Y such
that the intersection of X and Y is empty. The <strong>support of a
rule</strong> is the number of transactions that contains XâªY. The <strong>confidence
of a rule </strong>is the number of transactions that contains XâªY
divided by the number of transactions that contain X.</p>
  <p>If we apply an association rule mining algorithm, it will return
all the rules having a support and confidence respectively no less than
  <em>minsup</em> and <em>minconf</em>. </p>
  <p>For example, by applying the algorithm with minsup = 0.5 (50%),
minconf = 0.6 (60%), we obtains <strong>55 associations rules </strong>(<strong>run
the example in the SPMF distribution to see the result</strong>).</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by an integer, followed by a single space. After,
that the keyword "==&gt;" appears followed by a space. Then, the items
of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer.
Then, the keyword " #CONF: " appears followed by the confidence of the
rule represented by a double value (a value between 0 and 1,
inclusively). For example, here is a few lines from the output file for
this example:</p>
  <p> 1 ==&gt; 2 4 5 #SUP: 3 #CONF: 0,75<br>
5 ==&gt; 1 2 4 #SUP: 3 #CONF: 0,6<br>
4 ==&gt; 1 2 5 #SUP: 3 #CONF: 0,75</p>
  <p>For example, the first line indicates that the association rule
{1} --&gt; {2, 4, 5} has a support of 3 transactions and a confidence
of 75 %. The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>Association rule mining is traditionally performed in two steps :
(1) mining frequent itemset and (2) generating association rules by
using frequent itemsets. In this implementation, we use the <strong>FPGrowth</strong>
algorithm for Step 1 because it is very efficient. For Step 2, we use
the algorithm that was proposed by Agrawal &amp; Srikant (1994). </p>
  <p>Note that in SPMF, we offer also the alternative of choosing <strong>Apriori</strong>
instead of FPGrowth for Step1. This is called the <strong>"<span class="Style9">Apriori_association_rules</span>" </strong>algorithm
in the graphical user interface or command line interface. </p>
  <p><a name="cfpgrowth_ar"></a>Lastly, note that we offer also the
alternative of choosing <strong>CFPGrowth++</strong> instead of
FPGrowth for Step1. This is called the <strong>"<span class="Style9">CFPGrowth++_association_rules</span>"
  </strong>algorithm in the graphical user interface or command line
interface. <strong>CFPGrowth++</strong> allows to use multiple minimum
support threshold instead of a single minsup thresholds so the input
and output are slightly different (see the example about CFPGrowth++
for more details about this algorithm).</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p> The following technical report published in 1994 describes how to
generate association rules from frequent itemsets (Step 2):</p>
  <p><em>R. Agrawal and R. Srikant. <a href="http://www.philippe-fournier-viger.com/spmf/apriori_longer.pdf" rel="nofollow">Fast algorithms for mining association rules in large
databases</a>. Research Report RJ 9839, IBM Almaden Research Center,
San Jose, California, June 1994.</em></p>
  <p>You can also read <a rel="nofollow" href="http://www-users.cs.umn.edu/%7Ekumar/dmbook/ch6.pdf">chapter 6
of the book "introduction to data mining"</a> which provide a nice and
easy to understand introduction to how to discover frequent itemsets
and generate association rules.</p>
  <p>The following article describes the FPGrowth algorithm for mining
frequent itemsets:</p>
  <p><em>Jiawei Han, Jian Pei, Yiwen Yin, Runying Mao: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/fpgrowth_04.pdf">Mining
Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree
Approach</a>. Data Min. Knowl. Discov. 8(1): 53-87 (2004)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="lift" id="example36"> </a></span></strong>
Example 58 : Mining All Association Rules with the Lift
Measure</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FPGrowth_association_rules_with_lift</span>"</strong>
algorithm <strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set minsup = 50 %, minconf= 90% and <em>minlift</em> = 1 (5) click
"<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">FPGrowth_association_rules_with_lift</span></strong><strong>
contextIGB</strong>.txt output.txt 50% 90% 1</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAllAssociationRules_FPGrowth_version_with_lift.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>This is a variation of the algorithm for mining all association
rules from a transaction database, described in the previous example. </p>
  <p>Traditionally, association rule mining is performed by using two
interestingness measures named the <strong>support</strong> and <strong>confidence
  </strong>to evaluate rules. In this example, we show how to use
another popular measure that is called the <strong>lift </strong>or <strong>interest.</strong></p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a transaction database (aka binary context) and three
thresholds named <em>minsup </em>(a value between 0 and 1)<em>,
minconf </em>(a value between 0 and 1) and <em>minlift </em>(a value
between -infinity to +infinity)<em>.</em></p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of distinct items. For
example, consider the following transaction database. It contains 6
transactions (t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For
example, the first transaction represents the set of items 1, 2, 4 and
5. This database is provided as the file <strong>contextIGB.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output ? </p>

<blockquote>
  <p>The output of this algorithm is a set of all the association rules
that have a support, confidence and lift respectively higher than <em>minsup</em>,
  <em>minconf</em> and <em>minlift</em>. </p>
  <p>The <strong>lift</strong> of a rule X--&gt;Y is calculated as
lift(X--&gt;Y) = ( (sup(X U Y)/ N) / (sup(X)/ N*sup(Y)/ N ), where</p>
  <ul>
    <li> N is the number of transactions in the transaction database, </li>
    <li>sup(XâªY) is the number of transactions containing X and Y,</li>
    <li> sup(X) is the number of transactions containing X </li>
    <li> sup(Y) is the number of transactions containing Y. </li>
  </ul>
  <p>The <strong>confidence</strong> of a rule X--&gt;Y is calculated
as conf(X--&gt;Y) = sup(X U Y) / (sup(X)).</p>
  <p>The <strong>support</strong> of a rule X --&gt;Y is defined as
sup(X--&gt;Y) = sup(XâªY) / N</p>
  <p>By applying the algorithm with <em>minsup</em> = 0.5, <em>minconf</em>=
0.9 and <em>minlift</em> = 1 on the previous database, we obtains <strong>18
associations rules:</strong> </p>
  <pre>  rule 0:   4  ==&gt; 2      support :  0.66 (4/6) confidence :  1.0  lift :  1.0<br>  rule 1:   3  ==&gt; 2      support :  0.66 (4/6) confidence :  1.0  lift :  1.0<br>  rule 2:   1  ==&gt; 5      support :  0.66 (4/6) confidence :  1.0  lift :  1.2<br>  rule 3:   1  ==&gt; 2      support :  0.66 (4/6) confidence :  1.0  lift :  1.0<br>  rule 4:   5  ==&gt; 2      support :  0.833(5/6)  confidence :  1.0  lift :  1.0<br>  rule 5:   4 5  ==&gt; 2    support :  0.5 (3/6)   confidence :  1.0  lift :  1.0<br>  rule 6:   1 4  ==&gt; 5    support :  0.5 (3/6)   confidence :  1.0  lift :  1.2<br>  rule 7:   4 5  ==&gt; 1    support :  0.5 (3/6)   confidence :  1.0  lift :  1.5<br>  rule 8:   1 4  ==&gt; 2    support :  0.5 (3/6)   confidence :  1.0  lift :  1.0<br>  rule 9:   3 5  ==&gt; 2    support :  0.5 (3/6)   confidence :  1.0  lift :  1.0<br>  rule 10:  1 5  ==&gt; 2    support :  0.66 (4/6)  confidence :  1.0  lift :  1.0<br>  rule 11:  1 2  ==&gt; 5    support :  0.66 (4/6)  confidence :  1.0  lift :  1.2<br>  rule 12:  1  ==&gt; 2 5    support :  0.66 (4/6)  confidence :  1.0  lift :  1.2<br>  rule 13:  1 4 5  ==&gt; 2  support :  0.5 (3/6)   confidence :  1.0  lift :  1.0<br>  rule 14:  1 2 4  ==&gt; 5  support :  0.5 (3/6)   confidence :  1.0  lift :  1.2<br>  rule 15:  2 4 5  ==&gt; 1  support :  0.5 (3/6)   confidence :  1.0  lift :  1.5<br>  rule 16:  4 5  ==&gt; 1 2  support :  0.5 (3/6)   confidence :  1.0  lift :  1.5<br>  rule 17:  1 4  ==&gt; 2 5  support :  0.5 (3/6)   confidence :  1.0  lift :  1.5</pre></blockquote>

<p>How to interpret the results?</p>

<blockquote> For an association rule X ==&gt; Y, if the lift is equal
to 1, it means that X and Y are independent. If the lift is higher than
1, it means that X and Y are positively correlated. If the lift is
lower than 1, it means that X and Y are negatively correlated. For
example, if we consider the rule {1, 4} ==&gt; {2, 5}, it has a lift of
1.5, which means that the occurrence of the itemset {1, 4} is
positively correlated with the occurrence of {2, 5}.</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by an integer, followed by a single space. After,
that the keyword "==&gt;" appears followed by a space. Then, the items
of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer.
Then, the keyword " #CONF: " appears followed by the confidence of the
rule represented by a double value (a value between 0 and 1,
inclusively). Then, the keyword " #LIFT: " appears followed by the lift
of the rule represented by a double value (a value between -infinity to
+infinity). For example, here is a few lines from the output file for
this example:</p>
  <p> 1 ==&gt; 2 4 5 #SUP: 3 #CONF: 0,75 #LIFT: 1,5<br>
5 ==&gt; 1 2 4 #SUP: 3 #CONF: 0,6 #LIFT: 1,2 </p>
  <p>For example, the first line indicates that the association rule
{1} --&gt; {2, 4, 5} has a support of 3 transactions, a confidence of
75 % and a lift of 1.5 indicating a positive correlation (when the
value is higher than 1). The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In the source code version of SPMF, there are two versions of this
algorithm. The first version saves the result into memory (<strong><span class="Style2">"MainTestAllAssociationRules_FPGrowth_wifthLift</span></strong>").
The second one saves the result to a file (<strong><span class="Style2">"MainTestAllAssociationRules_FPGrowth_saveToFile_wifthLift</span></strong>").</p>
  <p>Note that we offer also the alternative of choosing <strong>CFPGrowth++</strong>
instead of FPGrowth. This is called the <strong>"<span class="Style9">CFPGrowth++_association_rules</span>_lift"
  </strong>algorithm in the graphical user interface or command line
interface. <strong>CFPGrowth++</strong> allows to use multiple minimum
support thresholds instead of a single minsup threshold so the input
and output are slightly different (see the example about CFPGrowth++
for more details about this algorithm).</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p> The following technical report published in 1994 describes how to
generate association rules from frequent itemsets (Step 2):</p>
  <p><em>R. Agrawal and R. Srikant. <a href="http://www.philippe-fournier-viger.com/spmf/apriori_longer.pdf" rel="nofollow">Fast algorithms for mining association rules in large
databases</a>. Research Report RJ 9839, IBM Almaden Research Center,
San Jose, California, June 1994.</em></p>
  <p>You can also read <a rel="nofollow" href="http://www-users.cs.umn.edu/%7Ekumar/dmbook/ch6.pdf">chapter 6
of the book "introduction to data mining"</a> which provide a nice and
easy to understand introduction to how to discover frequent itemsets
and generate association rules, and also describes the advantages of
using the lift measure.</p>
  <p>The following article describes the FPGrowth algorithm for mining
frequent itemsets:</p>
  <p><em>Jiawei Han, Jian Pei, Yiwen Yin, Runying Mao: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/fpgrowth_04.pdf">Mining
Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree
Approach</a>. Data Min. Knowl. Discov. 8(1): 53-87 (2004)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="gcd" id="example71"> </a></span></strong> Example 59 :
Mining All Association Rules using the GCD algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong> "<span class="Style9">GCD_association_rules</span>" </strong>algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
    (4) set minsup = 50 %, minconf= 60%, maxcomb = 3, and (5) click "<span class="Style2">Run
      algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> GCD_association_rules</span></strong><strong> contextIGB</strong>.txt output.txt 50% 60% 3</span> in a folder
    containing <span class="Style2">spmf.jar</span> and the example input
    file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
    this file <strong><span class="Style2">"MainTestGCDAssociationRules.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>
<p>What is this algorithm?</p>
<blockquote>
  <p>This algorithm finds the association rules in a given transaction database or sequence of transactions/events, using GCD calculations for prime numbers.It is an original algorithm implemented by Ahmed El-Serafy and Hazem El-Raffiee.</p>
</blockquote>
<p>What is the input?</p>
<blockquote>
  <p>The input is a transaction database (aka binary context) and three
    thresholds named <em>minsup</em> (a value between 0 and 1), <em>minconf </em>(a value between 0 and 1)<em>, </em>and <em>maxcomb </em>(a positive integer).</p>
  <p>A <strong>transaction database</strong> is a set of transactions.
    Each <strong>transaction</strong> is a set of distinct items. For
    example, consider the following transaction database. It contains 6
    transactions (t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For
    example, the first transaction represents the set of items 1, 2, 4 and
    5. This database is provided as the file <strong>contextIGB.txt</strong> in the SPMF distribution. It is important to note that an item is not
    allowed to appear twice in the same transaction and that items are
    assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>
<p>What is the output?</p>
<blockquote>
  <p>The output of an association rule mining algorithm is a set of
    association rules respecting the user-specified <em>minsup</em> and <em>minconf</em> thresholds. To explain how this algorithm works, it is necessary to
    review some definitions. An<strong> association rule</strong> X==&gt;Y
    is a relationship between two itemsets (sets of items) X and Y such
    that the intersection of X and Y is empty. The <strong>support of a
      rule</strong> is the number of transactions that contains XâªY. The <strong>confidence
        of a rule </strong>is the number of transactions that contains XâªY
    divided by the number of transactions that contain X.</p>
  <p>If we apply an association rule mining algorithm, it will return
    all the rules having a support and confidence respectively no less than <em>minsup</em> and <em>minconf</em>. </p>
  <p>For example, by applying the algorithm with minsup = 0.5 (50%),
    minconf = 0.6 (60%), and <em>maxcomb</em> = 3, we obtain <strong>56 associations rules </strong>(<strong>run
      the example in the SPMF distribution to see the result</strong>).</p>
  <p>Now let's explain the "maxcomb" parameter taken by the GCD algorithm. This parameter is used by the algorithm when finding the GCD (greatest common divisors) between two transactions. For example, consider 385, which comes from the multiplication of (5, 7 and 11), this actually means that (5), (7), (11), (5, 7), (5, 11), (7, 11), (5, 7, 11) are all common combinations between these two transactions. For larger GCD's, calculating all combinations grows exponentially in both time and memory. Hence, we introduced this parameter, to limit the maximum combinations' length generated from a single GCD. Although increasing this number might seem to provide more accurate results, the experiments showed that larger association rules occur at lower support (less important to the user). Hence, setting this parameter to values from <strong>1 to 4</strong> produces reasonable results.</p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
    transactions. Each lines represents a transaction. The items in the
    transaction are listed on the corresponding line. An item is
    represented by a positive integer. Each item is separated from the
    following item by a space. It is assumed that items are sorted
    according to a total order and that no item can appear twice in the
    same transaction. For example, for the previous example, the input file
    is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
      2 3 5<br>
      1 2 4 5<br>
      1 2 3 5<br>
      1 2 3 4 5<br>
      2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
    the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong> as an alternative to the default input format. The specification of the
    ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
    Most features of the ARFF format are supported except that (1) the
    character "=" is forbidden and (2) escape characters are not
    considered. Note that when the ARFF format is used, the performance of
    the data mining algorithms will be slightly less than if the native
    SPMF file format is used because a conversion of the input file will be
    automatically performed before launching the algorithm and the result
    will also have to be converted. This cost however should be small. </p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
    is a text file, where each line represents an <strong>association rule</strong>.
    On each line, the items of the rule antecedent are first listed. Each
    item is represented by an integer, followed by a single space. After,
    that the keyword "==&gt;" appears followed by a space. Then, the items
    of the rule consequent are listed. Each item is represented by an
    integer, followed by a single space. Then, the keyword " #SUP: "
    appears followed by the support of the rule represented by an integer.
    Then, the keyword " #CONF: " appears followed by the confidence of the
    rule represented by a double value (a value between 0 and 1,
    inclusively). For example, here is a few lines from the output file for
    this example:</p>
  <p> 1 ==&gt; 2 4 5 #SUP: 3 #CONF: 0,75<br>
    5 ==&gt; 1 2 4 #SUP: 3 #CONF: 0,6<br>
    4 ==&gt; 1 2 5 #SUP: 3 #CONF: 0,75</p>
  <p>For example, the first line indicates that the association rule
    {1} --&gt; {2, 4, 5} has a support of 3 transactions and a confidence
    of 75 %. The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
    default input format, the output format will be the same except that
    items will be represented by strings instead of integers.</p>
</blockquote>
<blockquote>&nbsp;</blockquote>
<p>Where can I get more information about this algorithm?</p>
<blockquote>
  <p> The GCD Association Rules algorithm is an original algorithm. More information about it can be obtained from the bitbucket repository dedicated to this algorithm: <a href="https://bitbucket.org/aelserafy/gcd-association-rules">https://bitbucket.org/aelserafy/gcd-association-rules</a></p>
</blockquote>
<h3><strong><span class="centered"><a name="example2" id="example7"> </a></span></strong>
  Example 60 : Mining the IGB basis of Association Rules</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">IGB</span>"</strong>
algorithm <strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set<strong> minsup = 50 %, minconf= 61%</strong> (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">IGB</span></strong><strong> contextIGB</strong>.txt
output.txt 50% 61%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestIGB_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>This algorithm mines a subset of all association rules that is
called IGB association rules (Informative and Generic Basis of
Association Rules) from a transaction database. </p>
  <p>To discover the IGB association rules, this algorithm performs two
steps: (1) first it discovers Closed itemsets and their associated
generators by applying the Zart algorithm. Then (2), association rules
are generated by using closed itemsets and generators.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a transaction database and two thresholds named <em>minsup</em>
(a value between 0 and 1) and <em>minconf</em> (a value between 0 and
1) . </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of distinct items. For
example, consider the following transaction database. It contains 6
transactions (t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For
example, the first transaction represents the set of items 1, 2, 4 and
5. This database is provided as the file <strong>contextIGB.txt</strong>
of the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is the <strong>IGB basis of association rules</strong>.
It is a compact set of association rules that is both informative and
generic. To explain what is the IGB basis of association rules, it is
necessary to review some definitions. An <strong>itemset</strong> is a
group of items. The <strong>support of an itemset </strong>is the
number of times that it appears in the database divided by the total
number of transactions in the database. For example, the itemset {1 3}
has a support of 33 % because it appears in 2 out of 6 transactions
from the database.</p>
  <p>An <strong>association rule</strong> X--&gt; Y is an association
between two itemsets X and Y that are disjoint. The <strong>support of
an association rule</strong> is the number of transactions that
contains X and Y divided by the total number of transactions. The<strong>
confidence of an association rule </strong>is the number of
transactions that contains X and Y divided by the number of
transactions that contains X.</p>
  <p> A <strong>closed itemset</strong> is an itemset that is strictly
included in no itemset having the same support. An itemset Y is the
closure of an itemset X if Y is a closed itemset, X is a subset of Y
and X and Y have the same support. <strong>A generator</strong> Y of a
closed itemset X is an itemset such that (1) it has the same support as
X and (2) it does not have a subset having the same support.</p>
  <p>The IGB set of association rules is the set of association rules
of the form X ==&gt; Y - X, where X is a minimal generator of Y, Y is a
closed itemset having a support higher or equal to <em>minsup</em>,
and the confidence of the rule is higher or equal to <em>minconf</em>.</p>
  <p>For example, by applying the <strong>IGB algorithm</strong> on
the transaction database previously described with minsup = 0.50 and
minconf= 0.61, we obtain the following set of association rules:</p>
</blockquote>

<table align="center" border="1" width="451">

  <tbody>
    <tr>
      <td width="157"><strong>Rule</strong></td>
      <td width="80"><strong>Support</strong></td>
      <td width="192"><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td>1 ==&gt; 2, 4, 5</td>
      <td>0.50</td>
      <td>0.75 </td>
    </tr>
    <tr>
      <td>4 ==&gt; 1, 2, 5</td>
      <td>0.50</td>
      <td>0.75 </td>
    </tr>
    <tr>
      <td>3 ==&gt; 2, 5</td>
      <td>0.50</td>
      <td>0.75 </td>
    </tr>
    <tr>
      <td> {} ==&gt; 2, 3</td>
      <td>0.66</td>
      <td>0.66 </td>
    </tr>
    <tr>
      <td>{} ==&gt; 1, 2, 5</td>
      <td>0.66</td>
      <td>0.66 </td>
    </tr>
    <tr>
      <td>{} ==&gt; 2, 4</td>
      <td>0.66</td>
      <td>0.66 </td>
    </tr>
    <tr>
      <td>{} ==&gt; 2, 5</td>
      <td>0.83</td>
      <td>0.83 </td>
    </tr>
    <tr>
      <td>{} ==&gt; 2</td>
      <td>1</td>
      <td>1 </td>
    </tr>
  </tbody>
</table>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by an integer, followed by a single space. After,
that the keyword "==&gt;" appears followed by a space. Then, the items
of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer.
Then, the keyword " #CONF: " appears followed by the confidence of the
rule represented by a double value (a value between 0 and 1,
inclusively). For example, here is a few lines from the output file for
this example:</p>
  <p> 1 ==&gt; 2 4 5 #SUP: 0,5 #CONF: 0,75<br>
5 ==&gt; 1 2 4 #SUP: 0,5 #CONF: 0,6</p>
  <p>For example, the first line indicates that the association rule
{1} --&gt; {2, 4, 5} has a support of 3 transactions and a confidence
of 75 %. The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Where can I get more information about IGB association rules?</p>

<blockquote>
  <p>This article described IGB rules:</p>
  <p>G. Gasmi, S. Ben Yahia, E. Mephu Nguifo, Y. Slimani: IGB: A New
Informative Generic Base of Association Rules. PAKDD 2005: 81-90</p>
</blockquote>

<h3><strong><span class="centered"><a name="example19" id="example19"> </a></span></strong>
Example 61 : Mining Perfectly Sporadic Association Rules</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Sporadic_association_rules</span>"</strong>
algorithm <strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextInverse</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set<strong> minsup = 1 %, maxsup = 60%, minconf= 60%</strong> (5)
click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Sporadic_association_rules</span></strong> <strong>contextInverse</strong>.txt
output.txt 1% 60%</span> <span class="Style2">60%</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextInverse.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestAllPerfectlySporadicRules.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>This is an algorithm for mining <strong>perfectly sporadic
association rules</strong>. The algorithm first uses <strong>AprioriInverse</strong>
to generate perfectly rare itemsets. Then, it uses these itemsets to
generate the association rules.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input of <strong>this algorithm</strong> is a transaction
database and three thresholds named <em>minusp, maxsup </em>and <em>minconf</em>.
A transaction database is a set of transactions. A transaction is a set
of distinct items (symbols), assumed to be sorted in lexical order. For
example, the following transactions database contains 5 transactions
(t1,t2...t5) and 5 items (1,2,3,4,5). This database is provided in the
file "<strong>contextInverse.txt</strong>" of the SPMF distribution:</p>
</blockquote>

<table align="center" border="1" width="316">

  <tbody>
    <tr>
      <td width="144"><strong>Transaction id</strong></td>
      <td width="156"><strong>Items</strong></td>
    </tr>
    <tr>
      <td><strong>t1</strong></td>
      <td>{1, 2, 4, 5}</td>
    </tr>
    <tr>
      <td><strong>t2</strong></td>
      <td>{1, 3}</td>
    </tr>
    <tr>
      <td><strong>t3</strong></td>
      <td>{1, 2, 3, 5}</td>
    </tr>
    <tr>
      <td><strong>t4</strong></td>
      <td>{2, 3}</td>
    </tr>
    <tr>
      <td><strong>t5</strong></td>
      <td>{1, 2, 4, 5}</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>The output is the set of <strong>perfectly sporadic association
rules </strong>respecting the <em>minconf</em> (a value in [0,1]), <em>minsup</em>
(a value in [0,1]) and <em>maxsup</em> (a value in [0,1]) parameters. </p>
  <p>To explain what it a <strong>perfectly sporadic association rule</strong>,
we need to review some definitions. An <strong>itemset</strong> is an
unordered set of distinct items. The<strong> support of an itemset </strong>is
the number of transactions that contain the itemset divided by the
total number of transactions. For example, the itemset {1, 2} has a
support of 60% because it appears in 3 transactions out of 5 (it
appears in t1, t2 and t5). A frequent itemset is an itemset that has a
support no less than the <em>maxsup </em>parameter. </p>
  <p>A <strong>perfectly rare itemset </strong>(aka sporadic itemset)
is an itemset that is not a frequent itemset and that all its subsets
are also not frequent itemsets. Moreover, it has to have a support
higher or equal to the <em>minsup</em> threshold.</p>
  <p>An <strong>association rule </strong> X==&gt;Y is a relationship
between two itemsets (sets of items) X and Y such that the intersection
of X and Y is empty. The <strong>support</strong> of a rule is the
number of transactions that contains XâªY divided by the total number of
transactions. The <strong>confidence</strong> of a rule is the number
of transactions that contains XâªY divided by the number of transactions
that contain X.</p>
  <p>A <strong>perfectly sporadic association rule </strong>X==&gt;Y
is an association rule such that the confidence is higher or equal to <em>minconf
  </em>and the support of any non empty subset of XâªY is lower than <em>maxsup</em>.</p>
  <p>For example, let's apply the algorithm with minsup = 0.1 %, maxsup
of 60 % and minconf = 60 %. </p>
  <p>The first step that the algorithm perform is to apply <strong>AprioriInverse</strong>
algorithm with <em>minsup</em> = 0.1 % and <em>maxsup</em> of 60 %.
The result is the following set of <strong>perfectly rare itemsets</strong>:</p>
</blockquote>

<table align="center" border="1" width="399">

  <tbody>
    <tr>
      <td><strong>Perfectly Rare Itemsets</strong></td>
      <td><strong>Support</strong></td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>60 %</td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>40 %</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>60 %</td>
    </tr>
    <tr>
      <td height="25">{4, 5}</td>
      <td>40 %</td>
    </tr>
    <tr>
      <td height="25"> {3, 5}</td>
      <td>20 % </td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Then, the second step is to generate all perfectly sporadic
association rules respecting <em>minconf</em> by using the perfectly
rare itemsets found in the first step. The result is :</p>
</blockquote>

<table align="center" border="1" width="315">

  <tbody>
    <tr>
      <td><strong>Rule</strong></td>
      <td><strong>Support</strong></td>
      <td><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td>5 ==&gt; 4</td>
      <td>40 %</td>
      <td>60 %</td>
    </tr>
    <tr>
      <td>4 ==&gt; 5</td>
      <td>40 %</td>
      <td>100 %</td>
    </tr>
  </tbody>
</table>

<p>How to interpret the result? </p>

<blockquote>
  <p>For example, consider the rule 5 ==&gt; 4. It means that if item 5
appears in a transaction, it is likely to be associated with item 4
with a confidence of 60 % (because 5 and 4 appears together in 40% of
the transactions where 5 appears). Moreover, this rule has a support of
40 % because it appears in 40% of the transactions of this database.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 <br>
1 2 4 5</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by an integer, followed by a single space. After,
that the keyword "==&gt;" appears followed by a space. Then, the items
of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer.
Then, the keyword " #CONF: " appears followed by the confidence of the
rule represented by a double value (a value between 0 and 1,
inclusively). For example, here is the output file for this example:</p>
  <p>5 ==&gt; 4 #SUP: 2 #CONF: 0,6<br>
4 ==&gt; 5 #SUP: 2 #CONF: 1</p>
  <p>For example, the first line indicates that the association rule
{5} --&gt; {4} has a support of 2 transactions and a confidence of 60
%. The second line follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The<strong> AprioriInverse </strong>algorithm and how to generate
sporadic rules are described in this paper:</p>
  <p><em>Yun Sing Koh, Nathan Rountree: <a href="http://www.philippe-fournier-viger.com/spmf/koh2005.pdf" rel="nofollow">Finding Sporadic Rules Using Apriori-Inverse</a>. PAKDD
2005: 97-106</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example20" id="example20"> </a></span></strong>Example 62 : Mining Closed Association Rules </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Closed_association_rules</span>"</strong>
algorithm <strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextZart</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set<strong> minsup = 60 %, minconf= 60%</strong> (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Closed_association_rules(using_fpclose)</span></strong> <strong>contextZart</strong>.txt
output.txt 60%</span> <span class="Style2">60%</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextZart.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestClosedAssociationRulesWithFPClose_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>It is an algorithm for mining "<strong>closed association rules</strong>",
which are a concise subset of all association rules.</p>
</blockquote>

<p>What is the input of this algorithm?</p>

<blockquote>
  <p>The input is a <strong>transaction database</strong> (aka binary
context) and two thresholds named <em><strong>minsup </strong></em>(a
value in [0,1] that represents a percentage) and <strong><em>minconf</em></strong>
(a value in [0,1] that represents a percentage). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of distinct items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextZart.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of this algorithm?</p>

<blockquote>
  <p>Given the minimum support threshold (<strong>minsup</strong>) and
minimum confidence threshold (<strong>minconf</strong>) set by the
user, the algorithm returns the set of closed association rules that
respect these thresholds. To explain what is a closed association rule,
it is necessary to review some definitions.</p>
  <p>An <strong>itemset</strong> is an unordered set of distinct
items. The<strong> support of an itemset </strong>is the number of
transactions that contain the itemset divided by the total number of
transactions. For example, the itemset {1, 2} has a support of 60%
because it appears in 3 transactions out of 5 (it appears in t1, t2 and
t5). A <strong>closed itemset</strong> is an itemset that is strictly
included in no itemset having the same support. </p>
  <p>An <strong>association rule </strong> X==&gt;Y is a relationship
between two itemsets (sets of items) X and Y such that the intersection
of X and Y is empty. The <strong>support</strong> of a rule X==&gt;Y
is the number of transactions that contains XâªY divided by the total
number of transactions. The <strong>confidence</strong> of a rule
X==&gt;Y is the number of transactions that contains XâªY divided by the
number of transactions that contain X. A <strong>closed association
rule</strong> is an association rule of the form X ==&gt; Y such that
the union of X and Y is a closed itemset.</p>
  <p>The algorithm returns all closed association rules such that their
support and confidence are respectively higher or equal to the <em>minsup
  </em>and <em>minconf</em> thresholds set by the user.</p>
  <p>For instance, by applying this algorithm with minsup = 60 %,
minconf= 60%, we obtains <strong>16 closed associations rules:</strong></p>
  <p>1 ==&gt; 3 #SUP: 3 #CONF: 0.75 // which means that this rule has a
support of 3 transactions and a confidence of 75 %<br>
3 ==&gt; 1 #SUP: 3 #CONF: 0.75 // which means that this rule has a
support of 3 transactions and a confidence of 75 %<br>
2 ==&gt; 5 #SUP: 4 #CONF: 1.0 // which means that this rule has a
support of 4 transactions and a confidence of 100 %<br>
5 ==&gt; 2 #SUP: 4 #CONF: 1.0 // ...<br>
2 5 ==&gt; 1 #SUP: 3 #CONF: 0.75<br>
1 5 ==&gt; 2 #SUP: 3 #CONF: 1.0<br>
1 2 ==&gt; 5 #SUP: 3 #CONF: 1.0<br>
1 ==&gt; 2 5 #SUP: 3 #CONF: 0.75<br>
2 ==&gt; 1 5 #SUP: 3 #CONF: 0.75<br>
5 ==&gt; 1 2 #SUP: 3 #CONF: 0.75<br>
3 5 ==&gt; 2 #SUP: 3 #CONF: 1.0<br>
2 3 ==&gt; 5 #SUP: 3 #CONF: 1.0<br>
2 5 ==&gt; 3 #SUP: 3 #CONF: 0.75<br>
5 ==&gt; 2 3 #SUP: 3 #CONF: 0.75<br>
3 ==&gt; 2 5 #SUP: 3 #CONF: 0.75<br>
2 ==&gt; 3 5 #SUP: 3 #CONF: 0.75<br>
  </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  </blockquote>
  <p>This file contains five lines (five transactions). Consider the
first line. It means that the first transaction is the itemset {1, 2,
4, 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by an integer, followed by a single space. After,
that the keyword "==&gt;" appears followed by a space. Then, the items
of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer.
Then, the keyword " #CONF: " appears followed by the confidence of the
rule represented by a double value (a value between 0 and 1,
inclusively). For example, here is the output file for this example:</p>
  <blockquote>
    <p>1 ==&gt; 3 #SUP: 3 #CONF: 0.75<br>
3 ==&gt; 1 #SUP: 3 #CONF: 0.75<br>
2 ==&gt; 5 #SUP: 4 #CONF: 1.0<br>
5 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
1 2 ==&gt; 5 #SUP: 3 #CONF: 1.0<br>
2 5 ==&gt; 1 #SUP: 3 #CONF: 0.75<br>
1 5 ==&gt; 2 #SUP: 3 #CONF: 1.0<br>
5 ==&gt; 1 2 #SUP: 3 #CONF: 0.75<br>
2 ==&gt; 1 5 #SUP: 3 #CONF: 0.75<br>
1 ==&gt; 2 5 #SUP: 3 #CONF: 0.75<br>
2 5 ==&gt; 3 #SUP: 3 #CONF: 0.75<br>
2 3 ==&gt; 5 #SUP: 3 #CONF: 1.0<br>
3 5 ==&gt; 2 #SUP: 3 #CONF: 1.0<br>
5 ==&gt; 2 3 #SUP: 3 #CONF: 0.75<br>
2 ==&gt; 3 5 #SUP: 3 #CONF: 0.75<br>
3 ==&gt; 2 5 #SUP: 3 #CONF: 0.75 </p>
  </blockquote>
  <p>For example, the last line indicates that the association rule {3}
--&gt; {2, 5} has a support of 3 transactions and a confidence of 75 %.
The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Implementation details and performance</p>
<blockquote>
  <p><strong>There are two versions of this algorithms implemented in SPMF.</strong> The first one uses CHARM for finding the frequent closed itemsets before generating the rules. The second one uses FPClose for finding the frequent closed itemsets before generating the rules. <strong>The version based on FPClose is generally faster than the version based on CHARM.</strong></p>
  <p><strong>In the release version of SPMF</strong>, the algorithm "<strong>Closed_association_rules(using_fpclose)</strong>" denotes the version using FPClose, while "<strong>Closed_association_rules</strong>" denotes the version based on CHARM.</p>
  <p><strong>In the source code version of SPMF</strong>, the files "MainTestClosedAssociationRulesWithFPClose_saveToMemory" and "MainTestClosedAssociationRulesWithFPClose_saveToFile" denotes respectively the version using FPClose which saves the result to memory or to a file. Moreover, the files "MainTestClosedAssociationRules_saveToMemory" and "MainTestClosedAssociationRules_saveToFile" denotes respectively the version using CHARM which saves the result to memory or to a file.</p>
</blockquote>
<p>Where can I get more information about closed association rules?</p>

<blockquote> The following Ph.D. thesis proposed "closed association
rules".
  <p> Szathmary, L. (2006). Symbolic Data Mining Methods with the Coron
Platform. Szathmary, L. PhD thesis, University Henri PoincarÃ© â Nancy
1, France.</p>
</blockquote>

<h3><strong><span class="centered"><a name="example21" id="example21"> </a></span></strong>
Example 63 : Mining Minimal Non Redundant Association
Rules </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">MNR</span>"</strong>
algorithm <strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextZart</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set<strong> minsup = 60 %, minconf= 60%</strong> (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> MNR</span></strong> <strong>contextZart</strong>.txt
output.txt 60%</span> <span class="Style2">60%</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextZart.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestMNRRules_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>This algorithm discover the set of "<strong>minimal non redundant
association rules</strong>" (<a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/Kryszkiewicz98.pdf">Kryszkiewicz,
1998</a>), which is a lossless and compact set of association rules.</p>
  <p>In this implementation we use the Zart algorithm for discovering
closed itemsets and their associated generators. Then, this information
is used to generate the "minimal non redundant association rules".</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>transaction database</strong> (aka binary
context), a threshold named <em><strong>minconf</strong></em> <em><strong>
  </strong></em>(a value in [0,1] that represents a percentage) and a
threshold named <em><strong>minsup</strong></em> <em><strong> </strong></em>(a
value in [0,1] that represents a percentage). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 2, 4 and 5. This database is
provided as the file <strong>contextZart.txt</strong> in the SPMF
distribution. It is important to note that an item is not allowed to
appear twice in the same transaction and that items are assumed to be
sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>This algorithm returns the set of<strong> minimal non redundant
association rules</strong>. </p>
  <p>To explain what is the set of minimal non redundant association
rules, it is necessary to review some definitions. An <strong>itemset</strong>
is a set of distinct items. The <strong>support of an itemset </strong>is
the number of times that it appears in the database divided by the
total number of transactions in the database. For example, the itemset
{1 3} has a support of 33 % because it appears in 2 out of 6
transactions from the database.</p>
  <p>An <strong>association rule</strong> X--&gt; Y is an association
between two itemsets X and Y that are disjoint. The <strong>support of
an association rule</strong> is the number of transactions that
contains X and Y divided by the total number of transactions. The<strong>
confidence of an association rule </strong>is the number of
transactions that contains X and Y divided by the number of
transactions that contains X.</p>
  <p> A <strong>closed itemset</strong> is an itemset that is strictly
included in no itemset having the same support. An itemset Y is the
closure of an itemset X if Y is a closed itemset, X is a subset of Y
and X and Y have the same support. <strong>A generator</strong> Y of a
closed itemset X is an itemset such that (1) it has the same support as
X and (2) it does not have a subset having the same support.</p>
  <p>The set of <strong>minimal non redundant association rules</strong>
is defined as the set of association rules of the form P1 ==&gt; P2 /
P1, where P1 is a generator of P2, P2 is a closed itemset, and the rule
has a support and confidence respectively no less than <em>minsup </em>and
  <em>mincon</em>f. </p>
  <p>For example, by applying this algorithm with minsup = 60 %,
minconf= 60% on the previous database, we obtains <strong>14 minimal
non redundant associations rules</strong>: </p>
  <pre>2 3 ==&gt; 5 support:: 0.6 confidence: 1<br>3 5 ==&gt; 2 support: 0.6 confidence: 1<br>1 ==&gt; 3 support: 0.6 confidence: 0,75<br>1 ==&gt; 2 5 support: 0.6 confidence: 0,75<br>1 2 ==&gt; 5 support: 0.6 confidence: 1<br>1 5 ==&gt; 2 support: 0.6 confidence: 1<br>3 ==&gt; 1 support: 0.6 confidence: 0,75<br>3 ==&gt; 2 5 support: 0.6 confidence: 0,75<br>2 ==&gt; 3 5 support: 0.6 confidence: 0,75<br>2 ==&gt; 1 5 support: 0.6 confidence: 0,75<br>2 ==&gt; 5 support: 0.8 confidence: 1<br>5 ==&gt; 2 3 support: 0.6 confidence: 0,75<br>5 ==&gt; 1 2 support: 0.6 confidence: 0,75<br>5 ==&gt; 2 support: 0.8 confidence: 1</pre>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
1 3<br>
1 2 3 5<br>
2 3 5<br>
1 2 3 5</p>
  </blockquote>
  <p>This file contains five lines (five transactions). Consider the
first line. It means that the first transaction is the itemset {1, 2,
4, 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by an integer, followed by a single space. After,
that the keyword "==&gt;" appears followed by a space. Then, the items
of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an double
value indicating a p. Then, the keyword " #CONF: " appears followed by
the confidence of the rule represented by a double value (a value
between 0 and 1, inclusively). For example, here is the output file for
this example:</p>
  <blockquote>
    <p>2 ==&gt; 5 #SUP: 0,8 #CONF: 1<br>
2 ==&gt; 3 5 #SUP: 0,6 #CONF: 0,75<br>
2 ==&gt; 1 5 #SUP: 0,6 #CONF: 0,75<br>
5 ==&gt; 2 #SUP: 0,8 #CONF: 1<br>
5 ==&gt; 2 3 #SUP: 0,6 #CONF: 0,75<br>
5 ==&gt; 1 2 #SUP: 0,6 #CONF: 0,75<br>
3 ==&gt; 2 5 #SUP: 0,6 #CONF: 0,75<br>
3 ==&gt; 1 #SUP: 0,6 #CONF: 0,75<br>
2 3 ==&gt; 5 #SUP: 0,6 #CONF: 1<br>
3 5 ==&gt; 2 #SUP: 0,6 #CONF: 1<br>
1 2 ==&gt; 5 #SUP: 0,6 #CONF: 1<br>
1 5 ==&gt; 2 #SUP: 0,6 #CONF: 1<br>
1 ==&gt; 3 #SUP: 0,6 #CONF: 0,75<br>
1 ==&gt; 2 5 #SUP: 0,6 #CONF: 0,75 </p>
  </blockquote>
  <p>For example, the last line indicates that the association rule {1}
--&gt; {2, 5} has a support of 60 % and a confidence of 75%. The other
lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Where can I get more information about closed association rules?</p>

<blockquote> The following article provides detailed information about
Minimal Non Redundant Association Rules:
  <p> M. Kryszkiewicz (1998). <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/Kryszkiewicz98.pdf">Representative
Association Rules and Minimum Condition Maximum Consequence Association
Rules</a>. Proc. of PKDD '98, Nantes, France, September 23-26. </p>
</blockquote>

<h3><strong><span class="centered"><a name="indirect" id="example31"> </a></span></strong>
Example 64 : Mining Indirect Association Rules with the
INDIRECT algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Indirect_association_rules</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIndirect</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 60 %, ts = 50 % and <em>minconf</em>= 10%
(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">Indirect_association_rules</span></strong> <strong>contextIndirect</strong>.txt
output.txt 60% 50% 10%</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextIndirect.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestIndirectRules_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is the INDIRECT algorithm?</p>

<blockquote>
  <p><strong>Indirect</strong> (Tan et al., KDD 2000; Tan, Steinbach
&amp; Kumar, 2006, p.469) is an algorithm for discovering indirect
associations between items in transactions databases. </p>
  <p>Why this algorithm is important? Because traditional association
rule mining algorithms focus on direct associations between itemsets.
This algorithm can discover indirect associations, which can be useful
in domains such as biology. Indirect association rule mining has
various applications such as stock market analysis and competitive
product analysis (Tan et al., 2000).</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input of the <strong>indirect</strong> <strong>algorithm </strong>is
a transaction database and three parameters named <em>minsup </em>(a
value in [0,1] that represents a percentage)<em>, ts </em>(a value in
[0,1] that represents a percentage) and <em>minconf</em> (a value in
[0,1] that represents a percentage). </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of distinct items. For
example, consider the following transaction database. It contains 5
transactions (t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example,
the first transaction represents the set of items 1, 4 and 5. This
database is provided as the file <strong>contextIndirect.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
    </tbody>
  </table>
  <p>The three numeric parameters of the<strong> indirect algorithm</strong>
are:</p>
</blockquote>

<ul>

  <li><em>minsup</em> : called the "mediator minimum support".</li>
  <li><em>ts</em> : called the "itempair minimum support"</li>
  <li><em>minconf</em> : representing the minimal confidence required
for indirect associations (note that in the original article it uses
the IS measure instead of the confidence).</li>
</ul>

<p>What is the output? </p>

<blockquote>
  <p>The result is all <strong>indirect associations</strong>
respecting the parameters <em>minsup</em>, <em>ts </em>and<em>
minconf</em>. An <strong>indirect association</strong> has the form
{x,y} ==&gt; M, where x and y are single items and M is an itemset
called the "mediator".</p>
  <p>An <strong>indirect association</strong> has to respect the
following conditions:</p>
  <ul>
    <li>The number of transactions containing all items of {x}âª M
divided by the total number of transaction must be higher or equal to <em>minsup</em>.</li>
    <li>The number of transactions containing all items of {y}âª M
divided by the total number of transaction must be higher or equal to <em>minsup</em>.</li>
    <li>The number of transactions containing {x,y} divided by the
total number of transaction must be higher or equal to <em>ts</em>.</li>
    <li>The confidence of {x}with respect to M and {y}with respect M
must be higher or equal to <em>minconf</em>. The confidence of an
itemset X with respect to another itemset Y is defined as the number of
transactions that contains X and Y divided by the number of
transactions that contain X. </li>
  </ul>
  <p>For example, by applying the <strong>indirect algorithm </strong>with
  <em>minsup</em> = 60 %, ts = 50 % and <em>minconf</em>= 10%, we
obtain <strong>3 indirect association rules:</strong></p>
  <ol>
    <li>{1, 2 | {4}}, which means that 1 and 2 are indirectly
associated by the mediator {4 }.</li>
    <li>{1, 5 | {4}}, which means that 1 and 5 are indirectly
associated by the mediator {4 }.</li>
    <li>{2, 5 | {4}}, which means that 1 and 5 are indirectly
associated by the mediator {4 }. </li>
  </ol>
  <p>To see additional details about each of these three indirect
rules, run this example.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 4<br>
2 3 4<br>
1 2 4 5<br>
4 5<br>
1 2 4 5</p>
  </blockquote>
  <p>This file contains five lines (five transactions). Consider the
first line. It means that the first transaction is the itemset {1, 4}.
The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>indirect
association rule</strong>. Each line starts by "(a=x b=y | mediator=M
)" indicating that the line represents the rule <strong> {x,y} ==&gt; M</strong>,
where <strong>x, y </strong>and <strong>M</strong> are integers
representing items. Then, the keyword "#sup(a,mediator)=" is followed
by the support of <strong>{x}âª M</strong> expressed as a number of
transactions (an integer). Then, the keyword "#sup(b,mediator)=" is
followed by the support of <strong>{y}âª M </strong>expressed as a
number of transactions (an integer). Then, the keyword
"#conf(a,mediator)= " is followed by the confidence of <strong>a</strong>
with respect to the mediator, expressed as a double value in the [0, 1]
interval. Then, the keyword "#conf(b,mediator)= " appears followed by
the confidence of <strong>b</strong> with respect to the mediator,
expressed as a double value in the [0, 1] interval.</p>
  <p>For example, the output file of this example is:</p>
  <blockquote>
    <p>(a=1 b=2 | mediator=4 ) #sup(a,mediator)= 3 #sup(b,mediator)= 3
#conf(a,mediator)= 1.0 #conf(b,mediator)= 1.0<br>
(a=1 b=5 | mediator=4 ) #sup(a,mediator)= 3 #sup(b,mediator)= 3
#conf(a,mediator)= 1.0 #conf(b,mediator)= 1.0<br>
(a=2 b=5 | mediator=4 ) #sup(a,mediator)= 3 #sup(b,mediator)= 3
#conf(a,mediator)= 1.0 #conf(b,mediator)= 1.0 </p>
  </blockquote>
  <p>This file contains three lines (three indirect association rules).
Consider the first line. It represents that items 1 and 2 are
indirectly associated by the item 4 as mediator. Furthermore, it
indicates that the support of {1, 4} is 3 transactions, the support of
{2,4} is 3 transactions, the confidence of item 1 with respect to item
4 is 100 % and the confidence of item 2 with respect to item 4 is 100%.
The other lines follow the same format.</p>
Note that if the ARFF format is used as input instead of the default
input format, the output format will be the same except that items will
be represented by strings instead of integers.</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The implementation attempts to be as faithful as possible to the
original algorithm, except that the confidence is used instead of the
IS measure.</p>
  <p>Note that some algorithms claimed to be more efficient than
Indirect such as HI-Mine but they have not been implemented in SPMF.</p>
</blockquote>

<p>Where can I get more information about indirect association rules?</p>

<blockquote>
  <p>The concept of indirect associations was proposed by Tan (2000) in
this conference paper:</p>
  <p> Pang-Ning Tan, Vipin Kumar, Jaideep Srivastava: Indirect
Association: Mining Higher Order Dependencies in Data. PKDD 2000:
632-637</p>
  <p>Moreover, note that the book "Introduction do data mining" by Tan,
Steinbach and Kumar provides an overview of indirect association rules
that is easy to read.</p>
</blockquote>

<h3><strong><span class="centered"><a name="FHSAR" id="example32"> </a></span></strong>
Example 65 : Hiding Sensitive Association Rules with the
FHSAR algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">FHSAR</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB.</span><span class="Style2">txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 50 %, <em>minconf</em>= 60% and <em>sar_file
= </em>"<span class="Style2">sar.txt</span>"(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9"> FHSAR </span></strong><strong>contextIGB</strong>.txt
output.txt 50% 60% sar.txt</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestFHSAR_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is FHSAR?</p>

<blockquote>
  <p>FHSAR is an algorithm for hiding sensitive association rules in a
transaction database. </p>
  <p>What are the applications? For example, consider a company that
want to release a transaction database to the public. But it does not
want to disclose some sensitive associations between items that appear
in the database and that could give a competitive advantage to their
competitor. The FHSAR algorithm can hide these associations by
modifying the database.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The FHSAR algorithm is designed to hide sensitive association
rules in a transaction database so that they will not be found for a
given <em>minsup</em> and <em>minconf</em> threshold generally used
by association rule mining algorithms. The input are: <em>minsup </em>(a
value in [0,1] that represents a percentage), <em>minconf </em>(a
value in [0,1] that represents a percentage)<em>, </em>a transaction
database and some sensitive association rules to be hidden. </p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of distinct items. For
example, consider the following transaction database. It contains 6
transactions (t1, t2, ..., t5, t6) and 5 items (1, 2, 3, 4, 5). For
example, the first transaction represents the set of items 1, 2, 4 and
5. This database is provided as the file <strong>contextIGB.txt</strong>
in the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>An <strong>association rule</strong> X==&gt;Yis an association
between two sets of items X and Y such that X and Y are disjoint. The <strong>support
of an association rule </strong>X==&gt;Yis the number of transactions
that contains both X and Y divided by the total number of transactions.
The <strong>confidence of an association rule</strong> X==&gt;Y is the
number of transactions that contains both X and Y divided by the number
of transactions that contain X. For example, the rule {1 2} ==&gt; {4
5} has a support of 50 % because it appears in 3 transactions out of 5.
Furthermore, it has a confidence of 75 % because {1 2} appears in 4
transactions and {1, 2, 4, 5} appears in 3 transactions.</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is a new transaction database such that the sensitive
rules will not be found if an association rule mining algorithm is
applied with <em>minsup</em> and <em>minconf</em>. </p>
  <p> For example, we can apply FHSAR with the parameters minsup = 0.5
and minconf = 0.60 to hide the following association rules provided in
the file "<strong>sar.txt</strong>":</p>
  <ul>
    <li>4 ==&gt; 1</li>
    <li>1 2 ==&gt; 4 5</li>
    <li>5 ==&gt; 2</li>
  </ul>
  <p>the result is a new transaction database where these rules are
hidden for the given thresholds <em>minsup</em> and <em>minconf</em>:</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
  <p>Note that the result of the algorithm is not always the same
because I use the HashSet data structure to represent transactions
internally and this data structure do not keep the order. Therefore,
the items that are removed may not be the same if the algorithm is run
twice.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>This algorithm takes two files as input.</p>
  <p>The <strong> first file </strong>is a text file containing
transactions (a transaction database) (e.g.<strong> contextIGB.txt</strong>).
Each line represents a transaction. The items in the transaction are
listed on the corresponding line. An item is represented by a positive
integer. Each item is separated from the following item by a space. It
is assumed that items are sorted according to a total order and that no
item can appear twice in the same transaction. For example, for the
previous example, the input file is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>This file contains six lines (six transactions). Consider the
first line. It means that the first transaction is the itemset {1, 2,
4, 5}. The following lines follow the same format. </p>
  <p>The <strong> second file </strong>is a text file containing
sensitive association rules to be hidden (e.g. <strong>sar.txt</strong>).
Each line is an association rule. First, the rule antecedent is
written. It is an itemset, where each item is represented by a positive
integer, and each item is separated from the following item by a single
space. Note that it is assumed that items within an itemset cannot
appear more than once and are sorted according to a total order. Then
the keyword " ==&gt; " appears followed by the rule consequent. The
consequent is an itemset where each item is represented by a positive
integer, and each item is separated from the following item by a single
space. For example, consider the file <strong>sar.txt</strong>.</p>
  <p>4 ==&gt; 1<br>
1 2 ==&gt; 4 5<br>
5 ==&gt; 2</p>
  <p>This file contains three lines (three association rules). The
second line indicates that the rule {1, 2} ==&gt; {4, 5} should be
hidden by the FHSAR algorithm.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file representing a transaction database. Each line
represents a transaction. The items in the transaction are listed on
the corresponding line. An item is represented by a positive integer.
Each item is separated from the following item by a space. It is
assumed that items are sorted according to a total order and that no
item can appear twice in the same transaction. For example, for the
previous example, an output file generated by the FHSAR algorithm is:</p>
  <p>4 5<br>
3 5<br>
4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  <p>In this example, the first line represents the transaction {4, 5}.
Other lines follow the same format.</p>
</blockquote>

<p>Where can I get more information about the FHSAR algorithm?</p>

<blockquote>
  <p>This algorithm was proposed in this paper:</p>
  <p> C.-C.Weng, S.-T. Chen, H.-C. Lo: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/FHSAR.pdf">A Novel
Algorithm for Completely Hiding Sensitive Association Rules</a>. ISDA
(3) 2008: 202-208</p>
</blockquote>

<h3><strong><span class="centered"><a name="topkrules" id="example33"> </a></span></strong>
Example 66 : Mining the Top-K Association Rules</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">TopKRules</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>k = 2 </em> and <em>minconf = 0.8</em> (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9"> TopKRules </span></strong><strong>contextIGB</strong>.txt
output.txt 2 80%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTopKRules.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TopKRules?</p>

<blockquote>
  <p><strong>TopKRules</strong> is an algorithm for discovering the <strong>top-k
association rules </strong>appearing in a transaction database. </p>
  <p>Why is it useful to discover top-k association rules? Because
other association rule mining algorithms requires to set a minimum
support (<em>minsup</em>) parameter that is hard to set (usually users
set it by trial and error, which is time consuming). <strong>TopKRules</strong>
solves this problem by letting users directly indicate <em>k</em>, the
number of rules to be discovered instead of using <em>minsup</em>. </p>
</blockquote>

<p>What is the input of TopKRules ?</p>

<blockquote>
  <p><strong>TopKRules </strong> takes three parameters as input:</p>
  <ul>
    <li>a transaction database,</li>
    <li>a parameter <em>k </em>representing the number of association
rules to be discovered (a positive integer),</li>
    <li>a parameter <em>minconf</em> representing the minimum
confidence that the association rules should have (a value in [0,1]
representing a percentage). </li>
  </ul>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 6 transactions
(t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For example, the
first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextIGB.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <blockquote>
    <table align="center" border="1" width="316">
      <tbody>
        <tr>
          <td width="144"><strong>Transaction id</strong></td>
          <td width="156"><strong>Items</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>{1, 2, 4, 5}</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>{2, 3, 5}</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>{1, 2, 4, 5}</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>{1, 2, 3, 5}</td>
        </tr>
        <tr>
          <td><strong>t5</strong></td>
          <td>{1, 2, 3, 4, 5}</td>
        </tr>
        <tr>
          <td><strong>t6</strong></td>
          <td>{2, 3, 4}</td>
        </tr>
      </tbody>
    </table>
  </blockquote>
</blockquote>

<p>What is the output of TopKRules ?</p>

<blockquote>
  <p><strong>TopKRules</strong> outputs the <strong>top-k association
rules</strong>. </p>
  <p>To explain what are <strong>top-k association rules</strong>, it
is necessary to review some definitions. An <strong>itemset</strong>
is a set of distinct items. The <strong>support of an itemset </strong>is
the number of times that it appears in the database divided by the
total number of transactions in the database. For example, the itemset
{1 3} has a support of 33 % because it appears in 2 out of 6
transactions from the database.</p>
  <p>An <strong>association rule</strong> X--&gt; Y is an association
between two itemsets X and Y that are disjoint. The <strong>support of
an association rule</strong> is the number of transactions that
contains X and Y divided by the total number of transactions. The<strong>
confidence of an association rule </strong>is the number of
transactions that contains X and Y divided by the number of
transactions that contains X.</p>
  <p>The<strong> top-k association rules</strong> are the <em>k </em>most
frequent <strong>association rules</strong> in the database having a
confidence higher or equal to <em>minconf</em>.</p>
  <p>For example, if we run <strong>TopKRules</strong> with <em>k = 2
  </em> and <em>minconf = 0.8</em>, we obtain the top-2 rules in the
database having a confidence higher or equals to 80 %.</p>
  <ul>
    <li>2 ==&gt; 5, which have a support of 5 (it appears in 5
sequences) and a confidence of 83% </li>
    <li>5 ==&gt; 2, which have a support of 5 (it appears in 5
sequences) and a confidence of 100 % </li>
  </ul>
  <p>For instance, the rule 2 ==&gt;5 means that if item 2 appears, it
is likely to be associated with item 5 with a confidence of 83% in a
transaction. Moreover, this rule has a support of 83 % because it
appears in five transactions (S1, S2 and S3) out of the six
transactions contained in this database.</p>
  <p>It is important to note that for some values of <em>k</em>, the
algorithm may return slightly more than <em>k</em> rules. This is can
happen if several rules have exactly the same support.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by a positive integer, followed by a single space.
After, that the keyword "==&gt;" appears followed by a space. Then, the
items of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer
(a number of transactions). Then, the keyword " #CONF: " appears
followed by the confidence of the rule represented by a double value (a
value between 0 and 1, inclusively). For example, here is a few lines
from the output file if we run TopKRules on contextIGB.txt with k=3 and
minconf=0.8 (80 %):</p>
  <p>3 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
1 ==&gt; 2 5 #SUP: 4 #CONF: 1.0<br>
5 ==&gt; 1 2 #SUP: 4 #CONF: 0.8<br>
1 2 ==&gt; 5 #SUP: 4 #CONF: 1.0<br>
2 5 ==&gt; 1 #SUP: 4 #CONF: 0.8<br>
1 5 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
2 ==&gt; 5 #SUP: 5 #CONF: 0.8333333333333334<br>
5 ==&gt; 2 #SUP: 5 #CONF: 1.0<br>
  </p>
  <p>For example, the first line indicates that the association rule
{3} --&gt; {2} has a support of 4 transactions and a confidence of 100
%. The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>TopKRules</strong> is a very efficient algorithm for
mining the <strong>top-k association rules</strong>.</p>
  <p>It provides the benefits that it is very intuitive to use. It
should be noted that the problem of <strong>top-k association rule
mining</strong> is more computationally expensive than the problem of
association rule mining. Using TopKRules is recommended for <em>k </em>values
of up to 5000, depending on the datasets. </p>
  <p>Besides, note that there is a variation of <strong>TopKRules</strong>
named <strong>TNR</strong> that is available in SPMF. The improvement
in TNR is that it eliminates some association rules that are deemed
"redundant" (rules that are included in other rules having the same
support and confidence - see the TNR example for the formal
definition). Using TNR is more costly than using TopKRules but it
brings the benefit of eliminating a type of redundancy in results.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TopKRules algorithm was proposed in this paper:</p>
  <p><em>Fournier-Viger, P., Wu, C.-W., Tseng, V. S. (2012). <a href="http://www.philippe-fournier-viger.com/spmf/top_k_association_rules_2012.pdf">Mining
Top-K Association Rules</a>. Proceedings of the 25th Canadian Conf. on
Artificial Intelligence (AI 2012), Springer, LNAI 7310, pp. 61-73.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="tnr" id="tnr"> </a></span></strong>
Example 67 : Mining the Top-K Non-Redundant Association
Rules</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">TNR</span>"</strong> algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style2">contextIGB</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
(e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (4)
set <em>k = 30,minconf = 0.5, </em>and<em> delta = 2 </em>(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9"> TNR </span></strong><strong>contextIGB</strong>.txt
output.txt 30 0.5 2</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextIGB.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTNR.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TopKRules?</p>

<blockquote>
  <p><strong>TNR</strong> is an algorithm for discovering the <strong>top-k
non-redundant association rules </strong>appearing in a transaction
database. It is an approximate algorithm in the sense that it always
generates non-redundant rules. But these rules may not always be the
top-k non-redundant association rules. TNR uses a parameter named <strong>delta</strong>,
which is a positive integer &gt;=0 that can be used to improve the
chance that the result is exact (the higher the delta value, the more
chances that the result will be exact).</p>
</blockquote>

<blockquote>
  <p>Why is it important to discover <strong>top-k non-redundant
association rules</strong>? Because other association rule mining
algorithms requires that the user set a minimum support (<em>minsup</em>)
parameter that is hard to set (usually users set it by trial and error,
which is time consuming). Moreover, the result of association rule
mining algorithms usually contains a high level of redundancy (for
example, thousands of rules can be found that are variation of other
rules having the same support and confidence). The <strong>TNR</strong>
algorithm provide a solution to both of these problems by letting users
directly indicate <em>k</em>, the number of rules to be discovered,
and by eliminating redundancy in results. </p>
</blockquote>

<p>What is the input of TNR ?</p>

<blockquote>
  <p><strong>TNR </strong> takes four parameters as input:</p>
  <ul>
    <li>a transaction database,</li>
    <li>a parameter <em>k </em>representing the number of rules to be
discovered (a positive integer &gt;= 1),</li>
    <li>a parameter <em>minconf</em> representing the minimum
confidence that association rules should have (a value in [0,1]
representing a percentage). </li>
    <li>a parameter <em>delta</em> (a positive integer &gt;=0) that is
used to increase the chances of having an exact result (because the TNR
algorithm is approximate).</li>
  </ul>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 6 transactions
(t1, t2, ..., t5, t6) and 5 items (1,2, 3, 4, 5). For example, the
first transaction represents the set of items 1, 2, 4 and 5. This
database is provided as the file <strong>contextIGB.txt</strong> in
the SPMF distribution. It is important to note that an item is not
allowed to appear twice in the same transaction and that items are
assumed to be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{2, 3, 4}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output of TNS ?</p>

<blockquote>
  <p><strong>TNR</strong> outputs an approximation of the <em>k </em>most
frequent <strong>non redundant association rules</strong> having a
confidence higher or equal to <em>minconf</em>.</p>
  <p>To explain what are <strong>top-k non redundant association rules</strong>,
it is necessary to review some definitions. An <strong>itemset</strong>
is a set of distinct items. The <strong>support of an itemset </strong>is
the number of times that it appears in the database divided by the
total number of transactions in the database. For example, the itemset
{1 3} has a support of 33 % because it appears in 2 out of 6
transactions from the database.</p>
  <p>An <strong>association rule</strong> X--&gt; Y is an association
between two itemsets X and Y that are disjoint. The <strong>support of
an association rule</strong> is the number of transactions that
contains X and Y divided by the total number of transactions. The<strong>
confidence of an association rule </strong>is the number of
transactions that contains X and Y divided by the number of
transactions that contains X.</p>
  <p>An <strong>association rule </strong><em>ra</em>: X â Y <strong>is
redundant </strong>with respect to another rule <em>rb</em> : X1 â Y1
if and only if:</p>
  <ul>
    <li><em>conf</em>(<em>ra</em>) = <em>conf</em>(<em>rb</em>) </li>
    <li><em>sup</em>(<em>ra</em>) = <em>sup</em>(<em>rb</em>)</li>
    <li>X1 â X â§ Y â Y1. </li>
  </ul>
  <p>The<strong> top-k non redundant association rules</strong> are the
  <em>k </em>most non-redundant frequent <strong>association rules</strong>
in the database having a confidence higher or equal to <em>minconf</em>.</p>
  <p>For example, If we run <strong>TNR </strong>with <em>k = 10 </em>
and <em>minconf = 0.5</em> and <em>delta = 2</em>, the following set
of rules is found</p>
  <pre>4, ==&gt; 2,  sup= 4  conf= 1.0<br>2, ==&gt; 1,5,  sup= 4  conf=0.66<br>2, ==&gt; 5,  sup= 5  conf= 0.8333333333333334<br>5, ==&gt; 2,  sup= 5  conf= 1.0<br>5, ==&gt; 1,2,  sup= 4  conf= 0.8<br>1, ==&gt; 2,5,  sup= 4  conf= 1.0<br>2, ==&gt; 3,  sup= 4  conf=0.66<br>2, ==&gt; 4,  sup= 4  conf=0.66<br>3, ==&gt; 2,  sup= 4  conf= 1.0<br>1,4, ==&gt; 2,5,  sup= 3  conf= 1.0</pre>
  <p>For instance, the association rule 2 ==&gt; 1 5 means that if
items 2 appears, it is likely to be associated with item 1 and item 5
with a confidence of 66 %. Moreover, this rule has a support 66 % (<em>sup
= 4</em>) because it appears in three transaction (S1, S2 and S3) out
of six transactions contained in this database.</p>
  <p>Note that for some values of <em>k </em>and some datasets, TNR
may return more than <em>k</em> association rules. This can happen if
several rules have exactly the same support, and it is normal. It
is also possible that the algorithm returns slightly less than <em>k</em>
association rules in some circumstances because the algorithm is
approximate.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is a text file containing
transactions. Each lines represents a transaction. The items in the
transaction are listed on the corresponding line. An item is
represented by a positive integer. Each item is separated from the
following item by a space. It is assumed that items are sorted
according to a total order and that no item can appear twice in the
same transaction. For example, for the previous example, the input file
is defined as follows:</p>
  <blockquote>
    <p>1 2 4 5<br>
2 3 5<br>
1 2 4 5<br>
1 2 3 5<br>
1 2 3 4 5<br>
2 3 4</p>
  </blockquote>
  <p>Consider the first line. It means that the first transaction is
the itemset {1, 2, 4 and 5}. The following lines follow the same format.</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong>
as an alternative to the default input format. The specification of the
ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
Most features of the ARFF format are supported except that (1) the
character "=" is forbidden and (2) escape characters are not
considered. Note that when the ARFF format is used, the performance of
the data mining algorithms will be slightly less than if the native
SPMF file format is used because a conversion of the input file will be
automatically performed before launching the algorithm and the result
will also have to be converted. This cost however should be small. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file, where each line represents an <strong>association rule</strong>.
On each line, the items of the rule antecedent are first listed. Each
item is represented by a positive integer, followed by a single space.
After, that the keyword "==&gt;" appears followed by a space. Then, the
items of the rule consequent are listed. Each item is represented by an
integer, followed by a single space. Then, the keyword " #SUP: "
appears followed by the support of the rule represented by an integer
(a number of transactions). Then, the keyword " #CONF: " appears
followed by the confidence of the rule represented by a double value (a
value between 0 and 1, inclusively). For example, here is a few lines
from the output file if we run TopKRules on contextIGB.txt with k=3 and
minconf=0.8 (80 %):</p>
  <p>2 ==&gt; 4 #SUP: 4 #CONF:0.66<br>
5 ==&gt; 1 2 #SUP: 4 #CONF: 0.8<br>
5 ==&gt; 2 #SUP: 5 #CONF: 1.0<br>
2 ==&gt; 5 #SUP: 5 #CONF: 0.8333333333333334<br>
2 ==&gt; 1 5 #SUP: 4 #CONF:0.66<br>
1 ==&gt; 2 5 #SUP: 4 #CONF: 1.0<br>
2 ==&gt; 3 #SUP: 4 #CONF:0.66<br>
3 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
4 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
4 5 ==&gt; 1 2 #SUP: 3 #CONF: 1.0<br>
  </p>
  <p>For example, the first line indicates that the association rule
{2} --&gt; {4} has a support of 4 transactions and a confidence of
66.66 %. The other lines follow the same format.</p>
  <p>Note that if the ARFF format is used as input instead of the
default input format, the output format will be the same except that
items will be represented by strings instead of integers.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>TNR</strong> is an efficient algorithm. It is based on the
TopKRules algorithm for discovering<strong> top-k association rule</strong>.
The main difference between TNR and TopKRules is that TNR includes
additional strategies to eliminate redundancy in results, and that TNR
is an approximate algorithm, while TopKRules is not.</p>
  <p>TNR and TopKRules are more intuitive to use than regular
association rule mining algorithms. However, it should be noted that
the problem of <strong>top-k association rule mining</strong> is more
computationally expensive than the problem of association rule mining.
Therefore, it is recommended to use TNR or TopKRules for <em>k </em>values
of up to 5000 depending on the dataset. If more rules should be found,
it could be better to find association rules with a classical
association rule mining algorithm like FPGrowth, for more efficiency.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TNR algorithm is described in this paper:</p>
  <p><em> Fournier-Viger, P., Tseng, V.S. (2012). <a href="http://www.philippe-fournier-viger.com/spmf/top_k_non_redundant_association_rules.pdf">Mining
Top-K Non-Redundant Association Rules</a>. Proc. 20th International
Symposium on Methodologies for Intelligent Systems (ISMIS 2012),
Springer, LNCS 7661, pp. 31- 40.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example8" id="example8"> </a></span></strong>
Example 68 : Clustering Values with the K-Means algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">KMeans</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">configKmeans</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>k =3</em> and distance function <em>= euclidian</em>,
and(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> KMeans</span></strong><strong> configKmeans</strong>.txt
output.txt 3 euclidian</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">configKmeans.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestKMeans_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is K-Means?</p>

<blockquote>
  <p><strong>K-Means </strong>is one of the most famous clustering
algorithm. It is used to separate a set of vectors of double values
into groups of vectors (clusters) according to their similarity. </p>
  <p>In this implementation the user can choose between various
distance functions to assess the similarity between vectors. SPMF
offers the Euclidian distance, correlation distance, cosine distance,
Manathan distance and Jaccard distance.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>K-Means </strong>takes as input a set of vectors
containing one or more double values, a parameter K (a positive integer
&gt;=1) indicating the number of clusters to be created, and a distance
function. </p>
  <p>The <strong>input file format</strong> is a text file such that
each line contains a list of double values separated by single spaces.
An example of input is provided in the file "<span class="Style3">configKmeans.txt</span>"
of the SPMF distribution. It contains 10 lines, each representing a
vector of data containing four doubles values separated by a space.
Note that it is assumed that all vectors have the same length.</p>
  <p>1 2 3 4<br>
1 6 8 8<br>
1 2 3 3<br>
2 4 5 5<br>
4 7 8 7<br>
7 6 8 9<br>
4 4 3 3<br>
2 2 5 5<br>
7 5 5 5<br>
5 6 8 9</p>
  <p>For example, the first line : 1 2 3 4 represents a vector with
four values, namely 1, 2, 3 and 4.</p>
  <p>The distance function can be the Euclidian distance, correlation
distance, cosine distance, Manathan distance and Jaccard distance. In
the command line or GUI of SPMF, the distance function is specified by
using one of these keywords: "euclidian", "correlation", "cosine",
"manathan" and "jaccard" as parameter.</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><strong>K-Means</strong> groups vectors in clusters according to
their similarity. In SPMF, the similarity is defined according to the
distance function chosen by the user such as the Euclidian distance. <strong>K-Means</strong>
returns K clusters or less. </p>
  <p>Note that running <strong>K-Means</strong> with the same data
does not always generate the same result because K-Means initializes
clusters randomly. </p>
  <p>By running K-Means on the previous input file and K=3, we can
obtain the following output file containing 3 clusters:</p>
  <p>cluster 1:
[1.0,2.0,3.0,4.0][1.0,2.0,3.0,3.0][2.0,4.0,5.0,5.0][4.0,4.0,3.0,3.0][2.0,2.0,5.0,5.0]<br>
cluster 2:
[7.0,6.0,8.0,9.0][1.0,6.0,8.0,8.0][4.0,7.0,8.0,7.0][5.0,6.0,8.0,9.0]<br>
cluster 3: [7.0,5.0,5.0,5.0]</p>
  <p>The <strong>output file format </strong>is defined as follows.
Each line is a cluster and lists the vectors contained in the cluster.
A vector is a list of double values separated by "," and between the
"[" and "]" characters.</p>
</blockquote>

<p>Where can I get more information about K-Means?</p>

<blockquote>
  <p>K-Means was proposed by MacQueen in 1967. K-means is one of the
most famous data mining algorithm. It is described in almost all data
mining books that focus on algorithms, and on many websites. By
searching on the web, you will find plenty of resources explaining
K-Means.</p>
</blockquote>

<h3><strong><span class="centered"><a name="bisecting" id="example43"> </a></span></strong>
Example 69 : Clustering Values with the Bisecting
K-Means algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">BisectingKMeans</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">configKmeans</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>k =3, </em> distance function <em>= euclidian</em>, <em>iter
    </em>= 10 and(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> BisectingKMeans</span></strong><strong> configKmeans</strong>.txt
output.txt 3 euclidian 10</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">configKmeans.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestBisectingKMeans_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is Bisecting K-Means?</p>

<blockquote>
  <p><strong>K-Means </strong>is one of the most famous clustering
algorithm. It is used to separate a set of vectors of double values
into groups of vectors (clusters) according to their similarity. </p>
  <p> The<strong> Bisecting K-Means</strong> algorithm is a variation
of the regular <strong>K-Means</strong> algorithm that is reported to
perform better for some applications. It consists of the following
steps: (1) pick a cluster, (2) find 2-subclusters using the basic
K-Means algorithm, * (bisecting step), (3) repeat step 2, the bisecting
step, for ITER times and take the split that produces the clustering,
(4) repeat steps 1,2,3 until the desired number of clusters is reached.</p>
  <p>In this implementation the user can choose between various
distance functions to assess the distance between vectors. SPMF offers
the Euclidian distance, correlation distance, cosine distance, Manathan
distance and Jaccard distance.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>Bisecting K-Means </strong>takes as input a set of
vectors containing one or more double values, a parameter K (a positive
integer &gt;=1) indicating the number of clusters to be created, a
distance function, and the parameter ITER. </p>
  <p>The <strong>input file format</strong> is a text file such that
each line contains a list of double values separated by single spaces.
An example of input is provided in the file "<span class="Style3">configKmeans.txt</span>"
of the SPMF distribution. It contains 10 lines, each representing a
vector of data containing four doubles values separated by a space.
Note that it is assumed that all vectors have the same length.</p>
  <p>1 2 3 4<br>
1 6 8 8<br>
1 2 3 3<br>
2 4 5 5<br>
4 7 8 7<br>
7 6 8 9<br>
4 4 3 3<br>
2 2 5 5<br>
7 5 5 5<br>
5 6 8 9</p>
  <p>For example, the first line : 1 2 3 4 represents a vector with
four values, namely 1, 2, 3 and 4.</p>
  <p>The distance function can be the Euclidian distance, correlation
distance, cosine distance, Manathan distance and Jaccard distance. In
the command line or GUI of SPMF, the distance function is specified by
using one of these keywords: "euclidian", "correlation", "cosine",
"manathan" and "jaccard" as parameter.</p>
  <p>The ITER specifies how much times the algorithm should repeat a
split to keep the best split. If it is set to a high value it should
provide better results but it should be more slow. Splits are evaluated
using the Squared Sum of Errors (SSE).</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><strong>Bisecting K-Means</strong> groups vectors in clusters
according to their similarity. In SPMF, the similarity is defined
according to the distance function chosen by the user such as the
Euclidian distance. <strong>K-Means</strong> returns K clusters or
less. </p>
  <p>Note that running <strong>Bisecting K-Means</strong> with the
same data does not always generate the same result because <strong>Bisecting
K-Means</strong> initializes clusters randomly. </p>
  <p>By running <strong>Bisecting K-Means </strong>on the previous
input file, with the euclidian distance, K=3 and ITER =10, we can
obtain the following output file containing 3 clusters:</p>
  <p>cluster 1:
[7.0,6.0,8.0,9.0][7.0,5.0,5.0,5.0][4.0,7.0,8.0,7.0][5.0,6.0,8.0,9.0][1.0,6.0,8.0,8.0]<br>
cluster 2: [4.0,4.0,3.0,3.0]<br>
cluster 3:
[1.0,2.0,3.0,4.0][1.0,2.0,3.0,3.0][2.0,4.0,5.0,5.0][2.0,2.0,5.0,5.0]</p>
  <p>The <strong>output file format </strong>is defined as follows.
Each line is a cluster and lists the vectors contained in the cluster.
A vector is a list of double values separated by "," and between the
"[" and "]" characters.</p>
</blockquote>

<p>Where can I get more information about Bisecting K-Means ?</p>

<blockquote>
  <p>The original K-Means was proposed by MacQueen in 1967. K-means is
one of the most famous data mining algorithm. It is described in almost
all data mining books that focus on algorithms, and on many websites.
By searching on the web, you will find plenty of resources explaining
K-Means.</p>
  <p>The Bisecting K-Means algorithms is described in this paper:</p>
  <p><em>A comparison of document clustering techniques", M. Steinbach,
G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="dbscan" id="example51"> </a></span></strong>
Example 70 : Clustering Values with the DBScan algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">DBScan</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">inputDBScan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minPts =2</em> and <em>epsilon = 5</em>,  and(5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> DBScan </span></strong><strong>inputDBScan</strong>.txt
output.txt 2 5  </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2"><strong>inputDBScan</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestDBScan_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is DBScan?</p>

<blockquote>
  <p><strong>DBScan </strong>is an old but famous clustering
algorithm. It is used to find clusters of points based on the density.</p>
  <p>Implementation note: To avoid having a O(n^2) time complexity,
this implementation uses a KD-Tree to store points internally. </p>
  <p>In this implementation the user can choose between various
distance functions to assess the similarity between vectors. SPMF
offers the Euclidian distance, correlation distance, cosine distance,
Manathan distance and Jaccard distance.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p><strong>DBScan </strong>takes as input (1) a set of vectors
(points) containing one or more double values, (2) a parameter <em>minPts</em>
(a positive integer &gt;=1) indicating the number of points that a core
point need to have in its neighborhood (see paper about DBScan for more
details) and (3) a radius <em>epsilon</em> that define the
neighborhood of a point. </p>
  <p>The <strong>input file format</strong> is a text file such that
each line contains a list of double values separated by single spaces.
An example of input is provided in the file "<span class="Style3">inpuDBScan.txt</span>"
of the SPMF distribution. It contains 10 lines, each representing a
vector of data containing four doubles values separated by a space.
Note that it is assumed that all vectors have the same length.</p>
  <p>1 1 <br>
0 1 <br>
1 0 <br>
10 10 <br>
10 13 <br>
13 13 <br>
54 54 <br>
55 55 <br>
89 89 <br>
57 55</p>
  <p>For example, the first line : 57 55 represents a vector with two
values, namely 57 and 55. </p>
  <p>The distance function used by DBscan is  the Euclidian distance.</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><strong>DBScans</strong> groups vectors (points) in clusters based
on density and distance between points. </p>
  <p>Note that it is normal that DBScan may generate a cluster having
less than minPts (this happens the neibhoors of a core points get
"stolen" by another cluster).</p>
  <p>Note also that DBScan eliminate points that are seen as noise (a
point having less than minPts neighboors within a radius of epsilon)</p>
  <p>By running <strong>DBScan</strong> on the previous input file and
minPts =2 and epsilon=5, we obtain the following output file containing
3 clusters:</p>
  <p>[1.0,0.0][0.0,1.0][1.0,1.0]<br>
[10.0,13.0][10.0,10.0][13.0,13.0]<br>
[54.0,54.0][57.0,55.0][55.0,55.0]</p>
  <p>The <strong>output file format </strong>is defined as follows.
Each line is a cluster and lists the vectors contained in the cluster.
A vector is a list of double values separated by "," and between the
"[" and "]" characters.</p>
</blockquote>

<p>Where can I get more information about DBScan?</p>

<blockquote>
  <p>DBScan is a most famous data mining algorithm for clustering. It is
described in almost all data mining books that focus on algorithms, and
on many websites. The original article describing DBScan is:</p>
  <p>Ester, Martin; Kriegel, Hans-Peter; Sander, JÃ¶rg; Xu, Xiaowei
(1996). Simoudis, Evangelos; * Han, Jiawei; Fayyad, Usama M., eds. A
density-based algorithm for discovering clusters in large spatial
databases with noise. Proceedings of the Second International
Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press.
pp. 226â231.</p>
</blockquote>

<h3><strong><span class="centered"><a name="optics" id="example52"> </a></span></strong>
Example 71 : Using Optics to extract a cluster-ordering
of points and DB-Scan style clusters</h3>

<p>What is <strong>OPTICS</strong>?</p>

<blockquote>
  <p><strong>OPTICS </strong>is a classic clustering algorithm. It
takes as input a set of points and output a cluster-ordering of points,
that is a total order on the set of points.</p>
  <p>This "cluster-ordering" of points can then be used to generate
density-based clusters similar to those generated by DBScan. </p>
  <p>In the paper describing Optics, the authors also proposed authors
tasks that can be done using the cluster-ordering of points such as
interactive visualization and automatically extracting hierarchical
clusters. Those tasks are not implemented here.</p>
  <p>Implementation note: To avoid having a O(n^2) time complexity,
this implementation uses a KD-Tree to store points internally.</p>
  <p>In this implementation the user can choose between various
distance functions to assess the similarity between vectors. SPMF
offers the Euclidian distance, correlation distance, cosine distance,
Manathan distance and Jaccard distance.</p>
</blockquote>

<p>How to run this example?</p>

<blockquote>
  <p>To generate a cluster-ordering of points using OPTICS:</p>
</blockquote>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">OPTICS-cluster-ordering</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">inputDBScan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minPts =2</em> and <em>epsilon = 5</em>, and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">OPTICS-cluster-ordering</span></strong> <strong>inputDBScan</strong>.txt
output.txt 2 5 </span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2"><strong>inputDBScan</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestOPTICS_extractClusterOrdering_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<blockquote>
  <p>To generate a DB-Scan style cluster of points using OPTICS:</p>
</blockquote>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">OPTICS-dbscan-clusters</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">inputDBScan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minPts =2</em>, epsilon = 5 and <em>epsilonPrime = 5</em>,
and(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> OPTICS-dbscan-clusters</span></strong> <strong>inputDBScan</strong>.txt
output.txt 2 5 5</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2"><strong>inputDBScan</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestOPTICS_extractDBScan_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is the input?</p>

<blockquote>
  <p><strong>Optics </strong>takes as input (1) a set of vectors
(points) containing one or more double values, (2) a parameter <em>minPts</em>
(a positive integer &gt;=1) indicating the number of points that a core
point need to have in its neighborhood (see paper about Optics for more
details) and (3) a radius <em>epsilon</em> that define the
neighborhood of a point. If clusters are generated, an extra parameter
named <em>epsilonPrime</em> is also taken as parameter. This latter
parameter can be set to the same value as <em>epsilon </em>or a
different value (see paper for details).</p>
  <p>The <strong>input file format</strong> is a text file such that
each line contains a list of double values separated by single spaces.
An example of input is provided in the file "<span class="Style3">inpuDBScan.txt</span>"
of the SPMF distribution. It contains 10 lines, each representing a
vector of data containing four doubles values separated by a space.
Note that it is assumed that all vectors have the same length.</p>
  <p>1 1 <br>
0 1 <br>
1 0 <br>
10 10 <br>
10 13 <br>
13 13 <br>
54 54 <br>
55 55 <br>
89 89 <br>
57 55</p>
  <p>For example, the first line : 57 55 represents a vector with two
values, namely 57 and 55.</p>
  <p>The distance function used by Optics is the Euclidian distance</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p><strong>Optics</strong> generates a a so-called cluster-ordering
of points, which is a list of points with their reachability distances.
For example, for minPts = 2 and epsilon = 5, the following cluster
ordering is generated, where each line indicates a data point and a
reachability distance. Note that a reachability distance equals to
"infinity" means "UNDEFINED" in the original paper. </p>
  <p>THE CLUSTER ORDERING:<br>
[data point] - reachability distance<br>
[1.0, 0.0] - Infinity<br>
[1.0, 1.0] - 1.0<br>
[0.0, 1.0] - 1.0<br>
[10.0, 10.0] - Infinity<br>
[10.0, 13.0] - 3.0<br>
[13.0, 13.0] - 3.0<br>
[54.0, 54.0] - Infinity<br>
[55.0, 55.0] - 1.4142135623730951<br>
[57.0, 55.0] - 2.0<br>
[89.0, 89.0] - Infinity</p>
  <p>The cluster ordering found by Optics can be used to do various
things. Among others, it can be used to generate DBScan style clusters
based on the density of points. This feature is implemented in SPMF and
is called ExtractDBScanClusters() in the original paper presenting
OPTICS. When extracting DBscan clusters it is possible to specify a
different epsilon value than the one used to extract the cluster
ordering. This new epsilon value is called "epsilonPrime" (see the
paper for details). By extracting clusters with epsilonPrime =5, we can
obtain the following clusters:</p>
  <p>CLUSTER(S) FOUND:<br>
Cluster 0<br>
1.0,0.0<br>
1.0,1.0<br>
0.0,1.0<br>
Cluster 1<br>
10.0,13.0<br>
10.0,10.0<br>
13.0,13.0<br>
Cluster 2<br>
54.0,54.0<br>
55.0,55.0<br>
57.0,55.0 </p>
  <p>Note that it is normal that OPTICS may generate a cluster having
less than minPts in some cases.</p>
  <p>Note also that OPTICS eliminate points that are seen as noise</p>
  <p>By running OPTICS<strong> </strong>on the previous input file and
minPts =2 and epsilon=5, we obtain the following output file containing
3 clusters: </p>
  <p>The <strong>output file format </strong>is defined as follows.
Each cluster is followed by several lines each representing a data
point in that cluster.</p>
</blockquote>

<p>Where can I get more information about OPTICS?</p>

<blockquote>
  <p>OPTICS is a quite popular data mining algorithm. The original
paper proposing this algorithm is:</p>
  <p>Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, JÃ¶rg Sander
(1999). OPTICS: Ordering Points To Identify the Clustering Structure.
ACM SIGMOD international conference on Management of data. ACM Press.
pp. 49â60.</p>
</blockquote>

<h3><strong><span class="centered"><a name="example1" id="example10"> </a></span></strong>
Example 72 : Clustering Values with a Hierarchical
Clustering algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HierarchicalClustering</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">configKmeans</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>maxDistance</em> to 4 (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Hierarchical_clustering</span></strong><strong>
configKmeans</strong>.txt output.txt 4 euclidian</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">configKmeans.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestHierarchicalClustering.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this algorithm? </p>

<blockquote>
  <p>We have implemented <strong>a hierarchical clustering algorithm</strong>
that is based on the description of Hierarchical Clustering Algorithms
from<br>
  <a rel="nofollow" href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html">http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html</a>.
The algorithm is used to separate a set of vectors of double values
into groups of vectors (clusters) according to their similarity. In
this implementation the euclidean distance is used to compute the
similarity.</p>
  <p>The algorithm works as follow. It first create a cluster for each
single vector. Then it recursively try to merge clusters together to
create larger clusters. To determine if two clusters can be merged, a
constant "threshold" indicate the maximal distance between two clusters
for merging. </p>
  <p>In this implementation the user can choose between various
distance functions to assess the similarity between vectors. SPMF
offers the Euclidian distance, correlation distance, cosine distance,
Manathan distance and Jaccard distance.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a set of vectors containing <em>double </em>values,
a parameter "maxdistance" and a distance function. </p>
  <p>The <strong>input file format</strong> is a text file such that
each line contains a list of double values separated by single spaces.
An example of input is provided in the file "<span class="Style3">configKmeans.txt</span>"
of the SPMF distribution. It contains 10 vectors (10 lines). Each
vector (line) is a list of double values separated by spaces. Each
vector is assumed to have the same size. For example, the first line of
the input file is "1 2 3 4". It represents a vector with four values,
namely 1,2,3 and 4.</p>
  <p>1 2 3 4<br>
1 6 8 8<br>
1 2 3 3<br>
2 4 5 5<br>
4 7 8 7<br>
7 6 8 9<br>
4 4 3 3<br>
2 2 5 5<br>
7 5 5 5<br>
5 6 8 9</p>
  <p>Furthermore, the user should also provide a parameter called <em><strong>maxDistance</strong></em>
(a positive value &gt; 0) to the algorithm. This parameter indicate the
maximal distance allowed between the mean of two clusters to merge them
into a single cluster.</p>
  <p>The distance function can be the Euclidian distance, correlation
distance, cosine distance, Manathan distance and Jaccard distance. In
the command line or GUI of SPMF, the distance function is specified by
using one of these keywords: "euclidian", "correlation", "cosine",
"manathan" and "jaccard" as parameter.</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is a set of cluster (a set of vectors). For example, by
running the algorithm with "<span class="Style3">configKmeans.txt</span>",
maxDistance = 4, and the euclidian distance function, we obtain the
following set of clusters:</p>
  <p>cluster 1:
[1.0,2.0,3.0,4.0][1.0,2.0,3.0,3.0][2.0,4.0,5.0,5.0][2.0,2.0,5.0,5.0][4.0,4.0,3.0,3.0]<br>
cluster 2: [1.0,6.0,8.0,8.0]<br>
cluster 3: [4.0,7.0,8.0,7.0][7.0,6.0,8.0,9.0][5.0,6.0,8.0,9.0]<br>
cluster 4: [7.0,5.0,5.0,5.0]</p>
  <p>The <strong>output file format </strong>is defined as follows.
Each line represents a cluster and lists vectors contained in the
cluster. A vector is a list of double values separated by "," and
between the "[" and "]" characters.</p>
</blockquote>

<p>Where can I get more information about Hierarchical clustering?</p>

<blockquote> There is a good introduction here: <br>
  <a rel="nofollow" href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html">http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html</a>.
  <p>Moreover, you could also read the free chapter on clustering from
the book "introduction to data mining" by Tan, Steinbach and Kumar on
the book website.</p>
</blockquote>

<blockquote> </blockquote>

<h3><strong><span class="centered"><a name="examplePrefixSpan" id="examplePrefixSpan"> </a></span></strong> Example 73: Mining Frequent Sequential Patterns Using the PrefixSpan Algorithm</h3>

<p>How to run this example?</p>

<blockquote>
  <p>To run the implementation of <strong>PrefixSpan</strong> by P.
Fournier-Viger (PFV):</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">PrefixSpan</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> PrefixSpan</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestPrefixSpan_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
    </li>
  </ul>
  <p>To run the version the implementation of <strong>PrefixSpan</strong>
by A. Gomariz PeÃ±alver (AGP):</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">PrefixSpan_AGP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em> and (5) click "<span class="Style2">Run
algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> PrefixSpan_AGP</span></strong><strong>
contextPrefixSpan</strong>.txt output.txt 50% </span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestPrefixSpan_AGP_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
  </ul>
</blockquote>

<p>What is PrefixSpan?</p>

<blockquote>
  <p><strong>PrefixSpan</strong> is an algorithm for discovering
sequential patterns in sequence databases, proposed by Pei et al.
(2001).</p>
</blockquote>

<p>What is the input of PrefixSpan?</p>

<blockquote>
  <p>The input of <strong>PrefixSpan</strong> is a sequence database
and a user-specified threshold named <em>minsup </em>(a value in
[0,1] representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
distinct items. For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 itemsets. It means
that item 1 was followed by items 1 2 and 3 at the same time, which
were followed by 1 and 3, followed by 4, and followed by 3 and 6. It is
assumed that items in an itemset are sorted in lexicographical order.
This database is provided in the file "<strong>contextPrefixSpan.txt</strong>"
of the SPMF distribution<strong>.</strong> Note that it is assumed that
no items appear twice in the same itemset and that items in an itemset
are lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of PrefixSpan?</p>

<blockquote>
  <p><strong>PrefixSpan</strong> discovers all <strong>frequent
sequential patterns</strong> occurring in a sequence database
(subsequences that occurs in more than <em>minsup</em> sequences of
the database.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>For example, if we run PrefixSpan with <em>minsup</em>= 50 % and
with a maximum pattern length of 100 items, 53 sequential patterns are
found. The list is too long to be presented here. An example of pattern
found is "(1,2),(6)" which appears in the first and the third sequences
(it has therefore a support of 50%). This pattern has a length of 3
because it contains three items. Another pattern is "(4), (3), (2)". It
appears in the second and third sequence (it has thus a support of 50
%). It also has a length of 3 because it contains 3 items.</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The <strong>PrefixSpan</strong> implementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database (sequences with ids 0 and 2). </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestPrefixSpan ...
.java"</span></strong> provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> PrefixSpan</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% 5 true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and patterns must have a maximum length of 5 items, and sequence
ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword "#SUP:" appears followed
by an integer indicating the support of the pattern as a number of
sequences. For example, a few lines from the output file from the
previous example are shown below:</p>
  <blockquote>
    <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>PrefixSpan </strong>is one of the fastest sequential
pattern mining algorithm. However, the SPAM and SPADE implementation in
SPMF can be faster than <strong>PrefixSpan </strong> (see the "<a href="http://www.philippe-fournier-viger.com/spmf/performance.php">performance</a>" section of the website for a
performance comparison).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>Note that in the source code, we also provide examples of how to
keep the result into memory instead of saving it to a file. This can be
useful if the algorithms are integrated into another Java software.</p>
  <ul>
    <li>For the AGP version of PrefixSpan, the file <span class="Style2">MainTestPrefixSpan_AGP_saveToMemory </span>shows how
to run the algorithm and keep the result into memory. </li>
    <li>For the PFV version, the file <strong><span class="Style2">MainTestPrefixSpan_saveToMemory.java
      </span></strong><span class="Style7">show how to run the
algorithm to keep the result into memory</span>.</li>
  </ul>
  <p>Note also that in the source code, there is a version of
PrefixSpan based on the PFV version that takes as input a dataset with
strings instead of integers. It can be run by using the files <strong><span class="Style2">MainTestPrefixSpan_WithStrings_saveToMemory.java</span></strong>
and <strong><span class="Style2">MainTestPrefixSpanWithStrings_saveToFile.java</span></strong>.
For the graphical user interface version of SPMF, it is possible to use
the version of PrefixSpan that uses Strings instead of integer by
selecting "<span class="Style3">PrefixSpan with strings</span>" and to
test it with the input file <span class="Style3">contextPrefixSpanStrings.txt</span>.
The version of PrefixSpan with Strings was made to temporarily
accommodate the needs of some users of SPMF. In the future, it may be
replaced by a more general mechanism for using files with strings for
all algorithms.</p>
</blockquote>

<p>Where can I get more information about PrefixSpan?</p>

<blockquote>
  <p>The PrefixSpan algorithm is described in this article:</p>
  <p><em> J. Pei, J. Han, B. Mortazavi-Asl, J. Wang, H. Pinto, Q. Chen,
U. Dayal, M. Hsu: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/prefixspan.pdf">Mining
Sequential Patterns by Pattern-Growth: The PrefixSpan Approach</a>.
IEEE Trans. Knowl. Data Eng. 16(11): 1424-1440 (2004)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="gsp" id="examplePrefixSpan3">
</a></span></strong> Example 74 : Mining Frequent
Sequential Patterns Using the GSP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">GSP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%, </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> GSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGSP_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is GSP?</p>

<blockquote>
  <p><strong>GSP</strong> is one of the first algorithm for discovering
sequential patterns in sequence databases, proposed by Srikant et al.
(1992). It uses an Apriori-like approach for discovering sequential
patterns. Note that this version does not include the constraints
proposed in the article.</p>
</blockquote>

<p>What is the input of GSP?</p>

<blockquote>
  <p>The input of <strong>GSP</strong> is a sequence database and a
user-specified threshold named <em>minsup </em>(a value in [0,1]
representing a percentage). Moreover, the implementation in SPMF adds
another parameter, which is the maximum sequential pattern length in
terms of items.</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
distinct items. For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 itemsets. It means
that item 1 was followed by items 1 2 and 3 at the same time, which
were followed by 1 and 3, followed by 4, and followed by 3 and 6. It is
assumed that items in an itemset are sorted in lexicographical order.
This database is provided in the file "<strong>contextPrefixSpan.txt</strong>"
of the SPMF distribution<strong>.</strong> Note that it is assumed that
no items appear twice in the same itemset and that items in an itemset
are lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of GSP?</p>

<blockquote>
  <p><strong>GSP</strong> discovers all <strong>frequent sequential
patterns</strong> occurring in a sequence database (subsequences that
occurs in more than <em>minsup</em> sequences of the database.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1<em data-bm="85">â</em>Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>For example, if we run GSP with <em>minsup</em>= 50 % and with a
maximum pattern length of 100 items, 53 sequential patterns are found.
The list is too long to be presented here. An example of pattern found
is "(1,2),(6)" which appears in the first and the third sequences (it
has therefore a support of 50%). This pattern has a length of 3 because
it contains three items. Another pattern is "(4), (3), (2)". It appears
in the second and third sequence (it has thus a support of 50 %). It
also has a length of 3 because it contains 3 items.</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The <strong>GSP</strong> implementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database (sequences with ids 0 and 2). </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestGSP ... .java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> GSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% 5 true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>See the "<a href="http://www.philippe-fournier-viger.com/spmf/performance.php">performance</a>" section of the
website for a performance comparison with other sequential pattern
mining algorithm.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The implementation is faithful to the article, except that the gap
constraints and window constraints are currently not implemented (will
be considered in future versions of SPMF).</p>
  <p>Also note that in the source code, we also provide an example of
how to run GSP and keep the result into memory instead of saving it to
a file ("<span class="Style2">MainTestGSP_saveToMemory.java</span>".).
This can be useful if the algorithms are integrated into another Java
software. </p>
</blockquote>

<p>Where can I get more information about GSP?</p>

<blockquote>
  <p>The GSP algorithm is described in this article:</p>
  <p><em> R. Srikant and R. Agrawal. 1996. <a href="http://www.philippe-fournier-viger.com/spmf/GSP96.pdf" rel="nofollow">Mining Sequential Patterns: Generalizations and
Performance Improvements</a>. In Proceedings of the 5th International
Conference on Extending Database Technology: Advances in Database
Technology (EDBT '96), Peter M. G. Apers, Mokrane Bouzeghoub, and
Georges Gardarin (Eds.). Springer-Verlag, London, UK, UK, 3-17. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="spade" id="examplePrefixSpan4"> </a></span></strong> Example 75: Mining Frequent Sequential Patterns Using the SPADE Algorithm</h3>

<blockquote>
  <p>How to run this example?</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">SPADE</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SPADE</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestSPADE_AGP_FatBitMap_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
  </ul>
  <p>What is SPADE?</p>
  <blockquote>
    <p><strong>SPADE</strong> is a popular sequential pattern mining
algorithm proposed by Zaki</p>
  </blockquote>
  <p>What is the input of SPADE?</p>
  <blockquote>
    <p>The input of <strong>SPADE is</strong> a sequence database and
a user-specified threshold named <em>minsup </em>(a value in [0,1]
representing a percentage). Moreover, the implementation in SPMF adds
another parameter, which is the maximum sequential pattern length in
terms of items.</p>
    <p>A <strong>sequence database</strong> is a set of sequences
where each sequence is a list of itemsets. An itemset is an unordered
set of distinct items. For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 itemsets. It means
that item 1 was followed by items 1 2 and 3 at the same time, which
were followed by 1 and 3, followed by 4, and followed by 3 and 6. It is
assumed that items in an itemset are sorted in lexicographical order.
This database is provided in the file "<strong>contextPrefixSpan.txt</strong>"
of the SPMF distribution<strong>.</strong> Note that it is assumed that
no items appear twice in the same itemset and that items in an itemset
are lexically ordered.</p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5 6), (1 2), (4 6), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1 6), (3), (2), (3)</td>
      </tr>
    </tbody>
  </table>
  <p>What is the output of SPADE?</p>
  <blockquote>
    <p><strong>SPADE </strong>discovers all <strong>frequent
sequential patterns</strong> occurring in a sequence database
(subsequences that occurs in more than <em>minsup</em> sequences of
the database.</p>
    <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
    <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
    <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are
itemsets<strong> is said to occur in another sequence SB</strong> = Y1,
Y2, ... Ym, where Y1, Y2... Ym are itemsets, if and only if there
exists integers 1 &lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â
Yi1, X2 â Yi2, ... Xk â Yik.</p>
    <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
    <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
    <p>For example, if we run SPADE with <em>minsup</em>= 50 % and
with a maximum pattern length of 100 items, 53 sequential patterns are
found. The list is too long to be presented here. An example of pattern
found is "(1,2),(6)" which appears in the first and the third sequences
(it has therefore a support of 50%). This pattern has a length of 3
because it contains three items. Another pattern is "(4), (3), (2)". It
appears in the second and third sequence (it has thus a support of 50
%). It also has a length of 3 because it contains 3 items.</p>
  </blockquote>
  <p>Optional parameter(s)</p>
  <blockquote>
    <p>The <strong>SPADE</strong> implementation allows to specify
additional optional parameter(s) :</p>
    <ul>
      <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
    </ul>
    <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestSPADE ... .java"</span></strong>
provided in the source code of SPMF.</p>
    <p>The parameter(s) can be also used in the command line with the
Jar file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SPADE</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
    </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
  </blockquote>
  <p>Output file format</p>
  <blockquote>
    <p>The <strong>output file format</strong> is defined as follows.
It is a text file. Each line is a frequent sequential pattern. Each
item from a sequential pattern is a positive integer and items from the
same itemset within a sequence are separated by single spaces. The
value "-1" indicates the end of an itemset. On each line, the
sequential pattern is first indicated. Then, the keyword " #SUP: "
appears followed by an integer indicating the support of the pattern as
a number of sequences. For example, a few lines from the output file
from the previous example are shown below:</p>
    <blockquote>
      <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
    </blockquote>
    <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>See the "<a href="http://www.philippe-fournier-viger.com/spmf/performance.php">performance</a>" section of
the website for a performance comparison with other sequential pattern
mining algorithm.</p>
  </blockquote>
  <p>Implementation details</p>
  <blockquote>
    <p>In the source code, we also provide examples of how to keep the
result into memory instead of saving it to a file. This can be useful
if the algorithms are integrated into another Java software. Examples
of how to save result into memory are named according to the following
naming convention: "<span class="Style2">MainTest..._saveToMemory</span>".</p>
    <p>Also note that in the source code, there are three variations
the SPADE implementations that tries different ways to perform the join
of IdLists. The fastest implementation is the one named "Fat_Bitmap".
It is the one offered in the graphical user interface.</p>
    <p class="Style2">"MainTestSPADE_AGP_BitMap_saveToFile.java"
"MainTestSPADE_AGP_BitMap_saveToMemory.java"
"MainTestSPADE_AGP_EntryList_saveToFile.java"
"MainTestSPADE_AGP_EntryList_saveToMemory.java"
"MainTestSPADE_AGP_FatBitMap_saveToFile.java"
"MainTestSPADE_AGP_FatBitMap_saveToMemory.java" </p>
    <p>Lastly, in the source code, a parallelized version of SPADE is
also offered:</p>
    <p class="Style2">"MainTestSPADE_AGP_Parallelized_BitMap_saveToFile.java"
"MainTestSPADE_AGP_Parallelized_BitMap_saveToMemory.java"
"MainTestSPADE_AGP_Parallelized_EntryList_saveToFile.java"
"MainTestSPADE_AGP_Parallelized_EntryList_saveToMemory.java"
"MainTestSPADE_AGP_Parallelized_FatBitMap_saveToFile.java"
"MainTestSPADE_AGP_Parallelized_FatBitMap_saveToMemory.java" </p>
    <p>Besides, note that an alternative input file <span class="Style2">contextSPADE.txt</span> is provided. It contains the
example used in the article proposing <strong>SPADE</strong>.</p>
  </blockquote>
  <p>Where can I get more information about SPADE?</p>
  <blockquote>
    <p>The SPADE algorithm is described in this article:</p>
    <p><em> Mohammed J. Zaki. 2001. <a href="http://www.philippe-fournier-viger.com/spmf/SPADE.pdf" rel="nofollow">SPADE:
An Efficient Algorithm for Mining Frequent Sequences</a>. Mach. Learn.
42, 1-2 (January 2001), 31-60. DOI=10.1023/A:1007652502315
http://dx.doi.org/10.1023/A:1007652502315 </em> </p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="cmspade" id="examplePrefixSpan5"> </a></span></strong> Example 76: Mining Frequent Sequential Patterns Using the CM-SPADE Algorithm</h3>

<blockquote>
  <p>How to run this example?</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">CM-SPADE</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CM-SPADE</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCMSPADE_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
  </ul>
  <p>What is CM-SPADE?</p>
  <blockquote>
    <p><strong>CM-SPADE</strong> is a sequential pattern mining
algorithm based on the SPADE algorithm. </p>
    <p>The main difference is that CM-SPADE utilizes a new technique
named co-occurrence pruning to prune the search space, which makes
faster.</p>
  </blockquote>
  <p>What is the input of CM-SPADE?</p>
  <blockquote>
    <p>The input of <strong>CM-SPADE</strong> is a sequence database
and a user-specified threshold named <em>minsup </em>(a value in
[0,1] representing a percentage). Moreover, the implementation in SPMF
adds another parameter, which is the maximum sequential pattern length
in terms of items.</p>
    <p>A <strong>sequence database</strong> is a set of sequences
where each sequence is a list of itemsets. An itemset is an unordered
set of distinct items. For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 itemsets. It means
that item 1 was followed by items 1 2 and 3 at the same time, which
were followed by 1 and 3, followed by 4, and followed by 3 and 6. It is
assumed that items in an itemset are sorted in lexicographical order.
This database is provided in the file "<strong>contextPrefixSpan.txt</strong>"
of the SPMF distribution<strong>.</strong> Note that it is assumed that
no items appear twice in the same itemset and that items in an itemset
are lexically ordered.</p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5 6), (1 2), (4 6), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1 6), (3), (2), (3)</td>
      </tr>
    </tbody>
  </table>
  <p>What is the output of CM-SPADE?</p>
  <blockquote>
    <p><strong>CM-SPADE </strong>discovers all <strong>frequent
sequential patterns</strong> occurring in a sequence database
(subsequences that occurs in more than <em>minsup</em> sequences of
the database.</p>
    <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
    <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
    <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are
itemsets<strong> is said to occur in another sequence SB</strong> = Y1,
Y2, ... Ym, where Y1, Y2... Ym are itemsets, if and only if there
exists integers 1 &lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â
Yi1, X2 â Yi2, ... Xk â Yik.</p>
    <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
    <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
    <p>For example, if we run CM-SPADE with <em>minsup</em>= 50 % and
with a maximum pattern length of 100 items, 53 sequential patterns are
found. The list is too long to be presented here. An example of pattern
found is "(1,2),(6)" which appears in the first and the third sequences
(it has therefore a support of 50%). This pattern has a length of 3
because it contains three items. Another pattern is "(4), (3), (2)". It
appears in the second and third sequence (it has thus a support of 50
%). It also has a length of 3 because it contains 3 items.</p>
  </blockquote>
  <p>Optional parameter(s)</p>
  <blockquote>
    <p>The <strong>CM-SPADE</strong> implementation allows to specify
additional optional parameter(s) :</p>
    <ul>
      <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
    </ul>
    <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestCMSpade ... .java"</span></strong>
provided in the source code of SPMF.</p>
    <p>The parameter(s) can be also used in the command line with the
Jar file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CM-SPADE</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
    </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
  </blockquote>
  <p>Output file format</p>
  <blockquote>
    <p>The <strong>output file format</strong> is defined as follows.
It is a text file. Each line is a frequent sequential pattern. Each
item from a sequential pattern is a positive integer and items from the
same itemset within a sequence are separated by single spaces. The
value "-1" indicates the end of an itemset. On each line, the
sequential pattern is first indicated. Then, the keyword " #SUP: "
appears followed by an integer indicating the support of the pattern as
a number of sequences. For example, a few lines from the output file
from the previous example are shown below:</p>
    <blockquote>
      <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
    </blockquote>
    <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>CM-SPADE is faster than SPADE and also the best sequential
pattern mining algorithm in SPMF according to our experiment in the
CM-SPADE paper.</p>
  </blockquote>
  <p>Implementation details</p>
  <blockquote>
    <p>In the source code, we also provide examples of how to keep the
result into memory instead of saving it to a file. This can be useful
if the algorithms are integrated into another Java software. Examples
of how to save result into memory is found in the following file: "<span class="Style2">MainTestCMSPADE_saveToMemory</span>". </p>
  </blockquote>
  <p>Where can I get more information about CM-SPADE?</p>
  <blockquote>
    <p>The CM-SPADE algorithm is described in this article:</p>
    <p><em> Fournier-Viger, P., Gomariz, A., Campos, M., Thomas, R.
(2014). <a href="http://www.philippe-fournier-viger.com/spmf/PAKDD2014_sequential_pattern_mining_CM-SPADE_CM-SPAM.pdf">Fast
Vertical Mining of Sequential Patterns Using Co-occurrence Information</a>.
Proc. 18th Pacific-Asia Conference on Knowledge Discovery and Data
Mining (PAKDD 2014), Part 1, Springer, LNAI, 8443. pp. 40-52.</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="spam" id="examplePrefixSpan2"> </a></span></strong> Example 77: Mining Frequent Sequential Patterns Using the SPAM Algorithm</h3>

<p>How to run this example?</p>

<blockquote>
  <p>To run the implementation of <strong>SPAM</strong> by P.
Fournier-Viger (PFV):</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">SPAM</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%</em> and <em>maximum pattern length = 100, </em>(5)
click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SPAM</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% 100</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestSPAM.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
  </ul>
  <p>To run the version the implementation of <strong>SPAM</strong> by
A. Gomariz PeÃ±alver (AGP):</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">SPAM_AGP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%</em> and<em> </em>(5) click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SPAM</span></strong><strong>_AGP contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestSPAM_AGP_FatBitMap_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
(other variations are also available in the source code)</li>
  </ul>
</blockquote>

<p>What is SPAM?</p>

<blockquote>
  <p><strong>SPAM</strong> is an algorithm for discovering frequent
sequential patterns in a sequence database. It was proposed by Ayres
(2002).</p>
</blockquote>

<p>What is the input of SPAM?</p>

<blockquote>
  <p>The input of <strong>SPAM</strong> is a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). Moreover, the implementation in SPMF adds
another parameter, which is the maximum sequential pattern length in
terms of items.</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of SPAM?</p>

<blockquote>
  <p><strong>SPAM </strong>discovers all <strong>frequent sequential
patterns</strong> occurring in a sequence database (subsequences that
occurs in more than <em>minsup</em> sequences of the database.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>For example, if we run SPAM with <em>minsup</em>= 50 %, 53
sequential patterns will be found. The list is too long to be presented
here. An example of pattern found is "(1,2),(6)" which appears in the
first and the third sequences (it has therefore a support of 50%). This
pattern has a length of 3 because it contains three items. Another
pattern is "(4), (3), (2)". It appears in the second and third sequence
(it has thus a support of 50 %). It also has a length of 3 because it
contains 3 items.</p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The SPAM implementation allows to specify four optional parameters
:</p>
  <ul>
    <li>"minimum pattern length" allows to specify the minimum number
of items that patterns found should contain.</li>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"max gap" allows to specify if gaps are allowed in sequential
patterns. For example, if "max gap" is set to 1, no gap is allowed
(i.e. each consecutive itemset of a pattern must appear consecutively
in a sequence). If "max gap" is set to N, a gap of N-1 itemsets is
allowed between two consecutive itemsets of a pattern. If the parameter
is not used, by default "max gap" is set to +â.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database (sequences with ids 0 and 2). </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestSPAM.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameters in the command line,
it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SPAM</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 0.5 2 6 1 true<br>
  </span>This command means to apply SPAM on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 0.5, that patterns must have a minimum length of 2 items, a maximum
length of 6 items, and have no gap between itemsets, and that ids of
sequence where the patterns is found must be shown in the output.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>SPAM </strong>is one of the fastest sequential pattern
mining algorithm. The SPAM implementation in SPMF is reported to be
faster than <strong>PrefixSpan</strong> (see the "performance" section
of the website for a performance comparison). However, <strong>CM-SPAM
  </strong>is faster than SPAM.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In the source code, we also provide examples of how to keep the
result into memory instead of saving it to a file. This can be useful
if the algorithms are integrated into another Java software. Examples
of how to save result into memory are named according to the following
naming convention: "<span class="Style2">MainTest..._saveToMemory</span>".</p>
  <p>For the AGP implementation of SPAM, several version are provided
in the source code that shows different way to perform the join of
IdLists. The fastest implementation is the one named "Fat_Bitmap". It
is the one offered in the graphical user interface.</p>
  <ul>
    <li>MainTestSPAM_AGP_BitMap_saveToFile.java"</li>
    <li>"MainTestSPAM_AGP_BitMap_saveToMemory.java"</li>
    <li>"MainTestSPAM_AGP_EntryList_saveToFile.java"</li>
    <li>"MainTestSPAM_AGP_EntryList_saveToMemory.java"</li>
    <li>"MainTestSPAM_AGP_FatBitMap_saveToFile.java"</li>
    <li>"MainTestSPAM_AGP_FatBitMap_saveToMemory.java"</li>
  </ul>
  <p>The AGP and PFV implementations of SPAM shares some source code
but also have some significant differences. See the <a href="http://www.philippe-fournier-viger.com/spmf/performance.php">performance</a> section of the website for a
performance comparison (will be added at the end of August 2013).</p>
</blockquote>

<p>Where can I get more information about SPAM?</p>

<blockquote>
  <p>The SPAM algorithm was proposed in this paper:</p>
  <p><em> J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/SPAM.pdf">Sequential
Pattern Mining Using Bitmaps</a>. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
Edmonton, Alberta, Canada, July 2002.</em></p>
  <p>The implementation of the optional "maxgap" constraint is based on
this paper:</p>
  <p><em>Ho, J., Lukov, L., &amp; Chawla, S. (2005). Sequential pattern
mining with constraints on large protein databases. In Proceedings
of the 12th International Conference on Management of Data
(COMAD) (pp. 89-100).</em> </p>
</blockquote>

<h3><strong><span class="centered"><a name="cmspam" id="examplePrefixSpan6"> </a></span></strong> Example 78: Mining Frequent Sequential Patterns Using the CM-SPAM Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">CM-SPAM</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%</em> and<em> </em>(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CMSPAM</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCMSPAM_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is CMSPAM?</p>

<blockquote>
  <p><strong>CM-SPAM</strong> (2013) is a sequential pattern mining
algorithm based on the SPAM algorithm. </p>
  <p>The main difference is that CM-SPAM utilizes a new technique named
co-occurrence pruning to prune the search space, which makes it faster
than the original SPAM algorithm.</p>
</blockquote>

<p>What is the input of CM-SPAM?</p>

<blockquote>
  <p>The input of <strong>CM-SPAM</strong> is a sequence database and
a user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). Moreover, the implementation in SPMF adds
another parameter, which is the maximum sequential pattern length in
terms of items.</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of CM-SPAM?</p>

<blockquote>
  <p><strong>CM-SPAM </strong>discovers all <strong>frequent
sequential patterns</strong> occurring in a sequence database
(subsequences that occurs in more than <em>minsup</em> sequences of
the database.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>For example, if we run CM-SPAM with <em>minsup</em>= 50 %, 53
sequential patterns will be found. The list is too long to be presented
here. An example of pattern found is "(1,2),(6)" which appears in the
first and the third sequences (it has therefore a support of 50%). This
pattern has a length of 3 because it contains three items. Another
pattern is "(4), (3), (2)". It appears in the second and third sequence
(it has thus a support of 50 %). It also has a length of 3 because it
contains 3 items.</p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The CM-SPAM implementation allows to specify additional optional
parameter(s) :</p>
  <ul>
    <li>"minimum pattern length" allows to specify the minimum number
of items that patterns found should contain.</li>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"required items" allow to specify a set of items that must
appears in every patterns found.</li>
    <li>"max gap" allows to specify if gaps are allowed in sequential
patterns. For example, if "max gap" is set to 1, no gap is allowed
(i.e. each consecutive itemset of a pattern must appear consecutively
in a sequence). If "max gap" is set to N, a gap of N-1 itemsets is
allowed between two consecutive itemsets of a pattern. If the parameter
is not used, by default "max gap" is set to +â.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database (sequences with ids 0 and 2). </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestCMSPAM.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameters in the command line,
it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CM-SPAM</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 0.5 2 6 1,3 1 true<br>
  </span>This command means to apply CM-SPAM on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 0.5, and patterns must have a minimum length of 2 items, a maximum
length of 6 items, must contain items 2 and 3, and have no gap between
itemsets. Moreover, sequence ids should be output for each pattern
found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>CM-SPAM</strong> is faster than <strong>SPAM</strong> and
one of the best sequential pattern mining algorithm in SPMF according
to our experiment in the CM-SPAM paper (see Performance section of the
website for more details).</p>
</blockquote>

<p>Where can I get more information about CM-SPAM?</p>

<blockquote>
  <p>The CM-SPAM algorithm is described in this article:</p>
  <p><em> Fournier-Viger, P., Gomariz, A., Campos, M., Thomas, R.
(2014). <a href="http://www.philippe-fournier-viger.com/spmf/PAKDD2014_sequential_pattern_mining_CM-SPADE_CM-SPAM.pdf">Fast
Vertical Mining of Sequential Patterns Using Co-occurrence Information</a>.
Proc. 18th Pacific-Asia Conference on Knowledge Discovery and Data
Mining (PAKDD 2014), Part 1, Springer, LNAI, 8443. pp. 40-52.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="lapin" id="examplePrefixSpan7"> </a></span></strong> Example 79: Mining Frequent Sequential Patterns Using the LAPIN Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">LAPIN</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%</em> and<em> </em>(5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> LAPIN</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestLAPIN_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is LAPIN?</p>

<blockquote>
  <p><strong>LAPIN </strong>(2005) is a sequential pattern mining
algorithm based on the SPAM algorithm. It replaces join operations by
border calculations (which are similar to a projected database) and
uses a table called "Item-is-exist-table" to know if an item can appear
after a given position in a sequence. There are several variations of
LAPIN. In this implementation, we have followed the main one also known
as LAPIN, LAPIN-SPAM and LAPIN-LCI, depending on the paper where it is
described by the authors.</p>
</blockquote>

<p>What is the input of LAPIN?</p>

<blockquote>
  <p>The input of <strong>LAPIN</strong> is a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). Moreover, the implementation in SPMF adds
another parameter, which is the maximum sequential pattern length in
terms of items.</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of LAPIN?</p>

<blockquote>
  <p><strong>LAPIN </strong>discovers all <strong>frequent sequential
patterns</strong> occurring in a sequence database (subsequences that
occurs in more than <em>minsup</em> sequences of the database.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>For example, if we run LAPIN with <em>minsup</em>= 50 %, 53
sequential patterns will be found. The list is too long to be presented
here. An example of pattern found is "(1,2),(6)" which appears in the
first and the third sequences (it has therefore a support of 50%). This
pattern has a length of 3 because it contains three items. Another
pattern is "(4), (3), (2)". It appears in the second and third sequence
(it has thus a support of 50 %). It also has a length of 3 because it
contains 3 items.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 2 3 -1 1 -1 #SUP: 2<br>
6 -1 2 -1 #SUP: 2<br>
6 -1 2 -1 3 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {2, 3}, followed by the itemset {1} has a
support of 2 sequences. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>LAPIN </strong>is quite a fast algorithm. However, it is
most of the times slower than CM-SPADE and CM-SPAM on the datasets that
we have compared. The implementation is quite optimized. Perhaps that
additional optimizations could be found to improve the speed further.</p>
</blockquote>

<p>Where can I get more information about LAPIN?</p>

<blockquote>
  <p>The LAPIN algorithm is described in this article:</p>
  <p><em> Z. Yang, Y. Wang, and M. Kitsuregawa. <a href="http://www.philippe-fournier-viger.com/spmf/LAPIN.pdf" rel="nofollow">LAPIN: Effective Sequential Pattern Mining Algorithms
by Last Position Induction</a>. Technical Report, Info. and Comm. Eng.
Dept., Tokyo University, 2005.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="clasp" id="exampleBIDE2"> </a></span></strong>
Example 80 : Mining Frequent Closed Sequential Patterns
Using the ClaSP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">ClaSP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> ClaSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestClaSP_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is ClaSP?</p>

<blockquote>
  <p><strong>ClaSP</strong> is a very efficient algorithm for
discovering closed sequential patterns in sequence databases, proposed
by Antonio Gomariz PeÃ±alver et al. (2013). This implementation is the
original implementation.</p>
</blockquote>

<p>What is the input of ClaSP?</p>

<blockquote>
  <p>The input of <strong> ClaSP</strong> is a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of ClaSP?</p>

<blockquote>
  <p><strong> ClaSP</strong> discovers all frequent <strong>closed
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a closed sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>Why using <strong>ClaSP</strong>? It can be shown that the set of
closed sequential patterns is generally much smaller than the set of
sequential patterns and that no information small. Moreover, finding
closed sequential patterns is often much more efficient than
discovering all patterns.</p>
  <p>For example, if we run <strong>ClaSP</strong><strong> </strong>with
  <em>minsup</em>= 50 % on the sequence database, the following
patterns are found. </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(6)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td>(5)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(2), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(4), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (3), (2)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S15</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S16</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S17</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). Another
pattern is "(4), (3)". It appears in the second and third sequence (it
has thus a support of 75 %).</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The <strong>ClaSP</strong> implementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestClaSP ... .java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> ClaSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 1 2 -1 4 -1 3 -1 #SUP: 2<br>
1 2 -1 6 -1 #SUP: 2<br>
1 -1 2 -1 3 -1 #SUP: 2</p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1, 2}, followed by the itemset {4}, followed
by the itemset {3} has a support of 2 sequences. The next lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong>ClaSP </strong>is a very efficient algorithm for closed
sequential pattern mining. See the article proposing <strong>ClaSP</strong>
for a performance comparison with CloSpan and SPADE. Note that <strong>CM-ClaSP
  </strong>is generally faster than ClaSP.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">In the source code
version of SPMF, there is also an example of how to use ClaSP and keep
the result in memory instead of saving it to a file </span></span>( <span class="Style2">MainTestClaSP_saveToMemory.java</span> ).</p>
  <p>An alternative input file <span class="Style2">contextClaSP.txt</span>
is also provided. It contains the example sequence database used in the
article proposing <strong>ClaSP</strong>.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>ClaSP</strong> algorithm was proposed in this paper: </p>
  <p>A. Gomariz, M. Campos, R. MarÃ­n and B. Goethals (2013), ClaSP: An
Efficient Algorithm for Mining Frequent Closed Sequences. Proc. PAKDD
2013, pp. 50-61.</p>
</blockquote>

<h3><strong><span class="centered"><a name="cmclasp" id="exampleBIDE5">
</a></span></strong> Example 81 : Mining Frequent Closed
Sequential Patterns Using the CM-ClaSP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">CM-ClaSP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CM-ClaSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCMClaSP_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is CM-ClaSP?</p>

<blockquote>
  <p><strong>ClaSP</strong> is a very efficient algorithm for
discovering closed sequential patterns in sequence databases, proposed
(Gomariz et al, 2013). </p>
  <p><strong>CM-ClaSP </strong>is a modification of the original ClaSP
algorithm using a technique co-occurrence pruning to prune the search
space (Fournier-Viger, Gomariz et al., 2014). It is generally faster
than the original ClaSP.</p>
</blockquote>

<p>What is the input of CM-ClaSP?</p>

<blockquote>
  <p>The input of <strong> CM-ClaSP</strong> is a sequence database
and a user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of CM-ClaSP?</p>

<blockquote>
  <p><strong> CM-ClaSP</strong> discovers all frequent <strong>closed
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a closed sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>Why using <strong>CM-ClaSP</strong>? It can be shown that the set
of closed sequential patterns is generally much smaller than the set of
sequential patterns and that no information small. Moreover, finding
closed sequential patterns is often much more efficient than
discovering all patterns.</p>
  <p>For example, if we run <strong>CM-ClaSP</strong><strong> </strong>with
  <em>minsup</em>= 50 % on the sequence database, the following
patterns are found. </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(6)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td>(5)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(2), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(4), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (3), (2)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S15</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S16</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S17</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). Another
pattern is "(4), (3)". It appears in the second and third sequence (it
has thus a support of 75 %).</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The <strong>CM-ClaSP</strong> implementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestCMClaSP ... .java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CM-ClaSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 1 2 -1 4 -1 3 -1 #SUP: 2<br>
1 2 -1 6 -1 #SUP: 2<br>
1 -1 2 -1 3 -1 #SUP: 2</p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1, 2}, followed by the itemset {4}, followed
by the itemset {3} has a support of 2 sequences. The next lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong>ClaSP </strong>is a very efficient algorithm for closed
sequential pattern mining. CM-ClaSP is generally a few times faster
than <strong>ClaSP</strong> on most dataset (see the CM-ClaSP paper
for details).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">In the source code
version of SPMF, there is also an example of how to use CM-ClaSP and
keep the result in memory instead of saving it to a file </span></span>(
  <span class="Style2">MainTestCMClaSP_saveToMemory.java</span> ).</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>CM-ClaSP</strong> algorithm was proposed in this
paper: </p>
  <p>Fournier-Viger, P., Gomariz, A., Campos, M., Thomas, R. (2014). <em><a href="http://www.philippe-fournier-viger.com/spmf/PAKDD2014_sequential_pattern_mining_CM-SPADE_CM-SPAM.pdf">Fast
Vertical Mining of Sequential Patterns Using Co-occurrence Information</a></em>.
Proc. 18th Pacific-Asia Conference on Knowledge Discovery and Data
Mining (PAKDD 2014), <em> Part 1, Springer, LNAI, 8443. pp. 40-52.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="clospan" id="exampleBIDE3">
</a></span></strong> Example 82 : Mining Frequent Closed
Sequential Patterns Using the CloSpan Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">CloSpan</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CloSpan</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCloSpan_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is CloSpan?</p>

<blockquote>
  <p><strong>CloSpan</strong> is a pattern-growth algorithm for
discovering closed sequential patterns in sequence databases, proposed
by Yan et al. (2003),</p>
</blockquote>

<p>What is the input of CloSpan?</p>

<blockquote>
  <p>The input of <strong> CloSpan</strong> is a sequence database and
a user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of CloSpan?</p>

<blockquote>
  <p><strong>CloSpan </strong>discovers all frequent <strong>closed
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a closed sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>Why using <strong>CloSpan</strong>? It can be shown that the set
of closed sequential patterns is generally much smaller than the set of
sequential patterns and that no information small. Moreover, finding
closed sequential patterns is often much more efficient than
discovering all patterns.</p>
  <p>For example, if we run <strong>CloSpan</strong><strong> </strong>with
  <em>minsup</em>= 50 % on the sequence database, the following
patterns are found. </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(6)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td>(5)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(2), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(4), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (3), (2)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S15</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S16</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S17</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). Another
pattern is "(4), (3)". It appears in the second and third sequence (it
has thus a support of 75 %).</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The <strong>CloSpan</strong> implementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestCloSpan ... .java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CloSpan</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 1 2 -1 4 -1 3 -1 #SUP: 2<br>
1 2 -1 6 -1 #SUP: 2<br>
1 -1 2 -1 3 -1 #SUP: 2</p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1, 2}, followed by the itemset {4}, followed
by the itemset {3} has a support of 2 sequences. The next lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong>CloSpan </strong>is an efficient algorithm for closed
sequential pattern mining. However, it should be noted that some newer
algorithm like <strong>ClaSP</strong> have shown better performance on
many datasets (see the ClaSP paper for a performance comparison).</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">In the source code
version of SPMF, there is also an example of how to use ClaSP and keep
the result in memory instead of saving it to a file </span></span>( <span class="Style2">MainTestCloSpan_saveToMemory.java</span> ).</p>
  <p>An alternative input file <span class="Style2">contextCloSpan.txt</span>
is also provided. It contains the example sequence database used in the
article proposing<strong> CloSpan</strong>.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>CloSpan</strong> algorithm was proposed in this
paper: </p>
  <p>Yan, X., Han, J., &amp; Afshar, R. (2003, May).<a href="http://www.philippe-fournier-viger.com/spmf/clospan.pdf" rel="nofollow"> CloSpan: Mining closed sequential
patterns in large datasets</a>. In Proc. 2003 SIAM Intâl Conf. Data
Mining (SDMâ03) (pp. 166-177).</p>
</blockquote>

<h3><strong><span class="centered"><a name="exampleBIDE" id="exampleBIDE"> </a></span></strong> Example 83 :
Mining Frequent Closed Sequential Patterns Using the BIDE+ Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">BIDE</span>+"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> BIDE</span>+</strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestBIDEPlus_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is BIDE+?</p>

<blockquote>
  <p><strong>BIDE+</strong> is an algorithm for discovering closed
sequential patterns in sequence databases, proposed by Wang et
al.(2007).</p>
</blockquote>

<p>What is the input of BIDE+?</p>

<blockquote>
  <p>The input of <strong> BIDE+</strong> is a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of BIDE+?</p>

<blockquote>
  <p><strong> BIDE+</strong> discovers all frequent <strong>closed
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a closed sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>Why using BIDE+? It can be shown that the set of closed sequential
patterns is generally much smaller than the set of sequential patterns
and that no information small. Moreover, finding closed sequential
patterns is often much more efficient than discovering all patterns.</p>
  <p>For example, if we run BIDE<strong>+</strong> with <em>minsup</em>=
50 % on the sequence database, the following patterns are found. </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(6)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td>(5)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(2), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(4), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (3), (2)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S15</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S16</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S17</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). Another
pattern is "(4), (3)". It appears in the second and third sequence (it
has thus a support of 75 %).</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The <strong>BIDE+</strong> implementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the second and the fourth sequences of the
sequence database. </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"MainTestBIDEPlus ...
.java"</span></strong> provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> BIDE+</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a positive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 1 2 -1 4 -1 3 -1 #SUP: 2<br>
1 2 -1 6 -1 #SUP: 2<br>
1 -1 2 -1 3 -1 #SUP: 2</p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1, 2}, followed by the itemset {4}, followed
by the itemset {3} has a support of 2 sequences. The next lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> BIDE+ </strong>is a very efficient algorithm for
closed sequential pattern mining. This implementations includes all <span class="Style5"><span class="Style7">the optimizations</span></span>
described in the paper. </p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">I have included three
versions of BIDE+ in the SPMF distribution.</span></span> The first one
keeps the frequent itemsets into memory and print the results to the
console (<strong><span class="Style2">MainTestBIDEPlus_saveToMemory.java).
  </span></strong><span class="Style7">The second one is a version</span>
that saves the result directly to a file (<strong><span class="Style2">MainTestBIDEPlus_saveToFile.java)</span></strong>.
The second version is faster. </p>
  <p>The third version of BIDE+ accepts strings instead of integers. It
is available under the name "<strong class="Style2">BIDE+ with strings</strong>"
in the GUI version of SPMF or in the package
ca.pfv.spmf.sequential_rules.bide_with_strings for the source code
version of SPMF. To run it, you should use the input file: <span class="Style2"><strong>contextPrefixSpanStrings.txt</strong>.</span></p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The BIDE algorithm is described in this paper: </p>
  <p><em>J. Wang, J. Han: <a href="http://www.philippe-fournier-viger.com/spmf/icde04_bide.pdf" rel="nofollow">BIDE: Efficient Mining of Frequent Closed Sequences</a>.
ICDE 2004: 79-90</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="exampleBIDE" id="exampleBIDE4"> </a></span></strong> Example 84 :
Mining Frequent Closed Sequential Patterns by Post-Processing Using the
PrefixSpan or SPAM Algorithm</h3>

<p>What is this?</p>

<blockquote>
  <p>This example shows how to use the PrefixSpan and SPAM algorithms
to discover all sequential patterns and keep only closed patterns by
post-processing. This should be less efficient than using a dedicated
algorithm for closed pattern mining like ClaSP, CloSpan and BIDE+.</p>
</blockquote>

<p>How to run this example?</p>

<blockquote>
  <p>If you want to use SPAM with post-processing: </p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">SPAM_PostProcessingClosed</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%</em> and <em>maximum pattern length = 100, </em>(5)
click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> SPAM_PostProcessingClosed</span></strong><strong>
contextPrefixSpan</strong>.txt output.txt 50% 100</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestSPAM_PostProcessingStepForClosedMining_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
(other variations are also available in the source code)</li>
  </ul>
  <p>If you want to use PrefixSpan with post-processing:</p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">PrefixSpan_PostProcessingClosed</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and maximum pattern length to 100, (5)
click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> PrefixSpan_PostProcessingClosed</span></strong><strong>
contextPrefixSpan</strong>.txt output.txt 50% 100</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextPrefixSpan.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestPrefixSpan_PostProcessingStepForClosedMining_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
  </ul>
</blockquote>

<p>What is the input ?</p>

<blockquote>
  <p>The input <strong> i</strong>s a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p><strong>The output is </strong>all frequent <strong>closed
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a closed sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>Why mining closed sequential patterns? It can be shown that the
set of closed sequential patterns is generally much smaller than the
set of sequential patterns and that no information small. Moreover,
finding closed sequential patterns is often much more efficient than
discovering all patterns.</p>
  <p>For example, for <em>minsup</em>= 50 %, the following patterns
are found in the previous sequence database . </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(6)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td>(5)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(2), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(4), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (3), (2)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S15</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S16</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S17</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). Another
pattern is "(4), (3)". It appears in the second and third sequence (it
has thus a support of 75 %).</p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 1 3" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"</span><span class="Style2">MainTestSPAM_PostProcessingStepForClosedMining</span><span class="Style2"> ... .java"</span></strong> provided in the source code
of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">SPAM_PostProcessingClosed</span></strong><strong>
contextPrefixSpan</strong>.txt output.txt 50% true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 1 2 -1 4 -1 3 -1 #SUP: 2<br>
1 2 -1 6 -1 #SUP: 2<br>
1 -1 2 -1 3 -1 #SUP: 2</p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1, 2}, followed by the itemset {4}, followed
by the itemset {3} has a support of 2 sequences. The next lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> Mining closed patterns by post-processing should be less
efficient than using a dedicated algorithms for closed sequential
pattern mining.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The implementations of SPAM and PrefixSpan used in this example
are made by Antonio Gomariz PeÃ±alver (AGP).</p>
  <p>In the source code, there is also two test files that shows how to
keep the result into memory instead of saving it to a file.</p>
  <ul>
    <li class="Style2">MainTestSPAM_PostProcessingStepForClosedMining_saveToMemory.java</li>
    <li class="Style2">MainTestPrefixSpan_PostProcessingStepForClosedMining_saveToMemory.java</li>
  </ul>
</blockquote>

<p>Where can I get more information?</p>

<blockquote>
  <p>For information about SPAM and PrefixSpan, see the examples about
PrefixSpan and SPAM.</p>
</blockquote>

<h3><strong><span class="centered"><a name="maxsp" id="exampleBIDE6"> </a></span></strong>
Example 85 : Mining Frequent Maximal Sequential Patterns
Using the MaxSP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">MaxSP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> MaxSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestMaxSP_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is MaxSP?</p>

<blockquote> <strong>MaxSP</strong> is an algorithm for discovering
maximal sequential patterns in sequence databases, proposed by
Fournier-Viger et al.(2013). </blockquote>

<p>What is the input of MaxSP?</p>

<blockquote>
  <p>The input of <strong> MaxSP</strong> is a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of MaxSP?</p>

<blockquote>
  <p><strong> MaxSP</strong> discovers all frequent <strong>maximal
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a maximal sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>A <strong>maximal sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another closed
sequential pattern. </p>
  <p>Why using <strong>MaxSP</strong>? It can be shown that the set of
maximal sequential patterns is generally much smaller than the set of
(closed sequential patterns and that all patterns could be recoved from
maximal patterns</p>
  <p>For example, if we run MaxSP<strong> </strong>with <em>minsup</em>=
50 % on the sequence database, the following patterns are found. </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). </p>
</blockquote>

<p>Optional parameter(s)</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameter(s) are available in the GUI of SPMF and also in
the example(s) <strong><span class="Style2">"</span><span class="Style2">MainTestMaxSP</span><span class="Style2"> ... .java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameter(s) in the command
line, it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">MaxSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
  </span>This command means to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 50%, and sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 4 -1 3 -1 2 -1 #SUP: 2<br>
5 -1 7 -1 3 -1 2 -1 #SUP: 2<br>
5 -1 1 -1 3 -1 2 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {4}, followed by the itemset {3}, followed by
the itemset {2} has a support of 2 sequences. The next lines follow the
same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> MaxSP </strong>is an efficient algorithm for maximal
sequential pattern mining. However, the <strong>VMSP</strong>
algorithm is faster.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">Note that there is a </span></span>test
file in the SPMF distribution to run the MaxSP algorithm and keep the
result into memory and print the results to the console, instead of
saving it to a file (<strong><span class="Style2">MainTestMaxSP_saveToMemory.java).
  </span></strong></p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>MaxSP</strong> algorithm is described in this paper: </p>
  <p><em>Fournier-Viger, P., Wu, C.-W., Tseng, V.-S. (2013). <a href="http://www.philippe-fournier-viger.com/ADMA2013_MaxSP_maximal_sequential_patterns.pdf">Mining
Maximal Sequential Patterns without Candidate Maintenance</a>. Proc.
9th International Conference on Advanced Data Mining and Applications
(ADMA 2013) Part I, Springer LNAI 8346, pp. 169-180. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="vmsp" id="exampleBIDE7"> </a></span></strong>
Example 86 : Mining Frequent Maximal Sequential Patterns
Using the VMSP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">VMSP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> VMSP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestVMSP_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is VMSP?</p>

<blockquote> <strong>VMSP</strong> is an algorithm for discovering
maximal sequential patterns in sequence databases, proposed by
Fournier-Viger et al.(2013). </blockquote>

<p>What is the input of VMSP?</p>

<blockquote>
  <p>The input of <strong> VMSP</strong> is a sequence database and a
user-specified threshold named <em>minsup</em> (a value in [0,1]
representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of VMSP?</p>

<blockquote>
  <p><strong> VMSP</strong> discovers all frequent <strong>maximal
sequential pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a maximal sequential pattern, it
is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong>closed sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another pattern having
the same support. </p>
  <p>A <strong>maximal sequential pattern </strong>is a sequential
pattern such that it is not strictly included in another closed
sequential pattern. </p>
  <p>Why using <strong>VMSP</strong>? It can be shown that the set of
maximal sequential patterns is generally much smaller than the set of
(closed sequential patterns and that all patterns could be recoved from
maximal patterns. </p>
  <p>For example, if we run VMSP<strong> </strong>with <em>minsup</em>=
50 % on the sequence database, the following patterns are found. </p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">(1 2), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(1) (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(1), (2 3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(1), (3), (3)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1 2), (4), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(6), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(5), (2), (3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(4), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(5), (6), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(5), (1), (3), (2)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1,2),(6)" appears in the
first and third sequence (it has therefore a support of 50%). </p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"max gap" allows to specify if gaps are allowed in sequential
patterns. For example, if "max gap" is set to 1, no gap is allowed
(i.e. each consecutive itemset of a pattern must appear consecutively
in a sequence). If "max gap" is set to N, a gap of N-1 itemsets is
allowed between two consecutive itemsets of a pattern. If the parameter
is not used, by default "max gap" is set to +â.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestVMSP.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>If you want to use these optional parameters in the command line,
it can be done as follows. Consider the command:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> VMSP</span> contextPrefixSpan</strong>.txt output.txt
50% 6 1 true<br>
  </span>It means that the user want to apply <strong>VMSP</strong> on
the file "contextPrefixSpan.txt" and output the results to
"output.txt". Moreover, the user wants to find patterns with a support
of at least 50 %, having a maximum length of 6 items, and having no gap
between itemsets. Moreover, sequence ids should be output for each
pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p> 4 -1 3 -1 2 -1 #SUP: 2<br>
5 -1 7 -1 3 -1 2 -1 #SUP: 2<br>
5 -1 1 -1 3 -1 2 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {4}, followed by the itemset {3}, followed by
the itemset {2} has a support of 2 sequences. The next lines follow the
same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> VMSP </strong>is the fastest maximal sequential
pattern mining algorithm offered in SPMF.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">Note that there is a </span></span>test
file in the SPMF distribution to run the VMSP algorithm and keep the
result into memory and print the results to the console, instead of
saving it to a file (<strong><span class="Style2">MainTestVMSP_saveToMemory.java).
  </span></strong></p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The VMSP algorithm is described in this paper: </p>
  <p><em>Fournier-Viger, P., Wu, C.-W., Gomariz, A. Tseng, V.-S.
(2014). <a href="http://www.philippe-fournier-viger.com/spmf/VMSP_maximal_sequential_patterns_2014.pdf">VMSP:
Efficient Vertical Mining of Maximal Sequential Patterns</a>, </em>Proc.
27th Canadian Conference on Artificial Intelligence (AI 2014),
Springer, LNAI, pp. 83-94<em>. </em></p>
</blockquote>

<h3><strong><span class="centered"><a name="feat" id="exampleBIDE11"> </a></span></strong>
Example 87 : Mining Frequent Sequential Generator
Patterns Using the FEAT Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">FEAT</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FEAT</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFEAT_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is FEAT?</p>

<blockquote> <strong>FEAT</strong> is an algorithm for discovering
sequential generator patterns in sequence databases, proposed by Gao et
al.(2008). </blockquote>

<p>What is the input of FEAT?</p>

<blockquote>
  <p>The input is a sequence database and a user-specified threshold
named <em>minsup</em> (a value in [0,1] representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of FEAT?</p>

<blockquote>
  <p> The algorithm discovers all frequent <strong> sequential
generator pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a sequential generator pattern,
it is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong> sequential generator pattern </strong>is a sequential
pattern SA such that there does not exists a smaller pattern SB having
the same support and such that SB occurs in SA. </p>
  <p>Why using <strong>FEAT</strong>? It can be shown that the set of
sequential generator patterns is generally much smaller than the set of
all sequential patterns. Moreover, sequential generator patterns are
minimal patterns. In some cases, it is interesting to discover minimal
patterns. For example, generator patterns could be used to generate
sequential rules with a minimal antecedent. </p>
  <p>For example, if we run <strong>FEAT</strong><strong> </strong>with
  <em>minsup</em>= 50 % on the sequence database, 24 patterns are
found. </p>
  <table align="center" border="1" width="440">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="305"><strong> Sequential Generator Pattern</strong></td>
        <td width="67"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">empty sequence</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(6)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(5)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(2), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(2, 3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1,4)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1,2)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(2), (4)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(2), (6)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(4), (2)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>...</td>
        <td bgcolor="#ffffff">...</td>
        <td bgcolor="#ffffff">...</td>
      </tr>
      <tr>
        <td>S24</td>
        <td bgcolor="#ffffff">(1), (2), (3)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1),(2),(3)" appears in the
first and fourth sequence (it has therefore a support of 50%). </p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestFEAT ....java"</span></strong>
provided in the source code of SPMF.</p>
  <p>If you want to use these optional parameters in the command line,
it can be done as follows. Consider the command:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FEAT</span> contextPrefixSpan</strong>.txt output.txt
50% 6 true<br>
  </span>It means that the user want to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, the user wants to find patterns with a support of at least 50
% having a maximum length of 6 items. Moreover, sequence ids should be
output for each pattern found.</p>
</blockquote>

<blockquote> </blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p>6 -1 3 -1 SUP: 2<br>
3 -1 2 -1 SUP: 3<br>
1 -1 2 -1 3 -1 SUP: 2</p>
  </blockquote>
  <p>The third line indicates that the frequent sequential pattern
consisting of the itemset {1}, followed by the itemset {2}, followed by
the itemset {3} has a support of 2 sequences. The other lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> FEAT </strong>is one of the first algorithms for
mining sequential generator patterns. The VGEN algorithm is usually
much faster.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">Note that there is a </span></span>test
file in the SPMF distribution to run the <strong>FEAT</strong>
algorithm and keep the result into memory and print the results to the
console, instead of saving it to a file (<strong><span class="Style2">MainTestFEAT_saveToMemory.java).
  </span></strong></p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>FEAT</strong> algorithm is described in this paper: </p>
  <p><em>Gao, C.,Wang, J., He, Y., Zhou, L.: Efficient mining of
frequent sequence generators.<br>
In: Proc. 17th Intern. Conf. World Wide Web:, pp. 1051-1052.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="fsgp" id="exampleBIDE12"> </a></span></strong>
Example 88 : Mining Frequent Sequential Generator
Patterns Using the FSGP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">FSGP</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FSGP</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFSGP_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is FSGP?</p>

<blockquote> <strong>FSGP</strong> is an algorithm for discovering
sequential generator patterns in sequence databases, proposed by Yi et
al. (2011). </blockquote>

<p>What is the input of FSGP?</p>

<blockquote>
  <p>The input is a sequence database and a user-specified threshold
named <em>minsup</em> (a value in [0,1] representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of FSGP?</p>

<blockquote>
  <p> The algorithm discovers all frequent <strong> sequential
generator pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a sequential generator pattern,
it is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong> sequential generator pattern </strong>is a sequential
pattern SA such that there does not exists a smaller pattern SB having
the same support and such that SB occurs in SA. </p>
  <p>Why using <strong>FSGP</strong>? It can be shown that the set of
sequential generator patterns is generally much smaller than the set of
all sequential patterns. Moreover, sequential generator patterns are
minimal patterns. In some cases, it is interesting to discover minimal
patterns. For example, generator patterns could be used to generate
sequential rules with a minimal antecedent. </p>
  <p>For example, if we run <strong>FSGP</strong><strong> </strong>with
  <em>minsup</em>= 50 % on the sequence database, 24 patterns are
found. </p>
  <table align="center" border="1" width="440">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="305"><strong> Sequential Generator Pattern</strong></td>
        <td width="67"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">empty sequence</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(6)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(5)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(2), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(2, 3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1,4)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1,2)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(2), (4)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(2), (6)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(4), (2)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>...</td>
        <td bgcolor="#ffffff">...</td>
        <td bgcolor="#ffffff">...</td>
      </tr>
      <tr>
        <td>S24</td>
        <td bgcolor="#ffffff">(1), (2), (3)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1),(2),(3)" appears in the
first and fourth sequence (it has therefore a support of 50%). </p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestFSGP ....java"</span></strong>
provided in the source code of SPMF.</p>
  <p>If you want to use these optional parameters in the command line,
it can be done as follows. Consider the command:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> FSGP</span> contextPrefixSpan</strong>.txt output.txt
50% 6 true<br>
  </span>It means that the user want to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, the user wants to find patterns with a support of at least 50
% having a maximum length of 6 items. Moreover, sequence ids should be
output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p>6 -1 3 -1 SUP: 2<br>
3 -1 2 -1 SUP: 3<br>
1 -1 2 -1 3 -1 SUP: 2</p>
  </blockquote>
  <p>The third line indicates that the frequent sequential pattern
consisting of the itemset {1}, followed by the itemset {2}, followed by
the itemset {3} has a support of 2 sequences. The other lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> FSGP </strong>is a recent algorithm for mining
sequential generator patterns (2011). However, the VGEN algorithm
(2014) is usually much faster.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p><span class="Style5"><span class="Style7">Note that there is a </span></span>test
file in the SPMF distribution to run the <strong>FSGP</strong>
algorithm and keep the result into memory and print the results to the
console, instead of saving it to a file (<strong><span class="Style2">MainTestFSGP_saveToMemory.java).
  </span></strong></p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The FSGP algorithm is described in this paper: </p>
  <p><em>Yi, S., Zhao, T., Zhang, Y., Ma, S., Che, Z.: An eective
algorithm for mining sequential generators. Procedia Engineering, 15,
3653-3657 (2011)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="vgen" id="exampleBIDE13"> </a></span></strong>
Example 89 : Mining Frequent Sequential Generator
Patterns Using the VGEN Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">VGEN</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> VGEN</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestVGEN_saveToFile.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is VGEN?</p>

<blockquote> <strong>VGEN</strong> is an algorithm for discovering
sequential generator patterns in sequence databases, proposed by
Fournier-Viger et al. (2014). </blockquote>

<p>What is the input of VGEN?</p>

<blockquote>
  <p>The input is a sequence database and a user-specified threshold
named <em>minsup</em> (a value in [0,1] representing a percentage). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of VGEN?</p>

<blockquote>
  <p> The algorithm discovers all frequent <strong> sequential
generator pattern</strong>s that occurs in a sequence database.</p>
  <p>To explain more formally what is a sequential generator pattern,
it is necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>A <strong>frequent sequential pattern </strong>is a sequential
pattern having a support no less than the <em>minsup</em> parameter
provided by the user.</p>
  <p>A <strong> sequential generator pattern </strong>is a sequential
pattern SA such that there does not exists a smaller pattern SB having
the same support and such that SB occurs in SA. </p>
  <p>Why using <strong>VGEN</strong>? It can be shown that the set of
sequential generator patterns is generally much smaller than the set of
all sequential patterns. Moreover, sequential generator patterns are
minimal patterns. In some cases, it is interesting to discover minimal
patterns. For example, generator patterns could be used to generate
sequential rules with a minimal antecedent. </p>
  <p>For example, if we run <strong>VGEN</strong><strong> </strong>with
  <em>minsup</em>= 50 % on the sequence database, 24 patterns are
found. </p>
  <table align="center" border="1" width="440">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="305"><strong> Sequential Generator Pattern</strong></td>
        <td width="67"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">empty sequence</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(6)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(5)</td>
        <td bgcolor="#ffffff">75 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td bgcolor="#ffffff">(2), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td bgcolor="#ffffff">(2, 3)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S8</td>
        <td bgcolor="#ffffff">(3), (1)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S9</td>
        <td bgcolor="#ffffff">(1,4)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S10</td>
        <td bgcolor="#ffffff">(1), (6)</td>
        <td bgcolor="#ffffff">50 %</td>
      </tr>
      <tr>
        <td>S11</td>
        <td bgcolor="#ffffff">(1,2)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S12</td>
        <td bgcolor="#ffffff">(2), (4)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S13</td>
        <td bgcolor="#ffffff">(2), (6)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>S14</td>
        <td bgcolor="#ffffff">(4), (2)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
      <tr>
        <td>...</td>
        <td bgcolor="#ffffff">...</td>
        <td bgcolor="#ffffff">...</td>
      </tr>
      <tr>
        <td>S24</td>
        <td bgcolor="#ffffff">(1), (2), (3)</td>
        <td bgcolor="#ffffff">50%</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1),(2),(3)" appears in the
first and fourth sequence (it has therefore a support of 50%). </p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"max gap" allows to specify if gaps are allowed in sequential
patterns. For example, if "max gap" is set to 1, no gap is allowed
(i.e. each consecutive itemset of a pattern must appear consecutively
in a sequence). If "max gap" is set to N, a gap of N-1 itemsets is
allowed between two consecutive itemsets of a pattern. If the parameter
is not used, by default "max gap" is set to +â.</li>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestVGEN.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>If you want to use these optional parameters in the command line,
it can be done as follows. Consider the command:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> VGEN</span> contextPrefixSpan</strong>.txt output.txt
50% 6 1 true<br>
  </span>It means that the user want to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, the user wants to find patterns with a support of at least 50
%, having a maximum length of 6 items, and having no gap between
itemsets. Moreover, sequence ids should be output for each pattern
found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines from the output file from
the previous example are shown below:</p>
  <blockquote>
    <p>6 -1 3 -1 SUP: 2<br>
3 -1 2 -1 SUP: 3<br>
1 -1 2 -1 3 -1 SUP: 2</p>
  </blockquote>
  <p>The third line indicates that the frequent sequential pattern
consisting of the itemset {1}, followed by the itemset {2}, followed by
the itemset {3} has a support of 2 sequences. The other lines follow
the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> VGEN </strong>is the fastest algorithm offered in SPMF
for mining sequential generator patterns (2014). It was shown to
greatly outperform FEAT and FSGP.</p>
</blockquote>

<p>Implementation details</p>

<blockquote> This is the original implementation of the algorithm.
  <p><span class="Style5"><span class="Style7">Note that there is a </span></span>test
file in the SPMF distribution to run the <strong>VGEN</strong>
algorithm and keep the result into memory and print the results to the
console, instead of saving it to a file (<strong><span class="Style2">MainTestVGEN_saveToMemory.java).
  </span></strong></p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The <strong>VGEN</strong> algorithm is described in this paper: </p>
  <p><em>Fournier-Viger, P., Gomariz, A., Sebek, M., Hlosta, M.
(2014). VGEN: Fast Vertical Mining of Sequential Generator
Patterns. Proc. 16th Intern. Conf. on Data Warehousing and
Knowledge Discovery (DAWAK 2014), Springer LNAI, Part 1, Springer,
LNAI, 8443. pp. 40-52.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="gokrimp" id="exampleBIDE10">
</a></span></strong> Example 90 : Mining Compressing
Sequential Patterns Using the GoKrimp Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">GoKrimp</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">test_goKrimp</span><span class="Style2">.dat"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set the label file as <strong>"<span class="Style2">test_goKrimp</span><span class="Style2">.lab"</span></strong><em> </em>and (5) click "<span class="Style2">Run algorithm</span>".<br>
Note that the algorithm can run without the label file if none is
available. </li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run GoKrimp<strong>
test_goKrimp</strong>.dat output.txt <strong>test_goKrimp</strong>.lab
    </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file<span class="Style2"><strong> test_goKrimp</strong>.dat</span>
and <span class="Style2"><strong>test_goKrimp</strong>.lab</span>.<br>
Note that the algorithm can run without the label file if none is
available. </li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGoKrimp_printResultToConsole.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>,
or <strong><span class="Style2">"MainTestGoKrimp_saveToFile.java"</span></strong></li>
</ul>

<p>What is GoKrimp?</p>

<blockquote>
  <p>The <strong>GoKrimp</strong> algorithm finds a non-redundant set
of compressing sequential patterns based on the Minimum Description
Length principle. It searches for a set of sequential pattern that
compresses the data most. In this way, the set of patterns found
by <strong>GoKrimp</strong> is usually non-redundant, and they are
much more meaningful compared the the set of frequent patterns.</p>
  <p> Another important property of <strong>GoKrimp</strong> is that
it is parameter-free, thus users are not required to fine tune the
parameters such as minimum support which is a difficult task in many
applications. </p>
  <p> The <strong>Gokrimp</strong> algorithm was first published in
the SIAM Data Mining conference in 2012 and was chosen to include in
the special issue of the best papers of SDM 2012 in the journal of
Statistical Analysis and Data Mining (SADM Wiley 2014). </p>
  <p>This implementation is the original implementation of GoKrimp.</p>
</blockquote>

<p>What is the input of GoKrimp?</p>

<blockquote>
  <p> The input is a sequence database and a label file (<em>optional</em>).</p>
  <p> A sequence database is a set of sequences where each sequence is
an ordered list of items. Current version of GoKrimp does not
work for a sequence of itemsets but the file input format is consistent
with the standard file input format of the SPMF package. </p>
  <p> The <strong>input file format </strong>is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and it is
separated by the value "-1". The value "-2" indicates the end of a
sequence (it appears at the end of each line). For example, the input
file may contain the following two lines (two sequences).</p>
  <p> 1 -1 2 -1 3 -1 4 -1 3-1 -2<br>
4 -1 3 -1 2 -1 1 -1 -2</p>
  <p> The first line represents a sequence where the item {1} is
followed by the item { 2}, followed by the item {3}, followed by
the item {4} and followed by the item {3}. The next lines follow the
same format.</p>
  <p>The <strong>label file format</strong><strong> </strong>is
defined as follows. It is a text file where each line contains a string
indicating the label of the item with identifier equal to the line
number. For example the label file may contain the following 4 lines:</p>
  <p>Support<br>
Vector<br>
Machine<br>
Algorithm</p>
  <p>which indicates that the label of the item 1 is âSupportâ, of item
2 is âVectorâ, of item 3 is âMachineâ, and of item
4 is âAlgorithmâ. Label file is optional.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p> The <strong>output file format</strong> is defined as follows.
It is a text file. Each line corresponds to a compressing sequential
pattern. Each item separated by a single space from a compressing
sequential pattern is either represented by a positive integer or by
its label if the label file is provided. On each line, the compressing
sequential pattern is first indicated. Then, the keyword "#SUP" appears
followed by a real number indicating the contribution of the pattern in
the compression. For example, here a few lines from the
output file from the journal of machine learning dataset (<strong>jmlr_goKrimp.dat</strong>):<br>
  <br>
support vector machin #SUP: 1922.0710148279322<br>
real world #SUP: 598.4753133154009<br>
machin learn #SUP: 514.3586664227769<br>
state art #SUP: 412.9730013575172<br>
high dimension #SUP: 362.7776787300827<br>
reproduc hilbert space #SUP: 359.42939766764175<br>
neural network #SUP: 210.35608129308093<br>
experiment result #SUP: 187.4169747827109<br>
compon analysi #SUP: 176.54417917714454<br>
supervis learn #SUP: 160.87427082075737<br>
support vector #SUP: 148.74911007808987<br>
well known #SUP: 138.22464635269716<br>
hilbert space #SUP: 21.132125171017833<br>
Compressed size: 839792.3563524645, uncompressed size:
845005.7388124614, compression ratio: 1.006207942261633<br>
Running time: 2 seconds</p>
  <p>The first line indicates that the compressing sequential pattern
is <em>support vector machin</em> followed by the #SUP tag indicating
the compression contribution of this pattern. The last two lines shows
the compressed size and uncompressed size of the data in the number of
bits, the compression ratio and the running times in seconds.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>GoKrimp</strong> is very efficient. It output one pattern
in each step so you can terminate the algorithm at anytime. In each
step, GoKrimp starts from a seed item (usually frequent
items) and tries to extend this item with related items chosen by the
Sign Test. When extension of a pattern does not give significant
compression benefit, GoKrimp outputs the pattern and starts looking for
the next pattern with the new seeds.</p>
</blockquote>

<p>Tips for using the source code: </p>

<blockquote>
  <p> <strong>GoKrimp</strong> algorithm uses the <strong>SignTest</strong>
to test for dependency between a pattern and events used to extend a
pattern. It requires that the event occurs at least in N=25
sequences to perform the test properly. If you have a very long
sequence instead of a database of many sequences, you should split the
long sequences into a set of short sequences.<br>
  <br>
The source code contains the <strong>SeqKrimp</strong> algorithm
implementation which gets the candidate pattern set and returns a
good subset of compressing patterns. You can feed <strong>SeqKrimp</strong>
with any frequent pattern mining algorithm, e.g. BIDE+ and PreFixSpan.</p>
  <p>The default encoding for an integer in our implementation is the
Elias Gamma code, you can try the Elias Delta code by uncomenting the
Elias Delta returning code in the function <em>bits</em> of GoKrimp
class. The result might be slightly different.</p>
  <p>The fields NSTART and NRELATED in the GoKrimp class are used to
control the number of initial most frequent events used as seeds to
extend to get the set of patterns and the maximum number of related
events used to extend a given candidate pattern. The default value for
NSTART and NRELATED is 1000. You can change this value to smaller
value if the source code runs too long yet the quality of the results
will be affected. </p>
</blockquote>

<p> The GoKrimp algorithm is described in this paper:</p>

<blockquote>
  <p> Hoang Thanh Lam, Fabian MÃ¶rchen, Dmitriy Fradkin, Toon Calders: <em><a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/gokrimp.pdf">Mining Compressing Sequential
Patterns</a></em> in Journal of Statistical Analysis and Data Mining
7(1): 34-52 (2014) by Wiley.</p>
  <p><strong>Acknowledgements</strong><br>
The work was done when the first author was doing his PhD at Eindhoven
University of Technology<strong>,</strong> the Netherlands under the
support by the Netherlands Organisation for Scientific Research (NWO)
in the project COMPASS. Part of the work has been done at Siemens
Corporate Research center in Princeton, NJ USA. </p>
</blockquote>

<h3><strong><span class="centered"><a name="tks" id="exampleBIDE8"> </a></span></strong>
Example 91 : Mining Top-K Sequential Patterns Using the
TKS Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">TKS</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>k = 5 </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TKS</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 5</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTKS.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TKS?</p>

<blockquote> <strong>TKS</strong> is an algorithm for discovering the
top-k most frequent sequential patterns in a sequence database. TKS was
proposed by Fournier-Viger et al.(2013). </blockquote>

<p>What is the input of TKS?</p>

<blockquote>
  <p>The input of <strong> TKS</strong> is a sequence database and a
user-specified parameter named <em>k</em> (a positive integer
representing the desired number of patterns to be found). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of TKS?</p>

<blockquote>
  <p><strong> TKS</strong> discovers the top-<em>k </em>most frequent <strong>sequential
pattern</strong>s that occurs in the input sequence database, where <em>k
  </em>is set by the user. Note that it is possible that TKS returns
more than <em>k </em>patterns if several patterns have exactly the
same support.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>Why using <strong>TKS</strong>? It is often hard to set the <em>minsup</em>
threshold of sequential pattern mining algorithm to get a fixed number
of patterns without running the algorithms several times and
fine-tuning the parameter. With a top-k sequential pattern mining
algorithm, the user can set <em>k </em>the number of patterns to be
output directly, which is more intuitive than using <em>minsup</em>.</p>
  <p>For example, if we run TKS<strong> </strong>with <em>k</em>=5 on
the sequence database, the top-5 most frequent patterns are:</p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">(2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(1) (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1),(2)" appears in the
first, second, third and fourth sequence (it has therefore a support of
100%). </p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The TKS implementation allows to specify four optional parameters :</p>
  <ul>
    <li>"minimum pattern length" allows to specify the minimum number
of items that patterns found should contain.</li>
    <li>"maximum pattern length" allows to specify the maximum number
of items that patterns found should contain.</li>
    <li>"required items" allow to specify a set of items that must
appears in every patterns found.</li>
    <li>"max gap" allows to specify if gaps are allowed in sequential
patterns. For example, if "max gap" is set to 1, no gap is allowed
(i.e. each consecutive itemset of a pattern must appear consecutively
in a sequence). If "max gap" is set to N, a gap of N-1 itemsets is
allowed between two consecutive itemsets of a pattern. If the parameter
is not used, by default "max gap" is set to +â.</li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestTKS.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>If you want to use these optional parameters in the command line,
it can be done as follows. Consider the command:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TKS</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 5 2 6 1,3 1<br>
  </span>It means that the user want to apply TKS on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, the user wants to find the top-5 patterns (k = 5) where
patterns must have a minimum length of 2 item, a maximum length of 6
items, must contain items 2 and 3, and have no gap between itemsets. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines of an output file
(different from the example above) could be:</p>
  <blockquote>
    <p> 1 -1 1 2 -1 #SUP: 2<br>
5 -1 7 -1 3 -1 2 -1 #SUP: 2<br>
5 -1 1 -1 3 -1 2 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1}, followed by the itemset {1, 2}, has a
support of 2 sequences. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> <strong> TKS </strong>is the fastest top-k sequential pattern
mining algorithm offered in SPMF.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TKS algorithm is described in this paper: </p>
  <p><em>Fournier-Viger, P., Gomariz, A., Gueniche, T., Mwamikazi, E.,
Thomas, R. (2013). <a href="http://www.philippe-fournier-viger.com/ADMA2013_TKS_top_k_sequential_patterns.pdf">Efficient
Mining of Top-K Sequential Patterns</a>. Proc. 9th International
Conference on Advanced Data Mining and Applications (ADMA 2013) Part I,
Springer LNAI 8346, pp. 109-120.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="tsp" id="exampleBIDE9"> </a></span></strong>
Example 92 : Mining Top-K Sequential Patterns Using the
TSP Algorithm</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">TSP_nonClosed</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>k = 5 </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TSP_nonClosed</span></strong><strong> contextPrefixSpan</strong>.txt
output.txt 5</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTSP_nonClosed.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TSP?</p>

<blockquote>
  <p><strong>TSP</strong> is the first algorithm for discovering the
top-k frequent sequential patterns in a sequence database. TSP was
proposed by Tzvetkov et al. (2003) and it is based on the PrefixSpan
algorithm. </p>
  <p>Note that this implementation is for discovering frequent
sequential patterns. In the paper proposing TSP, they also present a
version for mining closed patterns. This latter version is not
implemented in SPMF.</p>
</blockquote>

<p>What is the input of TSP?</p>

<blockquote>
  <p>The input of <strong> TSP</strong> is a sequence database and a
user-specified parameter named <em>k</em> (a positive integer
representing the desired number of patterns to be found). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong> Note that it is assumed that no
items appear twice in the same itemset and that items in an itemset are
lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of TSP?</p>

<blockquote>
  <p><strong> TSP</strong> discovers the top-<em>k </em>most frequent <strong>sequential
pattern</strong>s that occurs in the input sequence database, where <em>k
  </em>is set by the user. Note that it is possible that TSP returns
more than <em>k </em>patterns if several patterns have exactly the
same support.</p>
  <p>To explain more formally what is a sequential pattern, it is
necessary to review some definition.</p>
  <p>A <strong>sequential pattern</strong> is a sequence. A <strong>sequence</strong>
  <strong>SA</strong> = X1, X2, ... Xk, where X1, X2... Xk are itemsets<strong>
is said to occur in another sequence SB</strong> = Y1, Y2, ... Ym,
where Y1, Y2... Ym are itemsets, if and only if there exists integers 1
&lt;= i1 &lt; i2... &lt; ik &lt;= m such that X1 â Yi1, X2 â Yi2, ...
Xk â Yik.</p>
  <p>The <strong>support of a sequential pattern </strong>is the
number of sequences where the pattern occurs divided by the total
number of sequences in the database.</p>
  <p>Why using <strong>TSP</strong>? It is often hard to set the <em>minsup</em>
threshold of sequential pattern mining algorithm to get a fixed number
of patterns without running the algorithms several times and
fine-tuning the parameter. With a top-k sequential pattern mining
algorithm, the user can set <em>k </em>the number of patterns to be
output directly, which is more intuitive than using <em>minsup</em>.</p>
  <p>For example, if we run TSP<strong> </strong>with <em>k</em>=5 on
the sequence database, the top-5 most frequent patterns are:</p>
  <table align="center" border="1" width="299">
    <tbody>
      <tr>
        <td width="46"><strong>ID</strong></td>
        <td width="133"><strong>Closed Sequential Pattern</strong></td>
        <td width="98"><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td bgcolor="#ffffff">(2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(1) (2)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td bgcolor="#ffffff">(1), (3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td bgcolor="#ffffff">(3)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td bgcolor="#ffffff">(1)</td>
        <td bgcolor="#ffffff">100 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the sequential pattern "(1),(2)" appears in the
first, second, third and fourth sequence (it has therefore a support of
100%). <br>
  </p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The algorithm<strong> i</strong>mplementation allows to specify
additional optional parameter(s) :</p>
  <ul>
    <li>"show sequences ids?" (true/false) This parameter allows to
specify that sequence ids of sequences containing a pattern should be
output for each pattern found. For example, if the parameter is set to
true, each pattern in the output file will be followed by the keyword
#SID followed by a list of sequences ids (integers separated by space).
For example, a line terminated by "#SID: 0 2" means that the pattern on
this line appears in the first and the third sequences of the sequence
database. </li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestTSP ....java"</span></strong>
provided in the source code of SPMF.</p>
  <p>If you want to use these optional parameters in the command line,
it can be done as follows. Consider the command:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9">TSP_nonClosed</span> contextPrefixSpan</strong>.txt
output.txt 50% true<br>
  </span>It means that the user want to apply the algorithm on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, the user wants to find patterns with a support of at least 50
%. Moreover, sequence ids should be output for each pattern found.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single space. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent sequential pattern. Each item
from a sequential pattern is a postive integer and items from the same
itemset within a sequence are separated by single spaces. The value
"-1" indicates the end of an itemset. On each line, the sequential
pattern is first indicated. Then, the keyword " #SUP: " appears
followed by an integer indicating the support of the pattern as a
number of sequences. For example, a few lines of an output file
(different from the example above) could be:</p>
  <blockquote>
    <p> 1 -1 1 2 -1 #SUP: 2<br>
5 -1 7 -1 3 -1 2 -1 #SUP: 2<br>
5 -1 1 -1 3 -1 2 -1 #SUP: 2 </p>
  </blockquote>
  <p>The first line indicates that the frequent sequential pattern
consisting of the itemset {1}, followed by the itemset {1, 2}, has a
support of 2 sequences. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p> The <strong>TKS</strong> algorithm (Fournier-Viger, 2013) is
faster than TSP.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TSP algorithm is described in this paper: </p>
  <p><em>Petre Tzvetkov, Xifeng Yan, Jiawei Han: <a href="http://www.philippe-fournier-viger.com/spmf/tsp.pdf" rel="nofollow">TSP: Mining Top-K Closed Sequential Patterns</a>. ICDM
2003: 347-354</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="exampleMDSPM1" id="exampleMDSPM1"> </a></span></strong> Example 93 :
Mining Frequent Multi-dimensional Sequential Patterns from a
Multi-dimensional Sequence Database with SeqDIM, using PrefixSpan and
Apriori </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">SeqDim_(PrefixSpan+Apriori)</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">ContextMDSequenceNoTime</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 75% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run
SeqDim_(PrefixSpan+Apriori)<strong> ContextMDSequenceNoTime</strong>.txt
output.txt 75%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">ContextMDSequenceNoTime.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestMultiDimSequentialPatternMining.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is SeqDIM?</p>

<blockquote>
  <p>SeqDIM is an algorithm for discovering <strong>multi-dimensional
sequential patterns</strong> in a multi-dimensional sequence database.</p>
  <p>Why multi-dimensional sequential pattern is interesting? The
reason is that regular sequential pattern mining algorithms do not
consider the context of sequences. For example, the context of a
sequence of transactions at a supermarket could be the profile of the
customer such as his age, the city where he lives, etc. </p>
  <p>In multi-dimensional sequential pattern mining, a sequence
database is annotated with dimension to indicate the context of the
sequence. The dimensions are symbolic values.</p>
  <p>Multi-dimensional sequential pattern mining algorithm discovers
sequential patterns with context information. This can be very useful
for real applications such as market basket analysis. For example, one
could find patterns specific to customers who are teenagers and lives
in a particular city, or items boughts by adults living in another city.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>multi-dimensional sequence database (as
defined by <a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Pinto et al. 2001</a>)
  </strong>and a threshold named <strong>minsup</strong> (a value in
[0,1] representing a percentage). </p>
  <p>A <strong>multi-dimensional database </strong>is a set of <strong>multi-dimensional
sequences </strong>and a set of <strong>dimensions</strong> d1, d2...
dn. A <strong>multi-dimensional sequence (MD-Sequence) </strong>is
composed of an <strong>MD-pattern</strong> and a <strong>sequence</strong>.
A <strong>sequence</strong> is an ordered list of itemsets (groups of
items). Note that it is assumed that no items appear twice in the same
itemset and that items in an itemset are lexically ordered. An <strong>MD-pattern</strong>
is a set of symbolic values for the dimensions (here represented by
integer numbers). </p>
  <p>For example, consider the following database, provided in the file
"<strong>ContextMDSequenceNoTime.txt</strong>" of the SPMF distribution<strong>.</strong>
The database contains 4 MD-sequences. </p>
</blockquote>

<table align="center" border="1" width="573">

  <tbody>
    <tr>
      <td colspan="5">
      <div align="center"><strong>MD-Sequences</strong></div>
      </td>
    </tr>
    <tr>
      <td><strong>ID</strong></td>
      <td colspan="3"><strong>MD-Patterns</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td> <br>
      </td>
      <td><strong>d1</strong></td>
      <td><strong>d2</strong></td>
      <td><strong>d3</strong></td>
      <td> <br>
      </td>
    </tr>
    <tr>
      <td><strong>S1</strong></td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>(2 4), (3), (2), (1)</td>
    </tr>
    <tr>
      <td><strong>S2</strong></td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>(2 6), (3 5), (6 7)</td>
    </tr>
    <tr>
      <td><strong>S3</strong></td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>(1 8), (1), (2), (6)</td>
    </tr>
    <tr>
      <td><strong>S4</strong></td>
      <td>*</td>
      <td>3</td>
      <td>3</td>
      <td>(2 5), (3 5)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For instance, the first MD-Sequence represents that items 2 and 4
appeared together, then were followed by 3, which was followed by item
2, wich was followed by item 1. The context of this sequence is the
value 1 for dimension d1, the value 1 for dimension d2 and the value 1
for dimension d3. Note that the value "*" in the fourth MD-sequence
means "any values".</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is the set of all <strong>multi-dimensional sequential
patterns</strong> that appears in at least <strong>minsup</strong>
sequences of the database. Here, we will not provide a formal
definition but rather show an example. For a formal definition of what
is a multi-dimensional sequential pattern you can refer to the paper by
Pinto et al. (2001) which explains it very well. </p>
  <p> Let's look at the example. If we apply SeqDIM on the previous
database with a <strong>minsup of 50%</strong>, we obtain the
following result:</p>
</blockquote>

<table align="center" border="1" width="573">

  <tbody>
    <tr>
      <td colspan="6">
      <div align="center"><strong>Frequent MD-Sequences</strong></div>
      </td>
    </tr>
    <tr>
      <td><strong>ID</strong></td>
      <td colspan="3"><strong>MD-Patterns</strong></td>
      <td><strong>Sequences</strong></td>
      <td><strong>Support</strong></td>
    </tr>
    <tr>
      <td> <br>
      </td>
      <td><strong>d1</strong></td>
      <td><strong>d2</strong></td>
      <td><strong>d3</strong></td>
      <td> <br>
      </td>
      <td> <br>
      </td>
    </tr>
    <tr>
      <td><strong>P1</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(3)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td><strong>P2</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(2)</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td><strong>P3</strong></td>
      <td>1</td>
      <td>*</td>
      <td>*</td>
      <td>(2)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td><strong>P4</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(2), (3)</td>
      <td>75 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For instance, the third pattern (P3) represent the sequence
containing the item 2 only with the value 1 for dimension d1. Note that
the value"*" for dimension d2 and d3 means "any values". This pattern
P3 has a support of 75% because it appears in 75 % of the sequences of
the original database (it appears in S1, S2 and S3).</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a multi-dimensional sequence
from a sequence database. Each line is separated into two parts: (1) a
MD-pattern and (2) a sequence. </p>
  <ul>
    <li>The first part is a list of dimension values separated by
single spaces. A dimension value is a positive integer or the symbol
"*" meaning "any values". Finally, the value "-3" indicates the end of
the first part. Note that each line should have the same number of
dimension values.</li>
    <li>The second part of each line is a sequence. Each item in a
sequence is represented by a postive integers and items from the same
itemset within a sequence are separated by single space. Note that it
is assumed that items within a same itemset are sorted according to a
total order and that no item can appear twice in the same itemset. The
value "-1" indicates the end of an itemset. The value "-2" indicates
the end of a sequence (it appears at the end of each line). </li>
  </ul>
  <p>For example, the input file "<strong>ContextMDSequenceNoTime.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 1 1 -3 2 4 -1 3 -1 2 -1 1 -1 -2<br>
1 2 2 -3 2 6 -1 3 5 -1 6 7 -1 -2<br>
1 2 1 -3 1 8 -1 1 -1 2 -1 6 -1 -2<br>
* 3 3 -3 2 5 -1 3 5 -1 -2 </p>
  </blockquote>
  <p>This file contains four MD-sequences (four lines). Each line has 3
dimensions in each MD-Pattern. For example, consider the second line.
It represents a MD-sequence where the value for the three dimensions
are respectively 1, 2 and 2. Then, the sequence in this MD-Sequence is
the itemset {2, 6} followed by the itemset {3, 5}, followed by the
itemset {6, 7}.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent MD sequential pattern. Each
line is separated into three parts: (1) a MD-pattern, (2) a sequence
and (3) the support of the MD sequential pattern formed by the first
and second part. </p>
  <ul>
    <li>The first part is a list of dimension values separated by
single spaces between the symbols "[" and "]". A dimension value is a
positive integer or the symbol "*" meaning "any values". Note that each
line have the same number of dimension values.</li>
    <li>The second part of each line is a sequence. A sequence is a
list of itemset. Each itemset is represented by {t=0, ...} where "..."
is a list of items separated by single spaces. </li>
    <li>Each item in an itemset is represented by a postive integers.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. </li>
    <li>The third part is the keyword "#SUP:" followed by an integer
indicating the support of the pattern as a number of sequences.</li>
  </ul>
  <p>For example, a few lines from the output file from the previous
example are shown below: </p>
  <blockquote>
    <p> [ 1 * * ]{t=0, 2 }{t=0, 3 } #SUP: 2<br>
[ * * * ]{t=0, 2 }{t=0, 3 } #SUP: 3<br>
[ * * * ]{t=0, 2 }{t=0, 3 5 } #SUP: 2 </p>
  </blockquote>
  <p>Consider the first line. It presents a MD-sequential pattern
having the dimension values 1, * and *. Furthemore, the line indicates
that this pattern is for the sequence containing the itemset {2}
followed by the itemset {3} and that the MD-Sequential-pattern has a
support of 2 transactions. The next lines follow the same format.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The SeqDIM algorithm is a meta-algorithm. It requires a sequential
pattern mining algorithm for discovering sequential patterns and an
itemset mining algorithm to deal with the dimensions. In our
implementations, we have used the <strong>PrefixSpan</strong>
algorithm (<a href="http://www.philippe-fournier-viger.com/spmf/prefixspan.pdf" rel="nofollow">Pei et al., 2004</a>)
for sequential pattern mining and the <strong>Apriori</strong>
algorithm (<a href="http://www.philippe-fournier-viger.com/spmf/apriori.pdf" rel="nofollow">Agrawal &amp; Srikant,
1994</a>) for dealing with the dimensions. PrefixSpan is a very fast
algorithm. However, Apriori is not the most efficient algorithm. It
could be replaced by FPGrowth in future version for more efficiency.</p>
  <p>Note also that SeqDIM can generate a lot of patterns. A solution
to this problem is to use the algorithm by Songram et al., also offered
in SPMF. This latter algorithm eliminates a lot of redundancy in
patterns without any loss of information.</p>
  <p>Also note that the implementation of SeqDIM in SPMF needs a little
bit refactoring as it is currently integrated with the Fournier-Viger
(2008) algorithm in the code. In a future version of SPMF, the two
algorithms will be separated.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The algorithm is described in this paper: </p>
  <p><em>H. Pinto, J. Han, J Pei, K. Wang, Q. Chen, U. Dayal: <a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Multi-Dimensional Sequential Pattern Mining</a>. CIKM
2001: 81-88</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="exampleMDSPM2" id="exampleMDSPM2"> </a></span></strong> Example 94 :
Mining Frequent Closed Multi-dimensional Sequential Patterns from a
Sequence Database with SeqDIM/Songram, using Bide+ and
AprioriClose/Charm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">SeqDim_(BIDE+AprioriClose)</span>"
    </strong>algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">ContextMDSequenceNoTime</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50% </em>and (5) click "<span class="Style2">Run
algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run
SeqDim_(BIDE+AprioriClose)<strong> ContextMDSequenceNoTime</strong>.txt
output.txt 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">ContextMDSequenceNoTime.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestMultiDimSequentialPatternMiningClosed.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is the Songram et al. algorithm?</p>

<blockquote>
  <p>The Songram et al. algorithm is an algorithm for discovering <strong>closed
multi-dimensional sequential patterns</strong> in a multi-dimensional
sequence database.</p>
  <p>Why multi-dimensional sequential pattern is interesting? The
reason is that regular sequential pattern mining algorithms do not
consider the context of sequences. For example, the context of a
sequence of transactions from a supermarket could be the profile of the
customer such as his age, the city where he lives, etc. </p>
  <p>In multi-dimensional sequential pattern mining, a sequence
database is annotated with dimension to indicate the context of the
sequence. The dimensions are symbolic values.</p>
  <p>Multi-dimensional sequential pattern mining algorithm discovers
sequential patterns with context information. This can be very useful
for real applications such as market basket analysis. For example, one
could find patterns specific to customers who are teenagers and lives
in a particular city, or items boughts by adults living in another city.</p>
  <p>However, there is a problem with classical multi-dimensional
sequential pattern mining algorithm such as the one by Pinto et al.
(2001). It is that there can be a lot of redundancy in the results. For
this reason, Songram et al. proposed to discover <strong>closed
multi-dimensional sequential patterns</strong>. This allows to find a
subset of all patterns that eliminates a great deal of redundancy
without any information loss. The algorithm by Songram et al. is more
efficient than the one of Pinto (2001) in terms of execution time and
memory usage.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>multi-dimensional sequence database (as
defined by <a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Pinto et al. 2001</a>)
  </strong>and a threshold named <strong>minsup</strong> (a value in
[0,1] representing a percentage). </p>
  <p>A <strong>multi-dimensional database </strong>is a set of <strong>multi-dimensional
sequences </strong>and a set of <strong>dimensions</strong> d1, d2...
dn. A <strong>multi-dimensional sequence (MD-Sequence) </strong>is
composed of an <strong>MD-pattern</strong> and a <strong>sequence</strong>.
A <strong>sequence</strong> is an ordered list of itemsets (groups of
items). An <strong>MD-pattern</strong> is a set of symbolic values for
the dimensions (here represented by integer numbers). </p>
  <p>For example, consider the following database, provided in the file
"<strong>ContextMDSequenceNoTime.txt</strong>" of the SPMF distribution<strong>.</strong>
The database contains 4 MD-sequences. </p>
</blockquote>

<table align="center" border="1" width="573">

  <tbody>
    <tr>
      <td colspan="5">
      <div align="center"><strong>MD-Sequences</strong></div>
      </td>
    </tr>
    <tr>
      <td><strong>ID</strong></td>
      <td colspan="3"><strong>MD-Patterns</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td> <br>
      </td>
      <td><strong>d1</strong></td>
      <td><strong>d2</strong></td>
      <td><strong>d3</strong></td>
      <td> <br>
      </td>
    </tr>
    <tr>
      <td><strong>S1</strong></td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>(2 4), (3), (2), (1)</td>
    </tr>
    <tr>
      <td><strong>S2</strong></td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>(2 6), (3 5), (6 7)</td>
    </tr>
    <tr>
      <td><strong>S3</strong></td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>(1 8), (1), (2), (6)</td>
    </tr>
    <tr>
      <td><strong>S4</strong></td>
      <td>*</td>
      <td>3</td>
      <td>3</td>
      <td>(2 5), (3 5)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For instance, the first MD-Sequence represents that items 2 and 4
appeared together, then were followed by 3, which was followed by item
2, wich was followed by item 1. The context of this sequence is the
value 1 for dimension d1, the value 1 for dimension d2 and the value 1
for dimension d3. Note that the value "*" in the fourth MD-sequence
means "any values".</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is the set of all <strong>closed multi-dimensional
sequential patterns</strong> that appears in at least <strong>minsup</strong>
sequences of the database. The difference with the SeqDIM algorithm is
that this algorithm discovers "closed" multi-dimensional patterns. A
"closed" multi-dimensional pattern is a multi-dimensional pattern such
that no other pattern is include in it and appears in exactly the same
set of sequences (see the Songram et al. paper for a formal definition).</p>
  <p>Let's look at an example. If we apply the Songram et al. algorithm
on the previous database with a <strong>minsup of 50%</strong>, we
obtain the following result:</p>
</blockquote>

<table align="center" border="1" width="573">

  <tbody>
    <tr>
      <td colspan="6">
      <div align="center"><strong>Frequent Closed MD-Sequences</strong></div>
      </td>
    </tr>
    <tr>
      <td><strong>ID</strong></td>
      <td colspan="3"><strong>MD-Patterns</strong></td>
      <td><strong>Sequences</strong></td>
      <td><strong>Support</strong></td>
    </tr>
    <tr>
      <td> <br>
      </td>
      <td><strong>d1</strong></td>
      <td><strong>d2</strong></td>
      <td><strong>d3</strong></td>
      <td> <br>
      </td>
      <td> <br>
      </td>
    </tr>
    <tr>
      <td><strong>P1</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(2)</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td><strong>P2</strong></td>
      <td>1</td>
      <td>*</td>
      <td>1</td>
      <td>(1)</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td><strong>P3</strong></td>
      <td>1</td>
      <td>2</td>
      <td>*</td>
      <td>(2), (6)</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td><strong>P4</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(2), (3 5)</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td><strong>P5</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(2), (3)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td><strong>P6</strong></td>
      <td>1</td>
      <td>*</td>
      <td>*</td>
      <td>(2), (3)</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td><strong>P7</strong></td>
      <td>1</td>
      <td>*</td>
      <td>*</td>
      <td>(2)</td>
      <td>75 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For instance, the second patterns found (P2) represents that items
2 is followed by item 6. The context of this pattern is the value 1 for
dimension d1, 2 for dimension d2 and any value for dimension d3. Note
that the value "*" means "any values". This pattern is said to have a
support of 50% because it appears in 50 % of the MD-Sequences from the
original database (it appears in S2 and S3).</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a multi-dimensional sequence
from a sequence database. Each line is separated into two parts: (1) a
MD-pattern and (2) a sequence. </p>
  <ul>
    <li>The first part is a list of dimension values separated by
single spaces. A dimension value is a positive integer or the symbol
"*" meaning "any values". Finally, the value "-3" indicates the end of
the first part. Note that each line should have the same number of
dimension values.</li>
    <li>The second part of each line is a sequence. Each item in a
sequence is represented by a postive integers and items from the same
itemset within a sequence are separated by single space. Note that it
is assumed that items within a same itemset are sorted according to a
total order and that no item can appear twice in the same itemset. The
value "-1" indicates the end of an itemset. The value "-2" indicates
the end of a sequence (it appears at the end of each line). </li>
  </ul>
  <p>For example, the input file "<strong>ContextMDSequenceNoTime.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 1 1 -3 2 4 -1 3 -1 2 -1 1 -1 -2<br>
1 2 2 -3 2 6 -1 3 5 -1 6 7 -1 -2<br>
1 2 1 -3 1 8 -1 1 -1 2 -1 6 -1 -2<br>
* 3 3 -3 2 5 -1 3 5 -1 -2 </p>
  </blockquote>
  <p>This file contains four MD-sequences (four lines). Each line has 3
dimensions in each MD-Pattern. For example, consider the second line.
It represents a MD-sequence where the value for the three dimensions
are respectively 1, 2 and 2. Then, the sequence in this MD-Sequence is
the itemset {2, 6} followed by the itemset {3, 5}, followed by the
itemset {6, 7}.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent MD sequential pattern. Each
line is separated into three parts: (1) a MD-pattern, (2) a sequence
and (3) the support of the MD sequential pattern formed by the first
and second part. </p>
  <ul>
    <li>The first part is a list of dimension values separated by
single spaces between the symbols "[" and "]". A dimension value is a
positive integer or the symbol "*" meaning "any values". Note that each
line have the same number of dimension values.</li>
    <li>The second part of each line is a sequence. A sequence is a
list of itemset.Each itemset is represented by {t=0, ...} where "..."
is a list of items separated by single spaces. Each item in a is
represented by a postive integers. Note that it is assumed that items
within a same itemset are sorted according to a total order and that no
item can appear twice in the same itemset. </li>
    <li>The third part is the keyword "#SUP:" followed by an integer
indicating the support of the MD</li>
  </ul>
  <p>For example, here is the output file from the previous example: </p>
  <blockquote>
    <p>[ 1 2 * ]{t=0, 2 }{t=0, 6 } #SUP: 2<br>
[ 1 * * ]{t=0, 2 }{t=0, 3 } #SUP: 2<br>
[ 1 * * ]{t=0, 2 } #SUP: 3<br>
[ 1 * 1 ]{t=0, 1 } #SUP: 2</p>
  </blockquote>
  <p>Consider the first line. It presents a MD-sequential pattern
having the dimension values 1, 2 and *. Furthemore, the line indicates
that the sequence of this MD sequential pattern is the itemset {2}
followed by the itemset {6} and that the MD-Sequential-pattern has a
support of 2 transactions. The next lines follow the same format.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The Songram et al. algorithm is a meta-algorithm. It requires a
closed sequential pattern mining algorithm for discovering sequential
patterns and a closed itemset mining algorithm to deal with the
dimensions. Our implementation uses the <strong>SeqDIM/Songram </strong>algorithm
(<a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Pinto et al. 2001</a>, <a href="http://www.philippe-fournier-viger.com/spmf/songram06.pdf" rel="nofollow">Songram et al. 2006</a>) in
combination with <strong>BIDE+</strong> (<a href="http://www.philippe-fournier-viger.com/spmf/icde04_bide.pdf" rel="nofollow">Wang et al. 2007</a>) and <strong>AprioriClose</strong>(<a href="http://www.philippe-fournier-viger.com/spmf/pasquier99.pdf" rel="nofollow">Pasquier et al., 1999</a>) or <strong>Charm
  </strong>(Zaki, 2002). To choose AprioriClose in the graphical user
interface, select "<span class="Style2">SeqDim_(BIDE+AprioriClose)</span>".
To use Charm, select "<span class="Style2">SeqDim_(BIDE+Charm)</span>"</p>
  <p>Note that the implementation of Songram et al. algorithm in SPMF
needs a little bit refactoring as it is currently integrated with the
Fournier-Viger (2008) algorithm in the code. In a future version of
SPMF, they will be separated. This is not really a problem for
performance but it would make it easier to reuse the algorithms if both
were separated.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The algorithm is described in this paper:</p>
  <p><em>P. Songram, V. Boonjing, S. Intakosum: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/songram06.pdf">Closed
Multi-dimensional Sequential-Pattern Minin</a>.</em> Proc. of ITNG 2006.</p>
  <p>The idea of multi-dimensional pattern mining is based on this
paper: </p>
  <p><em>H. Pinto, J. Han, J Pei, K. Wang, Q. Chen, U. Dayal: <a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Multi-Dimensional Sequential Pattern Mining</a>. CIKM
2001: 81-88</em></p>
  <p>The idea of mining closed sequential pattern is based on this
paper:</p>
  <p><em>J. Wang, J. Han: <a href="http://www.philippe-fournier-viger.com/spmf/icde04_bide.pdf" rel="nofollow">BIDE: Efficient Mining of Frequent Closed Sequences</a>.
ICDE 2004: 79-90</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example11" id="example11"> </a></span></strong>
Example 95 : Mining Sequential Patterns <strong>with
Time Constraints</strong> from a Time-Extended Sequence Database </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">HirateYamana</span>" </strong>algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style2">contextSequencesTimeExtended</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
(e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (4)
minsup= 55 %, min_time_interval = 0, max_time_interval = 2,
min_whole_interval = 0, max_whole_interval = 2, (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run HirateYamana <strong>contextSequencesTimeExtended</strong>.txt
output.txt 55% 0 2 0 2</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextSequencesTimeExtended.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestSequentialPatternsMining1_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>The <a href="http://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf" rel="nofollow">Hirate-Yamana, 2006</a>
algorithm is an algorithm for discovering <strong>frequent sequential
patterns respecting some time-constraints </strong>to filter
uninteresting patterns.</p>
  <p>The idea of using time constraints is interesting because it can
greatly reduce the number of patterns found and it is also faster and
use less memory than if all patterns are discovered.</p>
  <p>Note that in our implementation, the Hirate &amp; Yamana algorithm
is not implemented as a standalone algorithm. The features of the
Hirate-Yamana 2006 algorithm are rather integrated in the
Fournier-Viger et al. (2008) algorithm that combines features from
several other sequential pattern mining algorithms. </p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>time-extended sequence</strong> <strong>database</strong>
(as defined by <a href="http://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf" rel="nofollow">Hirate-Yamana,
2006</a>) and some <strong>constraints</strong>.</p>
  <p>A <strong>time-extended sequence database</strong> is a set of <strong>time-extended
sequences</strong>. A <strong>time-extended sequences </strong>is a
list of itemsets (groups of items). Each <strong>itemset</strong> is
anotated with a <strong>timestamp</strong> that is an integer value.
Note that it is assumed that an item should not appear more than once
in an itemset and that items in an itemset are lexically ordered.</p>
  <p>For example, consider the following time-extended sequence
database provided in the file <strong>contextSequencesTimeExtended.txt
  </strong>of the SPMF distribution. The database contains 4
time-extended sequences. Each sequence contains itemsets that are
annotated with a timestamp. For example, consider the sequence S1. This
sequence indicates that itemset {1} appeared at time 0. It was followed
by the itemset {1, 2, 3} at time 1. This latter itemset was followed by
the itemset {1 2} at time 2. </p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td width="65"><strong>ID</strong></td>
      <td width="380"><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(<strong>0</strong>, 1), (<strong>1</strong>, 1 2 3}), (<strong>2</strong>,
1 3)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(<strong>0</strong>, 1 ) (<strong>1</strong>,
1 2 ), (<strong>2</strong>, 1 2 3), (<strong>3</strong>, 1 2 3 )</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(<strong>0</strong>, 1 2), (<strong>1</strong>, 1 2 )</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1 2 3 )</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The algorithms discovers time-extended sequential patterns that
are common to several sequences. To do that, the user needs to provide
five constraints (see the paper by Hirate &amp; Yamana, 2006 for full
details):</p>
  <ul>
    <li>minimum support (<strong>minsup</strong>): the minimum number
of sequences that should contain a sequential patterns (a positive
integer &gt;=0)</li>
    <li>minimum time interval allowed between two succesive itemsets of
a sequential pattern (<strong>min_time_interval</strong>) (an integer
&gt;=0)</li>
    <li>maximum time interval allowed between two succesive itemsets of
a sequential pattern (<strong>max_time_interval</strong>) (an integer
&gt;=0)</li>
    <li>minimum time interval allowed between the first itemset and the
last itemset of a sequential pattern (<strong>min_whole_interval</strong>)
(an integer &gt;=0)</li>
    <li>maximum time interval allowed between the first itemset and the
last itemset of a sequential pattern (<strong>max_whole_interval</strong>)
(an integer &gt;=0)</li>
  </ul>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is a set of time-extended sequential patterns meeting
the five constraints given by the user. For example, if we run the
algorithm with minsup= 55 %, min_time_interval = 0, max_time_interval =
2, min_whole_interval = 0, max_whole_interval = 2, we obtain the
following results:</p>
</blockquote>

<table align="center" border="1" width="477">

  <tbody>
    <tr>
      <td width="18"><strong>ID</strong></td>
      <td width="292"><strong>Sequential Patterns</strong></td>
      <td width="145"><strong>Support</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(<strong>0</strong>, 3)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(<strong>0</strong>, 2 3)</td>
      <td bgcolor="#ffffff">75 %</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(<strong>0</strong>, 2)</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(<strong>0</strong>, 1 2 3)</td>
      <td>75%</td>
    </tr>
    <tr>
      <td>S5</td>
      <td>(<strong>0</strong>, 1 2)</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>S6</td>
      <td>(<strong>0</strong>, 1 3)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S7</td>
      <td>(<strong>0</strong>, 1)</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>S8</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 3)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S9</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1 2)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S10</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1 3)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S11</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1)</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>S12</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 2)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S13</td>
      <td>(<strong>0</strong>, 1), (<strong>1</strong>, 1 2)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S14</td>
      <td>(<strong>0</strong>, 1), (<strong>1</strong>, 1)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S15</td>
      <td>(<strong>0</strong>, 1), (<strong>1</strong>, 2)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>S16</td>
      <td>(<strong>0</strong>, 1 2), (<strong>1</strong>, 1)</td>
      <td>75 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For instance, the pattern S16 indicates that the items 1 and 2
were followed by item 1 one time unit after. This pattern has a support
of 75 % because it appears in S1, S2 and S3. It is important to note
that the timestamps in the sequential patterns found are relative. For
example, the pattern S16 is considered to appear in S1, S2 and S3
because {1} appears one time unit after the itemset {1, 2} in all of
these sequences, even though the timestamps do not need to be the same
in all of these sequences.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a time-extended sequence from
a sequence database. Each line is a list of itemsets, where each
itemset has a timestamp represented by a positive integer and each item
is represented by a positive integer. Each itemset is first represented
by it timestamp between the "&lt;" and "&gt; symbol. Then, the items of
the itemset appear separated by single spaces. Finally, the end of an
itemset is indicated by "-1". After all the itemsets, the end of a
sequence (line) is indicated by the symbol "-2". Note that it is
assumed that items are sorted according to a total order in each
itemset and that no item appears twice in the same itemset.</p>
  <p>For example, the input file "<strong>contextSequencesTimeExtended.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>&lt;0&gt; 1 -1 &lt;1&gt; 1 2 3 -1 &lt;2&gt; 1 3 -1 -2<br>
&lt;0&gt; 1 -1 &lt;1&gt; 1 2 -1 &lt;2&gt; 1 2 3 -1 &lt;3&gt; 1 2 3 -1 -2<br>
&lt;0&gt; 1 2 -1 &lt;1&gt; 1 2 -1 -2<br>
&lt;0&gt; 2 -1 &lt;1&gt; 1 2 3 -1 -2</p>
  </blockquote>
  <p>Consider the first line. It indicates that at time "0" the itemset
{1} appeared, followed by the itemset {1, 2, 3} at time 1, then
followed by the itemset {1, 3} at time 2. Note that timestamps do not
need to be consecutive integers. But they should increase for each
succesive itemset within a sequence. The second, third and fourth line
follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a time-extended frequent sequential
pattern. Each line starts by listing the itemsets of the sequential
pattern, where each itemset has a relative timestamp represented by a
positive integer between the "&lt;" and "&gt; symbol. Then the
timestamp is followed by each item in the itemset, each represented by
a positive integer. The items of the itemset appear separated by single
spaces and the symbol "-1" indicates the end of an itemset. Finally,
after all the itemsets of a sequential pattern, the keyword "#SUP:" is
followed by an integer indicating the support of the pattern as a
number of sequences. For example, here is a two lines from the output
file from the previous example: </p>
  <blockquote>
    <p>&lt;0&gt; 1 2 -1 &lt;1&gt; 1 3 -1 #SUP: 2<br>
&lt;0&gt; 1 2 -1 &lt;1&gt; 1 2 -1 #SUP: 2</p>
  </blockquote>
  <p>Consider the first line. It represents a sequential pattern having
the itemset {1, 2} with a relative timestamp of 0, followed by the
itemset {1, 3} one time unit later. This pattern has a support of 2
sequences. The second line follow the same format.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>In this implementation, we have followed the Hirate &amp; Yamana
(2006) algorithm closely. The only difference is that we did not keep
the idea of interval itemization function for discretization. But we
have keep the core idea which is to use the time constraints.</p>
  <p>Note that the Hirate &amp; Yamana algorithm is an extension of the
PrefixSpan algorithm. We have implemented it based on our
implementation of PrefixSpan.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The Hirate &amp; Yamana algorithm is described in this paper</p>
  <p><em>Yu Hirate, Hayato Yamana (2006) <a href="http://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf" rel="nofollow">Generalized Sequential Pattern Mining with Item
Intervals. JCP 1(3): 51-60</a>.</em></p>
  <p>The implementation of the algorithm in SPMF is part of the
Fournier-Viger (2008) algorithm, which is described in this paper:</p>
  <p><em> Fournier-Viger, P., Nkambou, R &amp; Mephu Nguifo, E. (2008),<a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">
A Knowledge Discovery Framework for Learning Task Models from User
Interactions in Intelligent Tutoring Systems</a>. Proceedings of the
7th Mexican International Conference on Artificial Intelligence (MICAI
2008). LNAI 5317, Springer, pp. 765-778.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example12" id="example12"> </a></span></strong>Example 96 : Mining Closed Sequential Patterns <strong>with
Time Constraints</strong> from a Time-Extended Sequence Database </h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Fournier08-Closed+time</span>"
    </strong>algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextSequencesTimeExtended</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) minsup= 55 %, min_time_interval = 0, max_time_interval = 2,
min_whole_interval = 0, max_whole_interval = 2, (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run Fournier08-Closed+time<strong>
contextSequencesTimeExtended</strong>.txt output.txt 55% 0 2 0 2</span>
in a folder containing <span class="Style2">spmf.jar</span> and the
example input file <span class="Style2">contextSequencesTimeExtended.txt</span>.</li>
  <li><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestSequentialPatternsMining2_saveToMemory.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>The <a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">Fournier-Viger
et al., 2008</a> algorithm is a sequential pattern mining algorithm
combining features from several other sequential pattern mining
algorithms. It also offers some original features. In this example, we
show how it can be used to discover <strong>closed sequential patterns
with time-constraints. </strong></p>
  <p>Closed sequential patterns is a compact representation of all
sequential patterns. Mining closed sequential patterns is important
because it can greatly reduce the number of patterns found without loss
of information. Using time-constraint is important because it allows to
filter unininteresting patterns according to time-related constraints.</p>
  <p>Mining closed patterns or using time constraints is also important
because it can greatly improve the speed and memory usage when these
constraints are used.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>time-extended sequence</strong> <strong>database</strong>
(as defined by <a href="http://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf" rel="nofollow">Hirate-Yamana,
2006</a>) and some <strong>constraints</strong>.</p>
  <p>A <strong>time-extended sequence database</strong> is a set of <strong>time-extended
sequences</strong>. A<strong> time-extended sequences</strong> is a
list of itemsets (groups of items). Each <strong>itemset</strong> is
anotated with a <strong>timestamp</strong> that is an integer value. </p>
  <p>For example, consider the following time-extended sequence
database provided in the file <strong>contextSequencesTimeExtended.txt
  </strong>of the SPMF distribution. The database contains 4
time-extended sequences. Each sequence contains itemsets that are
annotated with a timestamp. For example, consider the sequence S1. This
sequence indicates that itemset {1} appeared at time 0. It was followed
by the itemset {1, 2, 3} at time 1. This latter itemset was followed by
the itemset {1 2} at time 2.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(<strong>0</strong>, 1), (<strong>1</strong>, 1 2 3}), (<strong>2</strong>,
1 3)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(<strong>0</strong>, 1 ) (<strong>1</strong>,
1 2 ), (<strong>2</strong>, 1 2 3), (<strong>3</strong>, 1 2 3 )</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(<strong>0</strong>, 1 2), (<strong>1</strong>, 1 2 )</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1 2 3 )</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The algorithms discovers <strong>closed time-extended sequential
patterns</strong> that are common to several sequences. To do that, the
user needs to provide five constraints (see the paper by Hirate &amp;
Yamana, 2006 for full details):</p>
  <ul>
    <li>minimum support (<strong>minsup</strong>): the minimum number
of sequences that should contain a sequential patterns (an integer
&gt;=1)</li>
    <li>minimum time interval allowed between two succesive itemsets of
a sequential pattern (<strong>min_time_interval</strong>) (an integer
&gt;=0)</li>
    <li>maximum time interval allowed between two succesive itemsets of
a sequential pattern (<strong>max_time_interval</strong>) (an integer
&gt;=0)</li>
    <li>minimum time interval allowed between the first itemset and the
last itemset of a sequential pattern (<strong>min_whole_interval</strong>)
(an integer &gt;=0)</li>
    <li>maximum time interval allowed between the first itemset and the
last itemset of a sequential pattern (<strong>max_whole_interval</strong>)
(an integer &gt;=0)</li>
  </ul>
  <p><strong>Note : </strong>It is recommended to set <strong>max_whole_interval</strong>
to a very large value because if it is used with closed sequential
pattern mining, the algorithm become approximate and may not return all
closed itemsets respecting the time constraint (the reason is that this
constraint is not compatible with the "backScan pruning" of the BIDE+
algorithm).</p>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is a set of <strong>closed time-extended sequential
patterns </strong>meeting the constraints given by the user. For
example, if we run the algorithm with minsup= 55 %, min_time_interval =
0, max_time_interval = 2, min_whole_interval = 0, max_whole_interval =
100, we obtain the following results:</p>
  <table align="center" border="1" width="477">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequential Patterns</strong></td>
        <td><strong>Support</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(<strong>0</strong>, 1 2 3)</td>
        <td>75%</td>
      </tr>
      <tr>
        <td>S2</td>
        <td>(<strong>0</strong>, 1 2)</td>
        <td>100 %</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1 2)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1 3)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S5</td>
        <td>(<strong>0</strong>, 2), (<strong>1</strong>, 1)</td>
        <td>100 %</td>
      </tr>
      <tr>
        <td>S6</td>
        <td>(<strong>0</strong>, 1), (<strong>1</strong>, 1 2)</td>
        <td>75 %</td>
      </tr>
      <tr>
        <td>S7</td>
        <td>(<strong>0</strong>, 1 2), (<strong>1</strong>, 1)</td>
        <td>75 %</td>
      </tr>
    </tbody>
  </table>
  <p>For instance, the last pattern S7 indicates that the items 1 and 2
were followed by item 1 one time unit after. This pattern has a support
of 75 % because it appears in S1, S2 and S3. It is important to note
that the timestamps in the sequential patterns found are relative. For
example, the pattern S16 is considered to appear in S1, S2 and S3
because {1} appears one time unit after {1, 2} in all of these
sequences, even though the timestamps do not need to be the same in all
of these sequences.</p>
</blockquote>

<blockquote>
  <p>If you compare the results of this example with the previous
example, you can observe that the number of closed time-extended
sequential patterns (6) is much smaller than the number of
time-extended sequential patterns (15) found in the previous example. </p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a time-extended sequence from
a sequence database. Each line is a list of itemsets, where each
itemset has a timestamp represented by a positive integer and each item
is represented by a positive integer. Each itemset is first represented
by it timestamp between the "&lt;" and "&gt; symbol. Then, the items of
the itemset appear separated by single spaces. Finally, the end of an
itemset is indicated by "-1". After all the itemsets, the end of a
sequence (line) is indicated by the symbol "-2". Note that it is
assumed that items are sorted according to a total order in each
itemset and that no item appears twice in the same itemset.</p>
  <p>For example, the input file "<strong>contextSequencesTimeExtended.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>&lt;0&gt; 1 -1 &lt;1&gt; 1 2 3 -1 &lt;2&gt; 1 3 -1 -2<br>
&lt;0&gt; 1 -1 &lt;1&gt; 1 2 -1 &lt;2&gt; 1 2 3 -1 &lt;3&gt; 1 2 3 -1 -2<br>
&lt;0&gt; 1 2 -1 &lt;1&gt; 1 2 -1 -2<br>
&lt;0&gt; 2 -1 &lt;1&gt; 1 2 3 -1 -2</p>
  </blockquote>
  <p>Consider the first line. It indicates that at time "0" the itemset
{1} appeared, followed by the itemset {1, 2, 3} at time 1, then
followed by the itemset {1, 3} at time 2. Note that timestamps do not
need to be consecutive integers. But they should increase for each
succesive itemset within a sequence. The second, third and fourth line
follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a time-extended frequent closed sequential
pattern. Each line starts by listing the itemsets of the sequential
pattern, where each itemset has a relative timestamp represented by a
positive integer between the "&lt;" and "&gt; symbol. Then the
timestamp is followed by each item in the itemset, each represented by
a positive integer. The items of the itemset appear separated by single
spaces and the symbol "-1" indicates the end of an itemset. Finally,
after all the itemsets of a sequential pattern, the keyword "#SUP:" is
followed by an integer indicating the support of the pattern as a
number of sequences. For example, here is a two lines from the output
file from the previous example: </p>
  <blockquote>
    <p>&lt;0&gt; 1 2 -1 &lt;1&gt; 1 3 -1 #SUP: 2<br>
&lt;0&gt; 1 2 -1 &lt;1&gt; 1 2 -1 #SUP: 2</p>
  </blockquote>
  <p>Consider the first line. It represents a sequential pattern having
the itemset {1, 2} with a relative timestamp of 0, followed by the
itemset {1, 3} one time unit later. This pattern has a support of 2
sequences. The second line follow the same format.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>To propose an algorithm to discover closed sequential patterns
with time constraints, we have combined ideas from the BIDE+ algorithm
and the Hirate &amp; Yamana algorithm. Both of these algorithms are
based on the PrefixSpan algorithm. </p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The Fournier-Viger (2008) algorithm is described in this paper:</p>
  <p><em> Fournier-Viger, P., Nkambou, R &amp; Mephu Nguifo, E. (2008),<a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">
A Knowledge Discovery Framework for Learning Task Models from User
Interactions in Intelligent Tutoring Systems</a>. Proceedings of the
7th Mexican International Conference on Artificial Intelligence (MICAI
2008). LNAI 5317, Springer, pp. 765-778.</em></p>
  <p>The idea of using time-constraints is based on this paper</p>
  <p><em>Yu Hirate, Hayato Yamana (2006) <a href="http://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf" rel="nofollow">Generalized Sequential Pattern Mining with Item
Intervals. JCP 1(3): 51-60</a>.</em></p>
  <p>The idea of mining closed sequential pattern is based on this
paper:</p>
  <p><em>J. Wang, J. Han: <a href="http://www.philippe-fournier-viger.com/spmf/icde04_bide.pdf" rel="nofollow">BIDE: Efficient Mining of Frequent Closed Sequences</a>.
ICDE 2004: 79-90</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example13" id="example13"> </a></span></strong>
Example 97 Mining Sequential Patterns <strong>with Time
Constraints</strong> from a Time-Extended Sequence Database containing
Valued Items</h3>

<p>How to run this example?</p>

<blockquote>
  <p><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestSequentialPatternsMining3.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</p>
  <p><strong>This example is not available in the release version of
SPMF.</strong></p>
</blockquote>

<p>What is this algorithm?</p>

<blockquote>
  <p>The <a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">Fournier-Viger
et al., 2008</a> algorithm is a sequential pattern mining algorithm
combining features from several other sequential pattern mining
algorithms. It also offers some original features. In this example, we
show how it can be used to discover <strong> sequential patterns with
time-constraints from a sequence database where items are annotated
with integer values. </strong></p>
  <p>To create an algorithm that can do that, we have extended the
Hirate &amp; Yamana (2006) algorithm to accept items with integer
values.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>time-extended sequence</strong> <strong>database
where items are annotated with integer values </strong>(<a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">Fournier-Viger
et al., 2008</a>), a <em>minsup </em>threshold (a value in [0, 1]
representing a percentage) and some <strong>constraints</strong>.</p>
</blockquote>

<blockquote>
  <p>Such a database is defined as a set of sequences. A sequence is
here a list of itemsets (groups of items), where each item is a symbol
represented by an integer and each item is annotated with an integer
representing a value associated with the item. Furthermore, an itemset
has a timestamp indicating the time at which it occured. Note that an
item is not allowed to occur more than once in an itemset and that
items in an itemset are assumed to be lexically ordered.</p>
  <p>For example, consider the following database. These integer values
that are annotation to items are indicated with a bold font and blue
color. Consider the sequence S1 of this database. At time 0, the item 1
occured with the value 2, and item 2 is not annotated with a value. At
time 1, item 3 appeared. At time 2, item 6 occured. At time 3, item 5
occured with the value 1.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(<strong>0</strong>, 1(<span class="Style3">2</span>) 2), (<strong>1</strong>,
3), (<strong>2</strong>, 6), (<strong>3</strong>, 5(<span class="Style3">1</span>))</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(<strong>0</strong>, 1(<span class="Style3">2</span>)
2), (<strong>1</strong>, 4(<span class="Style3">8</span>)), (<strong>2</strong>,
3), (<strong>3</strong>, 5(<span class="Style3">2</span>) 6 7)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(<strong>0</strong>, 1(<span class="Style3">3</span>) 2), (<strong>1</strong>,
4(<span class="Style3">7</span>)), (<strong>2</strong>, 3), (<strong>3,</strong>
5(<span class="Style3">4</span>))</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(<strong>0</strong>, 1(<span class="Style3">3</span>) 2), (<strong>1</strong>,
4(<span class="Style3">6</span>)), (<strong>2</strong>, 5(<span class="Style3">5</span>)), (<strong>3</strong>, 6 7)</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>To mine sequential patterns from a time-extended sequence database
where items can be annotated with integer values, we have added an
original mechanism in the <strong>Fournier-Viger et al. algorithm. </strong>Because
it is a little bit complicated to explain, please refer to <a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">Fournier-Viger
et al., 2008 </a>for a detailed description. </p>
  <p>As PrefixSpan, the algorithm grows patterns one item at a time by
recursively projecting a database by a frequent item. However, we have
modified this step so that when the support of an item that is
annotated is higher or equals to 2 * <em>minsup</em>, the database
projection operation calls the K-Means algorithm [15] to try to
separate these values in two or more clusters. Thereafter, the database
will be projected separately for each group of values. Thus, the
different groups will be considered as different items. </p>
  <p>This is best illustrated with an example. If we mine patterns from
the first table with <em>minsup = 50%</em>, we can get 56 sequential
patterns. Note however that the number of patterns found can vary
because K-Means is a randomized algorithm. For this example, six
patterns found are presented in the next table:</p>
</blockquote>

<table align="center" border="1" width="681">

  <tbody>
    <tr>
      <td width="20"><strong>ID</strong></td>
      <td width="489"><strong>Sequential Patterns</strong></td>
      <td width="150"><strong>Support</strong></td>
    </tr>
    <tr>
      <td>P1</td>
      <td>(<strong>0</strong>, 3)</td>
      <td>75%</td>
    </tr>
    <tr>
      <td>P2</td>
      <td>(<strong>0</strong>, 5 (<span class="Style3">average: 3 min:1
max:5</span>))</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>P3</td>
      <td>(<strong>0</strong>, 6)</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>P4</td>
      <td>(<strong>0</strong>, 3), (<strong>1</strong>, 6)</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td>P5</td>
      <td>(<strong>0</strong>, 1 (<span class="Style3">average: 3 min:
3 max: 3</span>) 2), (<strong>1</strong> 4 (<span class="Style3">average:
6.5 min: 6 max: 7</span>))</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td>P6</td>
      <td>(<strong>0</strong>, 1 ((<strong class="Style3">average: </strong><span class="Style3">2 min: 2 max: 2</span>) 2), (<strong>3</strong>, 5 (<span class="Style3">average: 3.5 min: 1 max: 2</span>))</td>
      <td>50 %</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>When the algorithm was executed, as some point, it considered
projecting the database with item "1" because this item is frequent.
Item "1" is annotated with values 2, 2, 3 and 3 in sequences S1, S2, S3
and S4, respectively. The algorithm applied the K-Means to find
clusters. Two clusters were found. They are {2, 2}and {3, 3}. We can
see this in sequences P5 and P6. In sequence P5, item "1" represents
the cluster {3, 3}, whereas sequence P6 include item "1" with the
cluster {2, 2}. This feature of clustering is useful as it allows to
group similar values together for an item and treat them differently.</p>
  <p>In the source code of <strong><span class="Style2">MainTestSequentialPatternsMining3.java</span></strong>,
there is a few parameters for K-Means. Two of these parameters are
particularly important:</p>
  <ul>
    <li><strong>MaxK</strong> (an integer value &gt;=1): This parameter
is the maximum number of clusters to create. If you set this parameter
to two for example, no more than two clusters will be made every time
that K-Means is called. If you want to desactivate the clustering, this
parameter should be set to 1 (only one cluster will always be made).</li>
    <li><strong>numberOfTriesForEachK </strong>(an integer value
&gt;=1): This parameter indicates the number of times that K-Means
should be run each time there is some values to be clustered. Normally,
this value should be set to 1. If you set this parameter to a value
different than 1, K-Means will be run the number of times that you
specify and the maximum number of clusters found will be kept. This
parameter was added because K-Means is a randomized algorithm.</li>
  </ul>
  <p class="Style4"><span class="Style5">Important note:</span> <span class="Style5">If the clustering described in this example is used
jointly with the mining of closed sequential patterns (described in a
previous example), the set of patterns found may not be a lossless
representation of all patterns. </span></p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a time-extended sequence from
a sequence database. Each line is a list of itemsets, where each
itemset has a timestamp represented by a positive integer and each item
is represented by a positive integer. Each itemset is first represented
by its timestamp between the "&lt;" and "&gt; symbol. Then, the items
of the itemset appear separated by single spaces. Finally, the end of
an itemset is indicated by "-1". After all the itemsets, the end of a
sequence (line) is indicated by the symbol "-2". Note that it is
assumed that items are sorted according to a total order in each
itemset and that no item appears twice in the same itemset.
Furthermore, items can be annotated with a positive integer value
specified between the "(" and ")" symbols.</p>
  <p>For example, the input file "<strong>contextSequencesTimeExtended_ValuedItems.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>&lt;0&gt; 1(2) 2 -1 &lt;1&gt; 3 -1 &lt;2&gt; 6 -1 &lt;3&gt; 5(1)
-1 -2<br>
&lt;0&gt; 1(2) 2 -1 &lt;1&gt; 4(8) -1 &lt;2&gt; 3 -1 &lt;3&gt; 5(2) 6 7
-1 -2<br>
&lt;0&gt; 1(3) 2 -1 &lt;1&gt; 4(7) -1 &lt;2&gt; 3 -1 &lt;3&gt; 5(4) -1
-2<br>
&lt;0&gt; 1(3) 2 -1 &lt;1&gt; 4(6) -1 &lt;2&gt; 5(5) -1 &lt;3&gt; 6 7
-1 -2</p>
  </blockquote>
  <p>Consider the first line. It indicates that at time "0" the item 1
appears with the value 2 and the item 2 with no value (a value of 0 is
assumed by default). Then, at time 1, the item 3 appears with no value.
Then, at time 2, the item 6 appears with no value. Then at time 3, the
item 5 appears with the value 1. The following sequences follow the
same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential pattern. Each line starts by
listing the itemsets of the sequential pattern, where each itemset has
a relative timestamp represented by a positive integer between the
"&lt;" and "&gt; symbol. Then the timestamp is followed by each item in
the itemset, each represented by a positive integer. The items of the
itemset appear separated by single spaces and the symbol "-1" indicates
the end of an itemset. Finally, after all the itemsets of a sequential
pattern, the keyword "#SUP:" is followed by an integer indicating the
support of the pattern as a number of sequences. Note that each item
can be annotated with information about values specified between the
"(" and ")" symbols. Information about values are a double value
indicating the average value for this item in this pattern and the
minimum and maximum values. For example, here is a few lines from the
output file from the previous example: </p>
  <blockquote>
    <p>&lt;0&gt; 1 (2.5, min=2.0 max=3.0) -1 &lt;3&gt; 7 -1 #SUP: 2<br>
&lt;0&gt; 1 (2.5, min=2.0 max=3.0) -1 &lt;3&gt; 6 7 -1 #SUP: 2<br>
&lt;0&gt; 1 (2.5, min=2.0 max=3.0) -1 &lt;3&gt; 6 -1 #SUP: 2</p>
  </blockquote>
  <p>Consider the first line. It represents a sequential pattern. The
first itemset occured has a relative timestamp of 0. Furthermore, it
contains the item 1 with an average value of 2.5 and a minimum and
maximum values respectively of 2 and 3. Then, a second itemset appears
with a relative timestamp of 3. It contains the item 7. The support of
the sequential pattern is 2 sequences. The two other lines follow the
same format.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The algorithm is described in this paper:</p>
  <p><em> Fournier-Viger, P., Nkambou, R &amp; Mephu Nguifo, E. (2008),<a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">
A Knowledge Discovery Framework for Learning Task Models from User
Interactions in Intelligent Tutoring Systems</a>. Proceedings of the
7th Mexican International Conference on Artificial Intelligence (MICAI
2008). LNAI 5317, Springer, pp. 765-778.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="example14" id="example14"> </a></span></strong>
Example 98 : (Closed) Multi-dimensional Sequential
Patterns <strong>Mining</strong> with Time Constraints</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">SeqDim_(BIDE+AprioriClose)+time</span>"
    </strong>algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">ContextMDSequence</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = 50%, mininterval=0, maxinterval=1000,
minwholeinterval=0, maxwholeinterval=1000 </em>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run
SeqDim_(BIDE+AprioriClose)+time<strong> ContextMDSequence</strong>.txt
output.txt 50% 0 1000 0 1000</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">ContextMDSequence.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestSequentialPatternsMining4.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this algorithm?</p>

<blockquote>
  <p>The <a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">Fournier-Viger
et al., 2008</a> algorithm is a sequential pattern mining algorithm
combining features from several other sequential pattern mining
algorithms. It offers also some original features. In this example, we
show how it can be used to discover <strong>multi-dimensional
sequential patterns with time-constraints. </strong></p>
  <p><strong>Multi-dimensional sequential pattern mining </strong>is
an extension of the problem of sequential pattern mining that consider
the context of each sequences. We have taken the SeqDIM algorithm for
multi-dimensional sequential pattern mining and modified it to use
features of the Hirate &amp; Yamana (2008) algorithms so that it only
discovers patterns that respect time constraints set by the user.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a <strong>multidimensional time-extended sequence</strong>
  <strong>database</strong>. We here only provide a brief explanation.
Please refer to the article by <a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">Fournier-Viger
et al., 2008</a> for more details and a formal definition.</p>
</blockquote>

<blockquote>
  <p>A <strong>time-extended multidimensional sequence database</strong>
is a set of time-extended multi-dimensional sequences. A <strong>time-extended
multi-dimensional sequence</strong> (here called MD-Sequence) is a
time-extended sequence (as defined by Hirate &amp; Yamana) but with
dimensional information (as defined by <a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Pinto et al. 2001</a>). The set of dimensional values
for an MD-Sequence is called an <strong>MD-Pattern</strong>. For a
multi-dimensional database, there is a fix set of dimensions. Each
dimensions can take a symbolic value or the value "*" which means any
value. In the following MD-Database, there is four MD-Sequences named
S1, S2, S3, S4. </p>
</blockquote>

<table align="center" border="1" width="573">

  <tbody>
    <tr>
      <td colspan="5">
      <div align="center"><strong>MD-Sequences</strong></div>
      </td>
    </tr>
    <tr>
      <td><strong>ID</strong></td>
      <td colspan="3"><strong>MD-Patterns</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td width="37"> <br>
      </td>
      <td width="36"><strong>d1</strong></td>
      <td width="29"><strong>d2</strong></td>
      <td width="40"><strong>d3</strong></td>
      <td width="397"> <br>
      </td>
    </tr>
    <tr>
      <td><strong>S1</strong></td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>(<strong>0</strong>, 2 4), (<strong>1</strong>, 3), (<strong>2</strong>,
2), (<strong>3</strong>, 1)</td>
    </tr>
    <tr>
      <td><strong>S2</strong></td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>(<strong>0</strong>, 2 6), (<strong>1</strong>, 3 5), (<strong>2</strong>,
6 7)</td>
    </tr>
    <tr>
      <td><strong>S3</strong></td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>(<strong>0</strong>, 1 8), (<strong>1</strong>, 1), (<strong>2</strong>,
2), (<strong>3</strong>, 6)</td>
    </tr>
    <tr>
      <td><strong>S4</strong></td>
      <td>*</td>
      <td>3</td>
      <td>3</td>
      <td>(<strong>0</strong>, 2 5), (<strong>1</strong>, 3 5)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>The task of multi-dimensional sequential pattern mining consists
of finding MD-Sequences that have a support higher than a minimum
support threshold <strong>minsup</strong> for a MD-Database.
Furthermore, in the <strong>Fournier-Viger algorithm</strong>, we
offers the possibility to mine <strong>only frequent closed
MD-Sequences,</strong> by implementing the idea of <a href="http://www.philippe-fournier-viger.com/spmf/songram06.pdf" rel="nofollow">Songram et al. 2006</a>. A
frequent closed MD-Sequence is a frequent MD-Sequence that is not
included in any other MD-Sequence having the same support.</p>
  <p>This algorithm has five parameters:</p>
</blockquote>

<ul>

  <li> <em>minsup</em> (a value in [0,1] representing a percentage), </li>
  <li><em>mininterval</em> (the minimum amount of time between two
consecutive itemsets in the patterns to be found) (an integer&gt;=0), </li>
  <li><em>maxinterval</em> (the maximum amount of time between two
consecutive itemsets in the patterns to be found) (an integer&gt;=1)</li>
  <li><em>minwholeinterval</em>(the minimum time duration of patterns
to be found) (an integer&gt;=0)</li>
  <li><em>maxwholeinterval </em>(the maximum time duration of the
patterns to be found) (an integer&gt;=1)</li>
</ul>

<p>What is the output?</p>

<blockquote>
  <p>The output is the set of <strong>frequent closed MD-Sequences</strong>
contained in the database (see the Fournier-Viger, 2008 paper for a
formal definition).</p>
  <p>For example, if we mine <strong>frequent closed MD-Sequences </strong>from
the previous database with a <em>minsup</em> of 50% <em>mininterval=0,
maxinterval=1000, minwholeinterval=0, maxwholeinterval=1000</em>, we
obtain the following result:</p>
</blockquote>

<table align="center" border="1" width="573">

  <tbody>
    <tr>
      <td colspan="5">
      <div align="center"><strong>Frequent Closed MD-Sequences</strong></div>
      </td>
    </tr>
    <tr>
      <td><strong>ID</strong></td>
      <td colspan="3"><strong>MD-Patterns</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td> <br>
      </td>
      <td><strong>d1</strong></td>
      <td><strong>d2</strong></td>
      <td><strong>d3</strong></td>
      <td> <br>
      </td>
    </tr>
    <tr>
      <td><strong>P1</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(<strong>0</strong>, 2)</td>
    </tr>
    <tr>
      <td><strong>P2</strong></td>
      <td>1</td>
      <td>*</td>
      <td>1</td>
      <td>(<strong>0</strong>, 1)</td>
    </tr>
    <tr>
      <td><strong>P3</strong></td>
      <td>1</td>
      <td>2</td>
      <td>*</td>
      <td>(<strong>0</strong>, 6)</td>
    </tr>
    <tr>
      <td><strong>P4</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 3 5)</td>
    </tr>
    <tr>
      <td><strong>P5</strong></td>
      <td>*</td>
      <td>*</td>
      <td>*</td>
      <td>(<strong>0</strong>, 2), (<strong>1</strong>, 3)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>If we mine <strong>frequent MD-sequences</strong> instead of <strong>frequent
closed MD-Sequences</strong>, we will obtain <strong>23 frequent
MD-Sequences</strong> instead.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a <strong>multi-dimensional
time-extended sequence </strong>from a <strong>multi-dimensional
time-extended sequence</strong> <strong>database</strong>. Each line
consists of two parts.</p>
  <ul>
    <li>The first part is a list of dimension values separated by
single spaces. A dimension value is a positive integer or the symbol
"*" meaning "any values". Finally, the value "-3" indicates the end of
the first part. Note that each line should have the same number of
dimension values.</li>
    <li>The second part is a list of itemsets, where each itemset has a
timestamp represented by a positive integer and each item is
represented by a positive integer. Each itemset is first represented by
its timestamp between the "&lt;" and "&gt; symbols. Then, the items of
the itemset appear separated by single spaces. Finally, the end of an
itemset is indicated by "-1". After all the itemsets, the end of a
sequence (line) is indicated by the symbol "-2". Note that it is
assumed that items are sorted according to a total order in each
itemset and that no item appears twice in the same itemset.</li>
  </ul>
  <p>For example, the input file "<strong>contextSequencesTimeExtended.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 1 1 -3 &lt;0&gt; 2 4 -1 &lt;1&gt; 3 -1 &lt;2&gt; 2 -1
&lt;3&gt; 1 -1 -2<br>
1 2 2 -3 &lt;0&gt; 2 6 -1 &lt;1&gt; 3 5 -1 &lt;2&gt; 6 7 -1 -2<br>
1 2 1 -3 &lt;0&gt; 1 8 -1 &lt;1&gt; 1 -1 &lt;2&gt; 2 -1 &lt;3&gt; 6 -1
-2<br>
* 3 3 -3 &lt;0&gt; 2 5 -1 &lt;1&gt; 3 5 -1 -2</p>
  </blockquote>
  <p>Consider the second line. It indicates that the second
multi-dimensional time-extended sequence of this database has the
dimension values 1, 2 and 2. Furthermore, the first itemset is {2, 4}
with a timestamp of 0. Then, the item 3 appears with a timestamp of 1.
Then the item 2 appears with a timestamp of 2. Finally, the item 1
appears with a timestamp of 3. The other sequence follows the same
format. Note that timestamps do not need to be consecutive integers.
But they should increase for each succesive itemset within a same
sequence. </p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a frequent (closed) MD sequential pattern.
Each line is separated into three parts: (1) a MD-pattern, (2) a
sequence and (3) the support of the MD sequential pattern formed by the
first and second part. </p>
  <ul>
    <li>The first part is a list of dimension values separated by
single spaces between the symbols "[" and "]". A dimension value is a
positive integer or the symbol "*" meaning "any values". Note that each
line have the same number of dimension values.</li>
    <li>The second part of each line is a sequence. A sequence is a
list of itemset. Each itemset is represented by {t=x, ...} where x is
an integer indicating that the itemset has the relative timestamp "x"
and where "..." is a list of items separated by single spaces. Each
item in an itemset is represented by a postive integers. Note that it
is assumed that items within a same itemset are sorted according to a
total order and that no item can appear twice in the same itemset. </li>
    <li>The third part is the keyword "#SUP:" followed by an integer
indicating the support of the pattern as a number of sequences.</li>
  </ul>
  <p>For example, here is the output file for this example: </p>
  <blockquote>
    <p>[ * * * ]{t=0, 2 }{t=1, 3 5 } #SUP: 2<br>
[ * * * ]{t=0, 2 }{t=1, 3 } #SUP: 3<br>
[ 1 * * ]{t=0, 2 }{t=1, 3 } #SUP: 2<br>
[ * * * ]{t=0, 2 } #SUP: 4<br>
[ 1 * * ]{t=0, 2 } #SUP: 3<br>
[ 1 * 1 ]{t=0, 1 } #SUP: 2<br>
[ 1 2 * ]{t=0, 6 } #SUP: 2</p>
  </blockquote>
  <p>Consider the first line. It presents a MD-sequential pattern
having the dimension values 1, 2 and *. Furthemore, the line indicates
that the sequence of this MD sequential pattern is the itemset {2}
followed by the itemset {6} and that the MD-Sequential-pattern has a
support of 2 transactions. The next lines follow the same format.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The algorithm is described in this paper:</p>
  <p><em> Fournier-Viger, P., Nkambou, R &amp; Mephu Nguifo, E. (2008),<a href="http://www.philippe-fournier-viger.com/fournier-viger_sequential_patterns_micai08.pdf">
A Knowledge Discovery Framework for Learning Task Models from User
Interactions in Intelligent Tutoring Systems</a>. Proceedings of the
7th Mexican International Conference on Artificial Intelligence (MICAI
2008). LNAI 5317, Springer, pp. 765-778.</em></p>
  <p>The idea of using time-constraints is based on this paper</p>
  <p><em>Hirate &amp; Yamana (2006) <a href="http://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf" rel="nofollow">Generalized Sequential Pattern Mining with Item
Intervals. JCP 1(3): 51-60</a>.</em></p>
  <p>The idea of multi-dimensional pattern mining is based on this
paper: </p>
  <p><em>H. Pinto, J. Han, J Pei, K. Wang, Q. Chen, U. Dayal: <a href="http://www.philippe-fournier-viger.com/spmf/pinto01.pdf" rel="nofollow">Multi-Dimensional Sequential Pattern Mining</a>. CIKM
2001: 81-88</em></p>
  <p>The idea of closed multi-dimensional pattern mining is based on
this paper: </p>
  <p><em>P. Songram, V. Boonjing, S. Intakosum: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/songram06.pdf">Closed
Multi-dimensional Sequential-Pattern Minin</a>.</em> Proc. of ITNG 2006.</p>
  <p>The idea of mining closed sequential pattern is based on this
paper:</p>
  <p><em>J. Wang, J. Han: <a href="http://www.philippe-fournier-viger.com/spmf/icde04_bide.pdf" rel="nofollow">BIDE: Efficient Mining of Frequent Closed Sequences</a>.
ICDE 2004: 79-90</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="cmrules" id="example25"> </a></span></strong>
Example 99 : Mining Sequential Rules Common to Several
Sequences with the CMRules algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">CMRules</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 75 %, <em>minconf</em>= 50% (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">CMRules </span></strong><strong>contextPrefixSpan</strong>.txt
output.txt 75% 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCMRules.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is CMRules?</p>

<blockquote>
  <p><strong>CMRules</strong> is an algorithm for discovering <strong>sequential
rules</strong> that appears in sequence databases. This algorithm was
proposed by Fournier-Viger et al. in 2010.</p>
</blockquote>

<p>What is the input of CMRules ?</p>

<blockquote>
  <p>The input of <strong>CMRules</strong> is a sequence database and
two user-specified thresholds named <em>minsup </em>(a value in [0,
1] representing a percentage) and <em>minconf</em> (a value in [0, 1]
representing a percentage).</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of CMRules ?</p>

<blockquote>
  <p>Given a sequence database, and parameters named <em>minsup</em>
and <em>minconf, </em><strong>CMRules</strong> outputs all <strong>sequential
rules</strong> having a support and confidence respectively higher than
  <em>minsup</em> and <em>minconf</em>.</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The support of a
rule X==&gt;Y is the number of sequences that contains XâªY divided by
the number of sequences in the database. The confidence of a rule is
the number of sequences that contains XâªY, divided by the number of
sequences that contains X. </p>
  <p>In this example, we apply <strong>CMRules</strong><strong> </strong>with
minsup = 75 %, minconf= 50%. We obtains <strong>9 sequential rules:</strong></p>
</blockquote>

<table align="center" border="1" width="315">

  <tbody>
    <tr>
      <td><strong>Rule</strong></td>
      <td><strong>Support</strong></td>
      <td><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td>1 ==&gt; 2</td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td height="25">1 ==&gt;3</td>
      <td>100 % </td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>2 ==&gt; 3</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>3 ==&gt; 2</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>4 ==&gt; 3</td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 3 ==&gt; 2 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 2 ==&gt; 3 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 4 ==&gt; 3 </td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 ==&gt; 2 3 </td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For example, the rule 1 4 ==&gt; 3 means that if 1 an 4 appears in
any order they will be followed by 3 with a confidence of 100 %.
Moreover, this rule has a support of 75 % because it appears in three
sequences (S1, S2 and S3) out of four sequences.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, a few lines from the output file from the previous
example is shown below:</p>
  <blockquote>
    <p> 1,6 ==&gt; 2,3 #SUP: 2 #CONF: 1.0<br>
1,5,6 ==&gt; 2 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,2,6 #SUP: 2 #CONF:0.66<br>
1,5,6 ==&gt; 3 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,3,6 #SUP: 2 #CONF:0.66<br>
    </p>
  </blockquote>
  <p>Consider the first line. It indicates that the rule {1, 6} ==&gt;
{2, 3} has a support of 2 sequences and a confidence of 100 %. The next
lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>CMRules is a relatively efficient algorihtm. However, the
RuleGrowth algorithm is faster. </p>
  <p>What is interesting about CMRules is that it uses an association
rule mining based approach for discovering sequential rules. Therefore
it could be used to discover both sequential rules and association
rules at the same time.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The CMRules algorithm is described in this paper:</p>
  <p><em>Fournier-Viger, P., Faghihi, U., Nkambou, R., Mephu Nguifo, E.
(2012). <a href="http://www.philippe-fournier-viger.com/CMRULES_journal_sequential_rules.pdf">CMRules:
Mining Sequential Rules Common to Several Sequences</a><strong>. </strong>Knowledge-based
Systems, Elsevier, 25(1): 63-76.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="cmdeo" id="example26"> </a></span></strong>
Example 100 : Mining Sequential Rules Common to Several
Sequences with the CMDeo algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">CMDeo</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 75 %, <em>minconf</em>= 50% (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> CMDeo</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 75% 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCMDeo.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is CMDeo ?</p>

<blockquote>
  <p><strong>CMDeo</strong> is an algorithm for discovering <strong>sequential
rules</strong> that appears in sequence databases. It was proposed by
Fournier-Viger in 2010.</p>
</blockquote>

<p>What is the input of CMDeo ?</p>

<blockquote>
  <p>The input of <strong>CMDeo</strong> is a sequence database and
two user-specified thresholds named <em>minsup </em>(a value in [0,
1] representing a percentage) and <em>minconf</em> (a value in [0, 1]
representing a percentage).</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of CMDeo ?</p>

<blockquote>
  <p>Given a sequence database, and parameters named <em>minsup</em>
and <em>minconf, </em><strong>CMDeo</strong> outputs all <strong>sequential
rules</strong> having a support and confidence respectively higher than
  <em>minsup</em> and <em>minconf</em>.</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The support of a
rule X==&gt;Y is the number of sequences that contains XâªY divided by
the number of sequences in the database. The confidence of a rule is
the number of sequences that contains XâªY, divided by the number of
sequences that contains X. </p>
  <p>In this example, we apply <strong>CMDeo</strong><strong> </strong>with
minsup = 75 %, minconf= 50%. We obtains <strong>9 sequential rules:</strong></p>
</blockquote>

<table align="center" border="1" width="315">

  <tbody>
    <tr>
      <td><strong>Rule</strong></td>
      <td><strong>Support</strong></td>
      <td><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td>1 ==&gt; 2</td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td height="25">1 ==&gt;3</td>
      <td>100 % </td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>2 ==&gt; 3</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>3 ==&gt; 2</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>4 ==&gt; 3</td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 3 ==&gt; 2 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 2 ==&gt; 3 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 4 ==&gt; 3 </td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 ==&gt; 2 3 </td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For example, the rule 1 4 ==&gt; 3 means that if 1 an 4 appears in
any order they will be followed by 3 with a confidence of 100 %.
Moreover, this rule has a support of 75 % because it appears in three
sequences (S1, S2 and S3) out of four sequences.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, a few lines from the output file from the previous
example is shown below:</p>
  <blockquote>
    <p> 1,6 ==&gt; 2,3 #SUP: 2 #CONF: 1.0<br>
1,5,6 ==&gt; 2 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,2,6 #SUP: 2 #CONF:0.66<br>
1,5,6 ==&gt; 3 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,3,6 #SUP: 2 #CONF:0.66<br>
    </p>
  </blockquote>
  <p>Consider the first line. It indicates that the rule {1, 6} ==&gt;
{2, 3} has a support of 2 sequences and a confidence of 100 %. The next
lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>CMDeo is a relatively efficient algorihtm. However, the RuleGrowth
algorithm is faster.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The CMDeo algorithm is described in this paper:</p>
  <p><em>Fournier-Viger, P., Faghihi, U., Nkambou, R., Mephu Nguifo, E.
(2012). <a href="http://www.philippe-fournier-viger.com/CMRULES_journal_sequential_rules.pdf">CMRules:
Mining Sequential Rules Common to Several Sequences</a><strong>. </strong>Knowledge-based
Systems, Elsevier, 25(1): 63-76.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="rulegrowth" id="example28">
</a></span></strong> Example 101 : Mining Sequential
Rules Common to Several Sequences with the RuleGrowth algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">RuleGrowth</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 75 %, <em>minconf</em>= 50% (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> RuleGrowth</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 75% 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestRuleGrowth.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is RuleGrowth?</p>

<blockquote>
  <p><strong>RuleGrowth</strong> is an algorithm for discovering <strong>sequential
rules</strong> that appears in sequence databases. It was proposed by
Fournier-Viger in 2011.</p>
</blockquote>

<p>What is the input of RuleGrowth ?</p>

<blockquote>
  <p>The input of <strong>RuleGrowth</strong> is a sequence database
and two user-specified thresholds named <em>minsup </em>(a value in
[0, 1] representing a percentage) and <em>minconf</em> (a value in [0,
1] representing a percentage).</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of RuleGrowth ?</p>

<blockquote>
  <p>Given a sequence database, and parameters named <em>minsup</em>
and <em>minconf, </em><strong>RuleGrowth</strong> outputs all <strong>sequential
rules</strong> having a support and confidence respectively higher than
  <em>minsup</em> and <em>minconf</em>.</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The support of a
rule X==&gt;Y is the number of sequences that contains XâªY divided by
the number of sequences in the database. The confidence of a rule is
the number of sequences that contains XâªY, divided by the number of
sequences that contains X. </p>
  <p>In this example, we apply <strong>RuleGrowth</strong><strong> </strong>with
minsup = 75 %, minconf= 50%. We obtains <strong>9 sequential rules:</strong></p>
</blockquote>

<table align="center" border="1" width="315">

  <tbody>
    <tr>
      <td><strong>Rule</strong></td>
      <td><strong>Support</strong></td>
      <td><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td>1 ==&gt; 2</td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td height="25">1 ==&gt;3</td>
      <td>100 % </td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>2 ==&gt; 3</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>3 ==&gt; 2</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>4 ==&gt; 3</td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 3 ==&gt; 2 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 2 ==&gt; 3 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 4 ==&gt; 3 </td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 ==&gt; 2 3 </td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For example, the rule 1 4 ==&gt; 3 means that if 1 an 4 appears in
any order they will be followed by 3 with a confidence of 100 %.
Moreover, this rule has a support of 75 % because it appears in three
sequences (S1, S2 and S3) out of four sequences.</p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The RuleGrowth implementation allows to specify some optional
parameters :</p>
  <ul>
    <li>"minimum antecedent length" allows to specify the maximum
number of items that can appear in the left side (antecedent) of a rule</li>
    <li>"maximum consequent length"allows to specify the maximum number
of items that can appear in the right side (consequent) of a rule</li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestRuleGrowth.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameters in the command line,
it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> RuleGrowth</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 75% 50% 2 3<br>
  </span>This command means to apply RuleGrowth on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 75 %, <em>minconf </em>= 50 % and rules found must contain
respectively a maximum of 2 items and 3 items in their antecedent and
consequent.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, a few lines from the output file from the previous
example is shown below:</p>
  <blockquote>
    <p> 1,6 ==&gt; 2,3 #SUP: 2 #CONF: 1.0<br>
1,5,6 ==&gt; 2 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,2,6 #SUP: 2 #CONF:0.66<br>
1,5,6 ==&gt; 3 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,3,6 #SUP: 2 #CONF:0.66<br>
    </p>
  </blockquote>
  <p>Consider the first line. It indicates that the rule {1, 6} ==&gt;
{2, 3} has a support of 2 sequences and a confidence of 100 %. The next
lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>RuleGrowth is a very efficient algorihtm. It is faster and more
memory efficient than CMDeo and CMRules. </p>
  <p>Note that there is a variation of RuleGrowth that accepts time
constraints. It is named <strong>TRuleGrowth</strong> and it is also
offered in SPMF. There is also a variations for mining top-k sequential
rules named <strong>TopSeqRules</strong> offered in SPMF.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The RuleGrowth algorithm is described in this paper:</p>
  <p><em> Fournier-Viger, P., Nkambou, R. &amp; Tseng, V. S. (2011). <a href="http://www.philippe-fournier-viger.com/spmf/rulegrowth.pdf">RuleGrowth:
Mining Sequential Rules Common to Several Sequences by Pattern-Growth</a>.
Proceedings of the 26th Symposium on Applied Computing (ACM SAC 2011).
ACM Press, pp. 954-959.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="erminer" id="example44"> </a></span></strong>
Example 102 : Mining Sequential Rules Common to Several
Sequences with the ERMiner algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">ERMiner</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 75 %, <em>minconf</em>= 50% (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> ERMiner</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 75% 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestERMiner.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is ERminer?</p>

<blockquote>
  <p><strong>ERMiner</strong> is an algorithm for discovering <strong>sequential
rules</strong> that appears in sequence databases. It was proposed by
Fournier-Viger in 2014. It is a variation of the RuleGrowth algorithm
that uses equivalence classes to discover rules. It can be up to 5
times faster than RuleGrowth. However, it generally consumes more
memory, so there is a trade-off.</p>
</blockquote>

<p>What is the input of ERMiner ?</p>

<blockquote>
  <p>The input of <strong>ERMiner</strong> is a sequence database and
two user-specified thresholds named <em>minsup </em>(a value in [0,
1] representing a percentage) and <em>minconf</em> (a value in [0, 1]
representing a percentage).</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of ERMiner ?</p>

<blockquote>
  <p>Given a sequence database, and parameters named <em>minsup</em>
and <em>minconf, </em><strong>ERMiner</strong> outputs all <strong>sequential
rules</strong> having a support and confidence respectively higher than
  <em>minsup</em> and <em>minconf</em>.</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The support of a
rule X==&gt;Y is the number of sequences that contains XâªY divided by
the number of sequences in the database. The confidence of a rule is
the number of sequences that contains XâªY, divided by the number of
sequences that contains X. </p>
  <p>In this example, we apply <strong>ERMiner</strong><strong> </strong>with
minsup = 75 %, minconf= 50%. We obtains <strong>9 sequential rules:</strong></p>
</blockquote>

<table align="center" border="1" width="315">

  <tbody>
    <tr>
      <td><strong>Rule</strong></td>
      <td><strong>Support</strong></td>
      <td><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td>1 ==&gt; 2</td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td height="25">1 ==&gt;3</td>
      <td>100 % </td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>2 ==&gt; 3</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>3 ==&gt; 2</td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>4 ==&gt; 3</td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 3 ==&gt; 2 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 2 ==&gt; 3 </td>
      <td>75 %</td>
      <td>75 %</td>
    </tr>
    <tr>
      <td>1 4 ==&gt; 3 </td>
      <td>75 %</td>
      <td>100 %</td>
    </tr>
    <tr>
      <td>1 ==&gt; 2 3 </td>
      <td>100 %</td>
      <td>100 %</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>For example, the rule 1 4 ==&gt; 3 means that if 1 an 4 appears in
any order they will be followed by 3 with a confidence of 100 %.
Moreover, this rule has a support of 75 % because it appears in three
sequences (S1, S2 and S3) out of four sequences.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, a few lines from the output file from the previous
example is shown below:</p>
  <blockquote>
    <p> 1,6 ==&gt; 2,3 #SUP: 2 #CONF: 1.0<br>
1,5,6 ==&gt; 2 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,2,6 #SUP: 2 #CONF:0.66<br>
1,5,6 ==&gt; 3 #SUP: 2 #CONF: 1.0<br>
5 ==&gt; 1,3,6 #SUP: 2 #CONF:0.66<br>
    </p>
  </blockquote>
  <p>Consider the first line. It indicates that the rule {1, 6} ==&gt;
{2, 3} has a support of 2 sequences and a confidence of 100 %. The next
lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>ERMiner is a very efficient algorihtm. It is faster than CMDeo and
CMRules. </p>
  <p>Moreover, ERMiner is also generally faster than RuleGrowth (up to
5 times faster than RuleGrowth). However, ERMiner generally consumes
more memory than RuleGrowth, so there is a trade-off.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The ERMiner algorithm is described in this paper:</p>
  <p><em> Fournier-Viger, P., Gueniche, T., Zida, S., Tseng, V. S.
(2014). <a href="http://www.philippe-fournier-viger.com/ERMiner_2014_sequential_rule_mining.pdf">ERMiner:
Sequential Rule Mining using Equivalence Classes</a>. Proc. 13th
Intern. Symposium on Intelligent Data Analysis (IDA 2014), Springer,
LNCS 8819, pp. 108-119</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="rulegen" id="example9"> </a></span></strong>
Example 103 : Mining Sequential Rules between Sequential
Patterns with the RuleGen algorithm</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">RuleGen</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup</em> = 3, <em>minconf</em>= 50% (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> RuleGen</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 3 50%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestRuleGen.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is RuleGen?</p>

<blockquote>
  <p><strong>RuleGen</strong> is an algorithm for discovering <strong>sequential
rules</strong> that appears in sequence databases. It was proposed by
Zaki (2000).</p>
</blockquote>

<p>What is the input of RuleGen ?</p>

<blockquote>
  <p>The input of <strong>RuleGen</strong> is a sequence database and
two user-specified thresholds named <em>minsup </em>(a value in [0,
1] representing a percentage) and <em>minconf</em> (a value in [0, 1]
representing a percentage).</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of RuleGen ?</p>

<blockquote>
  <p>The RuleGen algorithms outputs all sequential rules having a
support and confidence respectively higher or equals to user-specified <em>minsup</em>
and <em>minconf</em> thresholds.</p>
  <p>A rule X==&gt;Y is defined by RuleGen as a sequential relationship
between two sequential patterns X and Y. The <strong>confidence</strong>
of a rule X ==&gt; Y is defined as the number of sequences containing X
divided by the number of sequences containing Y. The <strong>support</strong>
of a rule X ==&gt; is defined as the number of sequences containing Y.</p>
  <p>In this example, we apply <strong>RuleGen </strong> with minsup
= 75 %, minconf= 50%. We obtains <strong>21</strong> <strong>sequential
rules:</strong></p>
</blockquote>

<table align="center" border="1" cellpadding="0" cellspacing="0">

  <tbody>
    <tr>
      <td><strong>Rule</strong></td>
      <td><strong>Support</strong></td>
      <td><strong>Confidence</strong></td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 } ==&gt; {1 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>4</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 } ==&gt; {1 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>4</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 } ==&gt; {1 }{3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 } ==&gt; {1 }{3 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{2 } ==&gt; {1 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>4</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{2 } ==&gt; {2 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{2 } ==&gt; {3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{2 } ==&gt; {1 }{3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {1 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>4</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {2 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {3 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {4 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {1 }{3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 } ==&gt; {1 }{3 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{4 } ==&gt; {4 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 }{2 } ==&gt; {1 }{3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 }{3 } ==&gt; {1 }{3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{1 }{3 } ==&gt; {1 }{3 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>0.75</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 }{2 } ==&gt; {1 }{3 }{2 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
    <tr>
      <td valign="top" width="288">
      <p>{3 }{3 } ==&gt; {1 }{3 }{3 } </p>
      </td>
      <td valign="top" width="138">
      <p>3</p>
      </td>
      <td valign="top" width="213">
      <p>1.0</p>
      </td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p> For example, the rule {1 } ==&gt; {1 }{3 }{3 } means that
if the sequential pattern {1} appears in a sequence, the
sequential pattern {1} {3 }{3 } also appear in this sequence. In other
words, it means that if {1} appears, it will be followed by {3}, {3}.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule and consists of three
parts:</p>
  <ul>
    <li>First, the rule antecedent is presented. It is a sequential
pattern, which is a list of itemsets. Each itemset is a list of items
separated by single spaces between the symbols "{" and "}". An item is
a positive integer.</li>
    <li>Second, the keyword "==&gt;" appears followed by the rule
consequent, which is a list of itemsets. Again, each itemset is a list
of items separated by single spaces between the symbols "{" and "}". An
item is a positive integer.</li>
    <li>Third, the keyword "sup=" appears followed by a positive
integer indicating the support of the sequential rule as a number of
sequences. Then, the keyword "conf=" appears followed by a double value
indicating the confidence of the sequential rule as a value in the [0,
1] interval.</li>
  </ul>
  <p> For example, the output file from the previous example is shown
below:</p>
  <blockquote>
    <p>{1 } ==&gt; {1 }{3 } sup= 4 conf= 1.0<br>
{1 } ==&gt; {1 }{2 } sup= 4 conf= 1.0<br>
{1 } ==&gt; {1 }{3 }{3 } sup= 3 conf= 0.75<br>
{1 } ==&gt; {1 }{3 }{2 } sup= 3 conf= 0.75<br>
{2 } ==&gt; {1 }{2 } sup= 4 conf= 1.0<br>
{2 } ==&gt; {2 }{3 } sup= 3 conf= 0.75<br>
{2 } ==&gt; {3 }{2 } sup= 3 conf= 0.75<br>
{2 } ==&gt; {1 }{3 }{2 } sup= 3 conf= 0.75<br>
{3 } ==&gt; {1 }{3 } sup= 4 conf= 1.0<br>
{3 } ==&gt; {2 }{3 } sup= 3 conf= 0.75<br>
{3 } ==&gt; {3 }{3 } sup= 3 conf= 0.75<br>
{3 } ==&gt; {3 }{2 } sup= 3 conf= 0.75<br>
{3 } ==&gt; {4 }{3 } sup= 3 conf= 0.75<br>
{3 } ==&gt; {1 }{3 }{3 } sup= 3 conf= 0.75<br>
{3 } ==&gt; {1 }{3 }{2 } sup= 3 conf= 0.75<br>
{4 } ==&gt; {4 }{3 } sup= 3 conf= 1.0<br>
{1 }{3 } ==&gt; {1 }{3 }{3 } sup= 3 conf= 0.75<br>
{1 }{3 } ==&gt; {1 }{3 }{2 } sup= 3 conf= 0.75<br>
{1 }{2 } ==&gt; {1 }{3 }{2 } sup= 3 conf= 0.75<br>
{3 }{3 } ==&gt; {1 }{3 }{3 } sup= 3 conf= 1.0<br>
{3 }{2 } ==&gt; {1 }{3 }{2 } sup= 3 conf= 1.0</p>
  </blockquote>
  <p>Consider the last line. It represents a rule where the antecedent
is the itemset {3} followed by the itemset {2}, and the consequent is
the itemset {1} followed by {3}, followed by {2}. The rule has a
support of 3 sequences and a confidence of 100%. The other lines of the
output file follow the same format.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p>The RuleGen algorithm first apply a sequential pattern mining
algorithm and then combines pairs of sequential patterns to generate
rules between two sequential patterns. Note that in our implementation
we use the PrefixSpan algorithm for mining sequential patterns instead
of SPADE because the PrefixSpan algorithm is generally faster than
SPADE.</p>
  <p>Also, it is important to note that rules found by <strong>RuleGen</strong>
always have the form X == &gt; Y such that X is a subsequence of Y.
This definition of a sequential rule is different from the definition
of a sequential rules used by other sequential rule mining algorithms
offered in SPMF such as <strong>CMRules, CMDeo, RuleGrowth,
TRuleGrowth, TopSeqRules</strong> and <strong>TNS</strong> where X and
Y are unordered itemsets, and X is not a subset of Y. The rules found
by these latter algorithms are more general. Moreover, we have shown
that we can achieve higher prediction accuracy by using the kind of
rules found by RuleGrowth, CMRules instead of using the rules generated
by RuleGen. (see this <a href="http://www.philippe-fournier-viger.com/sequential_rules_prediction_2012.pdf">article</a>
for details).</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The RuleGen algorithm is described in this paper:</p>
  <p><em>Mohammed Javeed Zaki: <a rel="nofollow" href="http://www.philippe-fournier-viger.com/spmf/zaki2000.pdf">Scalable
Algorithms for Association Mining</a>. IEEE Trans. Knowl. Data Eng.
12(3): 372-390 (2000)</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="trulegrowth" id="example35">
</a></span></strong> Example 104 : Mining Sequential
Rules Common to Several Sequences with the Window Size Constraint</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">TRuleGrowth</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>minsup = </em>0.7, minconf =0.8 and window_size = 3 (5)
click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TRuleGrowth</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 70% 80% 3</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTRuleGrowth.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TRuleGrowth?</p>

<blockquote>
  <p><strong>TRuleGrowth</strong> is an algorithm for discovering <strong>sequential
rules</strong> that appears in sequence databases. It was proposed by
Fournier-Viger in 2012. It is a variation of the RuleGrowth algorithm
that accepts a window size constraint.</p>
</blockquote>

<p>What is the input of TRuleGrowth ?</p>

<blockquote>
  <p>The input of <strong>TRuleGrowth</strong>is a sequence database,
two user-specified thresholds named <em>minsup </em>(a value in [0,
1] representing a percentage) and <em>minconf</em> (a value in [0, 1]
representing a percentage) and a parameter named <em>window_size</em>
(an integer &gt;=0). </p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of TRuleGrowth ?</p>

<blockquote>
  <p>Given a sequence database, and parameters named <em>minsup,
minconf </em>and <em>window_size</em>, <strong>TRuleGrowth</strong>
outputs all <strong>sequential rules</strong> having a support and
confidence respectively higher than <em>minsup</em> and <em>minconf</em>
that appears within <em>window_size </em>consecutive itemsets</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The support of a
rule X==&gt;Y is the number of sequences that contains XâªY divided by
the number of sequences in the database. The confidence of a rule is
the number of sequences that contains XâªY, divided by the number of
sequences that contains X. </p>
  <p>For example, if we set <em>minsup = </em>0.7, minconf =0.8 and
window_size = 3, TRuleGrowth discovers 4 rules:</p>
  <table border="1" cellpadding="0" cellspacing="0">
    <tbody>
      <tr>
        <td><strong>Rule</strong></td>
        <td><strong>Support</strong></td>
        <td><strong>Confidence</strong></td>
      </tr>
      <tr>
        <td valign="top" width="213">
        <p>{1 } ==&gt; {2 } </p>
        </td>
        <td valign="top" width="213">
        <p>80 % (4 sequences)</p>
        </td>
        <td valign="top" width="213">
        <p>100 %</p>
        </td>
      </tr>
      <tr>
        <td valign="top" width="213">
        <p>{1 } ==&gt; {2, 3
} </p>
        </td>
        <td valign="top" width="213">
        <p>80 % (4 sequences)</p>
        </td>
        <td valign="top" width="213">
        <p>100 %</p>
        </td>
      </tr>
      <tr>
        <td valign="top">{1 } ==&gt; {3
} </td>
        <td valign="top">80 % (4 sequences)</td>
        <td valign="top">100 %</td>
      </tr>
      <tr>
        <td valign="top" width="213">
        <p>{4 } ==&gt; {3 } </p>
        </td>
        <td valign="top" width="213">
        <p>60 % (3 sequences)</p>
        </td>
        <td valign="top" width="213">
        <p>100 %</p>
        </td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>For example, the rule {1} ==&gt; {2 3} means that if 1 appears in
a sequence, it will be followed by 2 and (in any order) with a
confidence of 100 %. Moreover, this rule has a support of 100 % because
it appears in four sequences (S1, S2, S3 and S4) out of four sequences
within <em>window_size</em> consecutive itemsets.</p>
</blockquote>

<p>Optional parameters</p>

<blockquote>
  <p>The RuleGrowth implementation allows to specify some optional
parameters :</p>
  <ul>
    <li>"minimum antecedent length" allows to specify the maximum
number of items that can appear in the left side (antecedent) of a rule</li>
    <li>"maximum consequent length"allows to specify the maximum number
of items that can appear in the right side (consequent) of a rule</li>
  </ul>
  <p>These parameters are available in the GUI of SPMF and also in the
example <strong><span class="Style2">"MainTestTRuleGrowth.java"</span></strong>
provided in the source code of SPMF.</p>
  <p>The parameter(s) can be also used in the command line with the Jar
file. If you want to use these optional parameters in the command line,
it can be done as follows. Consider this example:<br>
  <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TRuleGrowth</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 75% 50% 3 2 3<br>
  </span>This command means to apply RuleGrowth on the file
"contextPrefixSpan.txt" and output the results to "output.txt".
Moreover, it specifies that the user wants to find patterns for <em>minsup</em>
= 75 %, <em>minconf </em>= 50 %, a window size of 3 itemsets, and
rules found must contain respectively a maximum of 2 items and 3 items
in their antecedent and consequent.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, the output file from the previous example is shown
below:</p>
  <blockquote>
    <p>1 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
1 ==&gt; 2,3 #SUP: 4 #CONF: 1.0<br>
1 ==&gt; 3 #SUP: 4 #CONF: 1.0<br>
4 ==&gt; 3 #SUP: 3 #CONF: 1.0</p>
  </blockquote>
  <p>Consider the second line. It indicates that the rule {1} ==&gt;
{2, 3} has a support of 4 sequences and a confidence of 100 %. The
other lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>TRuleGrowth is a very efficient algorihtm. It is faster and more
memory efficient than CMDeo and CMRules. If the windows_size constraint
is used, it can also be much faster than RuleGrowth depending on how
the window_size constraint is set.</p>
</blockquote>

<p>Implementation details</p>

<blockquote>
  <p> In SPMF, there is also a version of TRuleGrowth that accepts
strings instead of integers. It is available under the name "<strong class="Style2">TRuleGrowth with strings</strong>" in the release
version of SPMF or in the package
ca.pfv.spmf.sequential_rules.trulegrowth_with_strings for the source
code version of SPMF. To run it, you should use the input file: <strong>contextPrefixSpanStrings.txt</strong>.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TRuleGrowth algorithm is described in this paper:</p>
  <p><em>Fournier-Viger, P., Wu, C.-W., Tseng, V.S., Nkambou, R.
(2012). <a href="http://www.philippe-fournier-viger.com/spmf/time_sequential_rules12.pdf">Mining
Sequential Rules Common to Several Sequences with the Window Size
Constraint</a>. Proceedings of the 25th Canadian Conf. on
Artificial Intelligence (AI 2012), Springer, LNAI 7310, pp.299-304</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="topseqrules" id="example34">
</a></span></strong> Example 105 : Mining the Top-K
Sequential Rules</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong> "<span class="Style9">TopSeqRules</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>k = 3 </em> and <em>minconf = 0.8</em>. (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TopSeqRules</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 3 80%</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTopSeqRules.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TopSeqRules?</p>

<blockquote>
  <p><strong>TopSeqRules</strong> is an algorithm for discovering the <strong>top-<em>k
  </em></strong> <strong>sequential rules</strong> appearing in a
sequence database. </p>
  <p>Why is it important to discover top-k sequential rules? Because
other sequential rule mining algorithms requires the user to set a
minimum support (<em>minsup</em>) parameter that is hard to set
(usually users set it by trial and error, which is time consuming). The
TopSeqRules algorithm solve this problem by letting users directly
indicate <em>k</em>, the number of rules to be discovered. </p>
</blockquote>

<p>What is the input of TopSeqRules ?</p>

<blockquote>
  <p><strong>TopSeqRules </strong> takes three parameters as input:</p>
  <ul>
    <li>a sequence database,</li>
    <li>a parameter <em>k </em>representing the number of rules to be
discovered (an integer &gt;=1),</li>
    <li>a parameter <em>minconf</em> representing the minimum
confidence that the rules should have (a value in [0,1] representing a
percentage). </li>
  </ul>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of TopSeqRules ?</p>

<blockquote>
  <p><strong>TopSeqRules</strong> outputs the <em>k </em>most
frequent <strong>sequential rules</strong> having a confidence higher
or equal to <em>minconf</em>.</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The<strong>
support of a sequential rule</strong> X==&gt;Y is the number of
sequences that contains XâªY divided by the number of sequences in the
database. The <strong>confidence of a sequential rule</strong> is the
number of sequences that contains XâªY, divided by the number of
sequences that contains X. </p>
  <p>For example, if we run TopSeqRules with <em>k = 3 </em> and <em>minconf
= 0.8</em>, the result is the following rules:</p>
  <table border="1" cellpadding="0" cellspacing="0">
    <tbody>
      <tr>
        <td><strong>Rule</strong></td>
        <td><strong>Support</strong></td>
        <td><strong>Confidence</strong></td>
      </tr>
      <tr>
        <td valign="top" width="213">
        <p>{3 } ==&gt; {4 } </p>
        </td>
        <td valign="top" width="213">
        <p>75 % (3 sequences)</p>
        </td>
        <td valign="top" width="213">
        <p>100 %</p>
        </td>
      </tr>
      <tr>
        <td valign="top" width="213">
        <p>{1 } ==&gt; {3 } </p>
        </td>
        <td valign="top" width="213">
        <p>100 % (4 sequences)</p>
        </td>
        <td valign="top" width="213">
        <p>100 %</p>
        </td>
      </tr>
      <tr>
        <td valign="top">{1,4 } ==&gt; {3
} </td>
        <td valign="top">75 % (3 sequences)</td>
        <td valign="top">100 %</td>
      </tr>
    </tbody>
  </table>
  <p>These rules are the top three rules appearing in the sequence
database having a confidence higher or equals to 80 %.</p>
  <p>For example, the rule 1 4 ==&gt; 3 means that if 1 an 4 appears in
any order they will be followed by 3 with a confidence of 100 %.
Moreover, this rule has a support 75 % because it appears in three
sequences (S1, S2 and S3) out of four sequences.</p>
  <p>It is important to note that for some values of <em>k</em>, the
algorithm may return slightly more rules than <em>k</em>. This can
happen if several rules have exactly the same support, and it is normal.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, an output file is shown below:</p>
  <blockquote>
    <p>1 ==&gt; 2 #SUP: 4 #CONF: 1.0<br>
1 ==&gt; 2,3 #SUP: 4 #CONF: 1.0<br>
1 ==&gt; 3 #SUP: 4 #CONF: 1.0</p>
  </blockquote>
  <p>Consider the second line. It indicates that the rule {1} ==&gt;
{2, 3} has a support of 4 sequences and a confidence of 100 %. The
other lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>TopSeqRules</strong> is a very efficient algorihtm. It is
based on the RuleGrowth algorithm, which is one of the most efficient
algorithm for mining <strong>sequential rules</strong>. </p>
  <p>It is more intuitive to use TopSeqRules than RuleGrowth. However,
it should be note that the problem of <strong>top-k sequential rule
mining</strong> is more computationally expensive than the problem of
sequential rule mining. Therefore, it is recommended to use TopSeqRules
for <em>k </em>values of up to 1000 or 2000 depending on the dataset.
If more rules should be found, it could be better to use RuleGrowth or
TRuleGrowth.</p>
  <p>Besides, note that there is a variation of TopSeqRules named TNS
that is available in SPMF. The improvement in TNS is that it eliminate
some sequential rules that are deemed "redundant" (rules that are
included in other rules having the same support and confidence - see
the TNS example for the formal definition). Using TNS is more costly
than using TopSeqRules but it brings the benefit of eliminating some
redundancy in the results.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TopSeqRules algorithm is described in this paper:</p>
  <p><em>Fournier-Viger, P. &amp; Tseng, V. S. (2011). <a href="http://www.philippe-fournier-viger.com/spmf/TopSeqRules_sequential_rules_2.pdf">Mining
Top-K Sequential Rules</a>. Proceedings of the 7th Intern. Conf.
on Advanced Data Mining and Applications (ADMA 2011). LNAI 7121,
Springer, pp.180-194.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="topseqrules" id="tns"> </a></span></strong>
Example 106 : Mining the Top-K Non-Redundant Sequential
Rules</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">TNS</span>"</strong> algorithm<strong>,
    </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixSpan</span><span class="Style2">.txt"</span></strong>, (3) set the output file name
(e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (4)
set <em>k = 30,minconf = 0.5, </em>and<em> delta = 2</em> (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TNS</span></strong> <strong>contextPrefixSpan</strong>.txt
output.txt 30 50% 2</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2">contextPrefixSpan.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTNS.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is TNS?</p>

<blockquote>
  <p><strong>TNS</strong> is an algorithm for discovering the <strong>top-<em>k</em></strong><em>
  </em> <strong>non-redundant sequential rules</strong> appearing in a
sequence database. It is an approximate algorithm in the sense that it
always generates non-redundant rules. But these may not always be the
top-k non-redundant rules. TNS uses a parameter named delta, which is a
positive integer that can be used to improve the chance that the result
is exact (the higher delta value, the more chances that the result will
be exact).</p>
  <p>Why is it important to discover <strong>top-k non-redundant
sequential rules</strong>? Because other sequential rule mining
algorithms requires that the user set a minimum support (<em>minsup</em>)
parameter that is hard to set (usually users set it by trial and error,
which is time consuming). Moreover, the result of sequential rule
mining algorithms usually contains a high level of redundancy (for
example, thousands of rules can be found that are variation of other
rules having the same support and confidence). The <strong>TNS</strong>
algorithm provide solution to both of these problems by letting users
directly indicate <em>k</em>, the number of rules to be discovered,
and by eliminating redundancy in results. </p>
</blockquote>

<p>What is the input of TNS ?</p>

<blockquote>
  <p><strong>TNS </strong> takes four parameters as input:</p>
  <ul>
    <li>a sequence database,</li>
    <li>a parameter <em>k </em>representing the number of rules to be
discovered (an integer &gt;= 1),</li>
    <li>a parameter <em>minconf</em> representing the minimum
confidence that rules should have (a value in [0,1] representing a
percentage),</li>
    <li>a parameter <em>delta</em> (an integer &gt;=0) that is used to
increase the chances of having an exact result (because this algorihm
is approximate).</li>
  </ul>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong>contextPrefixSpan.txt</strong>" of the
SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output of TNS ?</p>

<blockquote>
  <p><strong>TNS</strong> outputs an approximation of the <em>k </em>most
frequent <strong>non redundant sequential rules</strong> having a
confidence higher or equal to <em>minconf</em>.</p>
  <p>A <strong>sequential rule</strong> X==&gt;Y is a sequential
relationship between two sets of items X and Y such that X and Y are
disjoint, and that X is unordered and Y is unordered. The<strong>
support of a sequential rule</strong> X==&gt;Y is denoted as <em>sup</em>(X==&gt;Y)
and defined as the number of sequences that contains XâªY divided by the
number of sequences in the database. The <strong>confidence of a
sequential rule</strong> X==&gt;Y is denoted as <em>conf</em>(X==&gt;Y)
and defined as the number of sequences that contains XâªY, divided by
the number of sequences that contains X. </p>
  <p>A <strong>sequential rule</strong> <em>ra</em>: X â Y <strong>is
redundant </strong>with respect to another rule <em>rb</em> : X1 â Y1
if and only if:</p>
  <ul>
    <li><em>conf</em>(<em>ra</em>) = <em>conf</em>(<em>rb</em>) </li>
    <li><em>sup</em>(<em>ra</em>) = <em>sup</em>(<em>rb</em>)</li>
    <li>X1 â X â§ Y â Y1. </li>
  </ul>
  <p>For example, If we run <strong>TNS</strong> with <em>k = 10 </em>
and <em>minconf = 0.5</em> and <em>delta = 2</em>, the following set
of non-redundant rules is found</p>
  <pre>2 ==&gt; 3  sup= 3  conf= 0.75<br>1,3 ==&gt; 2  sup= 3  conf= 0.75<br>1,4 ==&gt; 3  sup= 3  conf= 1.0<br>1 ==&gt; 2,3  sup= 4  conf= 1.0<br>3 ==&gt; 4  sup= 3  conf= 1.0<br>2,5 ==&gt; 6  sup= 2  conf= 1.0<br>2,3 ==&gt; 4  sup= 2  conf=0.66<br>1 ==&gt; 2,3,4,6  sup= 2  conf= 0.5<br>3,5 ==&gt; 6  sup= 2  conf= 1.0<br>2 ==&gt; 3,4,6  sup= 2  conf= 0.5</pre>
  <p>For instance, the rule 1 4 ==&gt; 3 means that if 1 an 4 appears
in any order they will be followed by 3 with a confidence of 100 %.
Moreover, this rule has a support 75 % (<em>sup = 3</em>) because it
appears in three sequences (S1, S2 and S3) out of four sequences.</p>
  <p>Note that for some values of <em>k </em>and some datasets, TNS
may return more than <em>k</em> rules. This can happen if several
rules have exactly the same support, and it is normal. It is also
possible that the algorithm returns slightly less than <em>k</em>
rules in some circonstances because the algorithm is approximate.</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a postive integer and items from
the same itemset within a sequence are separated by single spaces. Note
that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the sample input file "<strong>contextPrefixSpan.txt</strong>"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file. Each line is a sequential rule. Each item from a
sequential rule is a postive integer. On each line, the items from the
rule antecedent are first listed, separated by single spaces. Then the
keyword "==&gt;" appears, followed by the items from the rule
consequent, separated by single spaces. Then, the keyword "#SUP:"
appears followed by an integer indicating the support of the rule as a
number of sequences. Then, the keyword "#CONF:" appears followed by a
double values in the [0, 1] interval indicating the confidence of the
rule. For example, an output file is shown below:</p>
  <blockquote>
    <p>3 ==&gt; 2 #SUP: 3 #CONF: 0.75<br>
1 ==&gt; 2,3 #SUP: 4 #CONF: 1.0<br>
4 ==&gt; 3 #SUP: 3 #CONF: 1.0</p>
  </blockquote>
  <p>Consider the second line. It indicates that the rule {1} ==&gt;
{2, 3} has a support of 4 sequences and a confidence of 100 %. The
other lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p><strong>TNS</strong> is an efficient algorihtm. It is based on the
TopSeqRules algorithm for discovering<strong> top-k sequential rules</strong>.
The main difference between TNS and TopSeqRules is that TNS includes
additional strategies to eliminate redundancy in results, and that TNS
is an approximate algorithm, while TopSeqRules is not.</p>
  <p>TNS and TopSeqRules are more intuitive to use than regular
sequential rule mining algorithms such as RuleGrowth. However, it
should be note that the problem of <strong>top-k sequential rule mining</strong>
is more computationally expensive than the problem of sequential rule
mining. Therefore, it is recommended to use TNS or TopSeqRules for <em>k
  </em>values of up to 1000 or 2000 depending on the dataset. If more
rules should be found, it could be better to use RuleGrowth or
TRuleGrowth, for more efficiency.</p>
</blockquote>

<p>Where can I get more information about this algorithm?</p>

<blockquote>
  <p>The TNS algorithm is described in this paper:</p>
  <p><em>Fournier-Viger, P., Tseng, V. S. (2013).<a href="http://www.philippe-fournier-viger.com/spmf/TNS_Mining_top-k_non_redundant%20sequential_rules_6pages.pdf">
TNS: Mining Top-K Non-Redundant Sequential Rules</a>. Proc. 28th
Symposium on Applied Computing (ACM SAC 2013). ACM Press, pp. 164-166.</em></p>
</blockquote>

<h3><strong><span class="centered"><a name="cptPlus" id="cptPlus"> </a></span></strong>
Example 107 : Perform Sequence Prediction using the CPT+
Sequence Prediction Model</h3>

<p>How to run this example?</p>

<blockquote>
  <p>To run the implementation of <strong>CPT+</strong></p>
  <ul>
    <li><strong>This Example </strong> is not available in the graphical
user interface of SPMF.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCPTPlus.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
  </ul>
</blockquote>

<p>What is CPT+ (Compact Prediction Tree+)?</p>

<blockquote>
  <p><strong>CPT+</strong> (Compact Prediction Tree+) is a sequence
prediction model. It is used for performing sequence predictions. A
sequence prediction consists of predicting the next symbol of a
sequence based on a set of training sequences. The task of sequence
prediction has numerous applications in various domains. For example,
it can be used to predict the next webpage that a user will visit based
on previously visited webpages by the user and other users. </p>
  <p>The <strong>CPT+ prediction model </strong>(2015) is an improved
version of the <strong>CPT</strong> model (Gueniche et al., 2013).
CPT+ was shown to provide better accuracy than several state-of-the-art
prediction models such as DG, AKOM, TDAG, PPM and CPT on various
datasets (see Gueniche et al., 2015 for details).</p>
  <p>The implementation of CPT+ in SPMF is the original implementation
(obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict
project</a>).</p>
</blockquote>

<p>What is the input of CPT+?</p>

<blockquote>
  <p>The input of <strong>CPT+</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
  <p>In the context of <strong>CPT+</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (2), (3), (4), (6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5), (1), (4), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1), (4), (2), (3)</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>CPT+ </strong>also takes as set of parameters as input.</p>
</blockquote>

<p>What is the output of <strong>CPT+</strong>?</p>

<blockquote>
  <p><strong>CPT+ </strong> performs sequence prediction. After <strong>CPT+
  </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
  <p>For example, if <strong>CPT+</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(2) is the symbol (3).</p>
</blockquote>

<p>Parameter(s)</p>

<blockquote>
  <p>There are several parameters that can be set for CPT+. In the
source code, these parameters are passed as a string to the class
CPTPlusPredictor. The main parameters are:</p>
</blockquote>

<ul>

  <li id="yui_3_16_0_1_1441197496937_4140"><strong>CCF:true :</strong>
if true, the Frequent Subsequence Compression (FSC) strategy will be
activated. Otherwise, if false, it is deactivated. This strategy can
slows down the processing time but it can be used to reduce the memory
usage of CPT+ </li>
  <li><strong>CBS:true :</strong> if true, the Simple Branches
Compression (SBC) strategy will be activated. Otherwise, if false, it
is deactivated. This strategy can slows down the processing time but it
can be used to reduce the memory usage of CPT+</li>
  <li><strong>CCFmin:1 : </strong>this parameter allows to specify the
minimum subsequence length for the FSC strategy. It takes a positive
integer as value such as 1 in this example. This parameter is only
needed when the FSC strategy is activated</li>
  <li><strong>CCFmax:6 : </strong>this parameter allows to specify the
maximum subsequence length for the FSC strategy. It takes a positive
integer as value such as 6 in this example. This parameter is only
needed when the FSC strategy is activated.</li>
  <li><strong>CCFsup:2 : </strong>this parameter allows to specify the
minimum support (minsup) for the FSC strategy. It takes a positive
integer as value such as 2 in this example. This parameter is only
needed when the FSC strategy is activated.</li>
  <li><strong>splitMethod:0:</strong> This parameter indicates whether
the training sequences should be kept entirely or if they should be cut
to reduce memory usage when training the model. If this parameter is
set to 0, the training sequences are kept entirely. If the parameter is
set to a value <em>k &gt; 0</em>, then only the last <em>k</em> items
of each sequences will be used for training. This can reduce memory
usage by reducing the size of the prediction model.</li>
  <li><strong>minPredictionRatio:1.0: </strong>This parameter takes a
value in the [0,1] interval that affects the algorithms's accuracy. A
high value may offer more accurate predictions but it may reduces the
coverage while a lower value typically boost the coverage and reduce
the overall accuracy. </li>
  <li><strong>noiseRatio:1.0:</strong> This is a parameter of the
Prediction with improved Noise Reduction (PNR) strategy. It takes a
value in the [0,1] interval that represents the ratio of items in each
sequence that should be considered as noise.<br>
  </li>
</ul>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the item 1 was followed
by items 2, followed by item 3, followed by item 4, and followed by
item 6. The next lines follow the same format.</p>
</blockquote>

<p>Performance</p>

<blockquote>
  <p>CPT+ is a sequence prediction models that often provides the best
accuracy according to a performance comparison (see Gueniche et al.,
2015). Training the model is also very fast. However, performing a
prediction may be slower than with some other models.</p>
</blockquote>

<p>Where can I get more information about CPT+?</p>

<blockquote>
  <p>The CPT+ (Compact Prediction Tree+) model is described in this
article:</p>
  <p><em> Gueniche, T., Fournier-Viger, P., Raman, R., Tseng, V. S.
(2015). CPT+: <a href="http://www.philippe-fournier-viger.com/PAKDD2015_sequence_prediction.pdf">Decreasing
the time/space complexity of the Compact Prediction Tree</a>. Proc.
19th Pacific-Asia Conf. Knowledge Discovery and Data Mining (PAKDD
2015), Springer, LNAI9078, pp. 625-636.</em></p>
  <p>The original CPT algorithm was described in this paper:</p>
  <p><em>Gueniche, T., Fournier-Viger, P., Tseng, V. S. (2013). <a href="http://www.philippe-fournier-viger.com/ADMA2013_Compact_Prediction_trees.pdf">Compact
Prediction Tree: A Lossless Model for Accurate Sequence Prediction</a>.
Proc. 9th Intern. Conference on Advanced Data Mining and Applications
(ADMA 2013) Part II, Springer LNAI 8347, pp. 177-188.</em></p>
  <p> </p>
</blockquote>

<h3><strong><span class="centered"><a name="cpt" id="cptPlus2"> </a></span></strong>
Example 108 : Perform Sequence Prediction using the CPT
Sequence Prediction Model</h3>

<blockquote>
  <p>How to run this example?</p>
  <blockquote>
    <p>To run the implementation of <strong>CPT</strong></p>
    <ul>
      <li><strong>This Example </strong> is not available in the
graphical user interface of SPMF.</li>
      <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCPT.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
    </ul>
  </blockquote>
  <p>What is CPT (Compact Prediction Tree)?</p>
  <blockquote>
    <p><strong>CPT </strong>(Compact Prediction Tree) is a sequence
prediction model. It is used for performing sequence predictions. A
sequence prediction consists of predicting the next symbol of a
sequence based on a set of training sequences. The task of sequence
prediction has numerous applications in various domains. For example,
it can be used to predict the next webpage that a user will visit based
on previously visited webpages by the user and other users. </p>
    <p>The <strong>CPT prediction model </strong>(2013) is an early
version of the <strong>CPT</strong>+ model (Gueniche et al., 2015).
CPT+ was shown to provide better accuracy than <strong>CPT </strong>and
other several state-of-the-art prediction models such as DG, AKOM,
TDAG, PPM on various datasets (see Gueniche et al., 2015 for details).</p>
    <p>The implementation of CPT in SPMF is the original implementation
(obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict
project</a>).</p>
  </blockquote>
  <p>What is the input of CPT?</p>
  <blockquote>
    <p>The input of <strong>CPT</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
    <p>In the context of <strong>CPT</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (2), (3), (4), (6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5), (1), (4), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1), (4), (2), (3)</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p><strong>CPT </strong>also takes as set of parameters as input.</p>
  </blockquote>
  <p>What is the output of <strong>CPT</strong>?</p>
  <blockquote>
    <p><strong>CPT </strong> performs sequence prediction. After <strong>CPT
    </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
    <p>For example, if <strong>CPT</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(4) is the symbol (2).</p>
  </blockquote>
  <p>Parameter(s)</p>
  <blockquote>
    <p>There are several parameters that can be set for CPT. In the
source code, these parameters are passed as a string to the class
CPTPredictor. The main parameters are:</p>
  </blockquote>
  <ul>
    <li><strong>splitLength:6:</strong> This parameter indicates
whether the training sequences should be kept entirely or if they
should be cut to reduce memory usage when training the model. If this
parameter is set to 0, the training sequences are kept entirely. If the
parameter is set to a value <em>k &gt; 0</em>, then only the last <em>k</em>
items of each sequences will be used for training. This can reduce
memory usage by reducing the size of the prediction model.</li>
    <li><strong>recursiveDividerMin:1</strong> : The minimum number of
items to be removed by the Recursive Divider strategy.</li>
    <li><strong>recursiveDividerMax:5</strong> : The maximum number of
items to be removed by the Recursive Divider strategy.<br>
      <br>
    </li>
  </ul>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the item 1 was
followed by items 2, followed by item 3, followed by item 4, and
followed by item 6. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>CPT is a sequence prediction models that often provides a high
accuracy according to a performance comparison (see Gueniche et al.,
2015). Training the model is also very fast. However, performing a
prediction may be slower than with some other models. An improved
version of CPT called CPT+ is also offered in SPMF. It generally
consumes less memory, is faster and provides better accuracy.</p>
  </blockquote>
  <p>Where can I get more information about CPT?</p>
  <blockquote>
    <p>The CPT (Compact Prediction Tree) sequence prediction model was
proposed in this paper:</p>
    <p><em>Gueniche, T., Fournier-Viger, P., Tseng, V. S. (2013). <a href="http://www.philippe-fournier-viger.com/ADMA2013_Compact_Prediction_trees.pdf">Compact
Prediction Tree: A Lossless Model for Accurate Sequence Prediction</a>.
Proc. 9th Intern. Conference on Advanced Data Mining and Applications
(ADMA 2013) Part II, Springer LNAI 8347, pp. 177-188.</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="ppm" id="cptPlus3"> </a></span></strong>
Example 109 : Perform Sequence Prediction using the PPM
Sequence Prediction Model</h3>

<blockquote>
  <p>How to run this example?</p>
  <blockquote>
    <p>To run the implementation of <strong>PPM</strong></p>
    <ul>
      <li><strong>This Example </strong> is not available in the
graphical user interface of SPMF.</li>
      <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestPPM.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
    </ul>
  </blockquote>
  <p>What is PPM?</p>
  <blockquote>
    <p><strong>PPM</strong> (Prediction by Partial Matching) is a
sequence prediction model proposed by Cleary &amp; Witten (1984). It is
used for performing sequence predictions. A sequence prediction
consists of predicting the next symbol of a sequence based on a set of
training sequences. The task of sequence prediction has numerous
applications in various domains. For example, it can be used to predict
the next webpage that a user will visit based on previously visited
webpages by the user and other users. </p>
    <p>The <strong>PPM prediction model </strong> is quite simple.
This is one reason why it is still popular. But can it be outperformed
by newer models such as CPT+ in terms of prediction accuracy</p>
    <p>This implementation has been obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict project</a>.</p>
  </blockquote>
  <p>What is the input of PPM?</p>
  <blockquote>
    <p>The input of <strong>PPM</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
    <p>In the context of <strong>PPM</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (2), (3), (4), (6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5), (1), (4), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1), (4), (2), (3)</td>
      </tr>
    </tbody>
  </table>
  <blockquote> </blockquote>
  <p>What is the output of <strong>PPM</strong>?</p>
  <blockquote>
    <p><strong>PPM </strong> performs sequence prediction. After <strong>PPM
    </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
    <p>For example, if <strong>PPM</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(4) is the symbol (3).</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the item 1 was
followed by items 2, followed by item 3, followed by item 4, and
followed by item 6. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>PPM is a markovian sequence prediction model that assumes that
the next symbol only depends on the previous symbol. This results in a
very simple model that is memory efficient. However, it can be
outperformed in terms of prediction accuracy by newer models such as
CPT+.</p>
  </blockquote>
  <p>Where can I get more information about PPM?</p>
  <blockquote>
    <p>The PPM sequence prediction model was proposed in this paper:</p>
    <p><em>] J. G. Cleary, I. Witten, "Data compression using adaptive
coding and partial string matching".IEEE Transactions on
Communications, vol. 32, pp. 396-402, 1984.</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="dg" id="cptPlus4"> </a></span></strong>
Example 110 : Perform Sequence Prediction using the DG
Sequence Prediction Model</h3>

<blockquote>
  <p>How to run this example?</p>
  <blockquote>
    <p>To run the implementation of <strong>DG</strong> <strong>(Dependency
Graph)</strong></p>
    <ul>
      <li><strong>This Example </strong> is not available in the
graphical user interface of SPMF.</li>
      <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestDG.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
    </ul>
  </blockquote>
  <p>What is DG (Dependency Graph)?</p>
  <blockquote>
    <p><strong>DG</strong> (Dependency Graph) is a sequence prediction
model proposed by Padmanabhan &amp; Mogul (1996). It is used for
performing sequence predictions. A sequence prediction consists of
predicting the next symbol of a sequence based on a set of training
sequences. The task of sequence prediction has numerous applications in
various domains. For example, it can be used to predict the next
webpage that a user will visit based on previously visited webpages by
the user and other users. </p>
    <p>The <strong>DG prediction model </strong> is quite simple.
This is one reason why it is still popular. But can it be outperformed
by newer models such as CPT+ in terms of prediction accuracy</p>
    <p>This implementation has been obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict project</a>.</p>
  </blockquote>
  <p>What is the input of DG?</p>
  <blockquote>
    <p>The input of <strong>DG</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
    <p>In the context of <strong>DG</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (2), (3), (4), (6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5), (1), (4), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1), (4), (2), (3)</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Parameter(s)</p>
  <blockquote>
    <p>The DG algorithm takes the "lookahead window" as parameter. In
the source code, the look-ahead window value is passed as a string to
the class DGPredictor. For example, the string "lookahead:2" means to
set the look-ahead windows to 2. This means that DG will assume that a
symbol only depends on the two previous symbols for performing a
prediction.<br>
    </p>
  </blockquote>
  <p>What is the output of <strong>DG</strong>?</p>
  <blockquote>
    <p><strong>DG </strong> performs sequence prediction. After <strong>DG
    </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
    <p>For example, if <strong>DG</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(4) is the symbol (3).</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the item 1 was
followed by items 2, followed by item 3, followed by item 4, and
followed by item 6. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>DG is a simple sequence prediction model that is thus memory
efficient. However, it can be outperformed in terms of prediction
accuracy by newer models such as CPT+.</p>
  </blockquote>
  <p>Where can I get more information about DG?</p>
  <blockquote>
    <p>The DG sequence prediction model was proposed in this paper:</p>
    <p><em>V. N. Padmanabhan, J. C. Mogul, "Using predictive
prefetching to improve world wide web latency". ACM SIGCOMM Computer
Communication Review, vol. 26, pp. 22-36, 1996.</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="akom" id="cptPlus5"> </a></span></strong>
Example 111 : Perform Sequence Prediction using the AKOM
Sequence Prediction Model</h3>

<blockquote>
  <p>How to run this example?</p>
  <blockquote>
    <p>To run the implementation of <strong>AKOM</strong> <strong>(Dependency
Graph)</strong></p>
    <ul>
      <li><strong>This Example </strong> is not available in the
graphical user interface of SPMF.</li>
      <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAKOM.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
    </ul>
  </blockquote>
  <p>What is <strong>AKOM</strong> (All-k Order Markov)?</p>
  <blockquote>
    <p><strong>AKOM</strong> (All-k Order Markov) is a sequence
prediction model proposed by Pitkow &amp; Piroli (1999) that combines
markovian models of order 1 to k, where k is parameter that need to be
set by the user. This model is used for performing sequence
predictions. A sequence prediction consists of predicting the next
symbol of a sequence based on a set of training sequences. The task of
sequence prediction has numerous applications in various domains. For
example, it can be used to predict the next webpage that a user will
visit based on previously visited webpages by the user and other users.
    </p>
    <p>The <strong>AKOM prediction model </strong> can consume a huge
amount of memory if the parameter k is set to a high value. But it can
have a quite high accuracy. This is one reason why it is still popular.
But AKOM is often outperformed by newer models such as CPT+ in terms of
prediction accuracy and memory usage.</p>
    <p>This implementation has been obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict project</a>.</p>
  </blockquote>
  <p>What is the input of AKOM?</p>
  <blockquote>
    <p>The input of <strong>AKOM</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
    <p>In the context of <strong>AKOM</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (2), (3), (4), (6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5), (1), (4), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1), (4), (2), (3)</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>Parameter(s)</p>
  <blockquote>
    <p>The <strong>AKOM</strong> algorithm takes a value <em>k </em>as
parameter that indicates the order of the model. In the source code,
the value of parameter k is passed as a string to the class <strong>AKOMPredictor</strong>.
For example, the string "order:4" means to set the parameter k to 4.
This indicates that AKOM will create a model of order 4, which means
that it can use up to the four previous symbols in a sequence to
perform a prediction.<br>
    </p>
  </blockquote>
  <p>What is the output of <strong>AKOM</strong>?</p>
  <blockquote>
    <p><strong>AKOM </strong> performs sequence prediction. After <strong>AKOM
    </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
    <p>For example, if <strong>AKOM</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(4) is the symbol (2).</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the item 1 was
followed by items 2, followed by item 3, followed by item 4, and
followed by item 6. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>AKOM is a sequence prediction model that can consume a huge
amount of memory if the parameter <em>k</em> is set to a high value.
Moreover, it can be outperformed in terms of prediction accuracy by
newer models such as CPT+. One of the reason is that AKOM is not noise
tolerant.</p>
  </blockquote>
  <p>Where can I get more information about AKOM?</p>
  <blockquote>
    <p>The All-k Order Markov sequence prediction model was proposed in
this paper:</p>
    <p><em>Pitkow, J., Pirolli, P.: Mining longest repeating
subsequence to predict world wide web surng. In: Proc. 2nd USENIX
Symposium on Internet Technologies and Systems, Boulder, CO, pp. 13â25
(1999)</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="tdag" id="cptPlus6"> </a></span></strong>
Example 112 : Perform Sequence Prediction using the TDAG
Sequence Prediction Model</h3>

<blockquote>
  <p>How to run this example?</p>
  <blockquote>
    <p>To run the implementation of <strong>TDAG</strong> <strong>(Transition
Directed Acyclic Graph)</strong></p>
    <ul>
      <li><strong>This Example </strong> is not available in the
graphical user interface of SPMF.</li>
      <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTDAG.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
    </ul>
  </blockquote>
  <p>What is <strong>TDAG</strong> (<strong>Transition Directed
Acyclic Graph</strong>)?</p>
  <blockquote>
    <p><strong>TDAG</strong> (<strong>Transition Directed Acyclic Graph</strong>)
is a sequence prediction model proposed by Pitkow &amp; Piroli (1999)
that combines markovian models of order 1 to k, where k is parameter
that need to be set by the user. This model is used for performing
sequence predictions. A sequence prediction consists of predicting the
next symbol of a sequence based on a set of training sequences. The
task of sequence prediction has numerous applications in various
domains. For example, it can be used to predict the next webpage that a
user will visit based on previously visited webpages by the user and
other users. </p>
    <p>The <strong>TDAG prediction model </strong> is quite simple.
This is one reason why it is still popular. But TDAG is often
outperformed by newer models such as CPT+ in terms of prediction
accuracy.</p>
    <p>This implementation has been obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict project</a>.</p>
  </blockquote>
  <p>What is the input of TDAG?</p>
  <blockquote>
    <p>The input of <strong>TDAG</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
    <p>In the context of <strong>TDAG</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (2), (3), (4), (6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5), (1), (4), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1), (4), (2), (3)</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>What is the output of <strong>TDAG</strong>?</p>
  <blockquote>
    <p><strong>TDAG </strong> performs sequence prediction. After <strong>TDAG
    </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
    <p>For example, if <strong>TDAG</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(2) is the symbol (3).</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the item 1 was
followed by items 2, followed by item 3, followed by item 4, and
followed by item 6. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>TDAG is a sequence prediction model that is quite simple and
thus is generally memory efficient. But, it can often be outperformed
in terms of prediction accuracy by newer models such as CPT+. </p>
  </blockquote>
  <p>Where can I get more information about TDAG?</p>
  <blockquote>
    <p>The TDAG sequence prediction model was proposed in this paper:</p>
    <p> <em>Laird, P., Saul, R.: Discrete sequence prediction and its
applications. Machine learning, vol. 15, no. 1, 43-68 (1994) </em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="lz78" id="cptPlus7"> </a></span></strong>
Example 113 : Perform Sequence Prediction using the LZ78
Sequence Prediction Model</h3>

<blockquote>
  <p>How to run this example?</p>
  <blockquote>
    <p>To run the implementation of <strong>LZ78</strong></p>
    <ul>
      <li><strong>This Example </strong> is not available in the
graphical user interface of SPMF.</li>
      <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestLZ78.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
    </ul>
  </blockquote>
  <p>What is <strong>LZ78</strong> ?</p>
  <blockquote>
    <p><strong>LZ78</strong> is a sequence prediction model proposed by
Ziiv &amp; Lempel (1978), that is also a compression algorithm. This
model is here used for performing sequence predictions. A sequence
prediction consists of predicting the next symbol of a sequence based
on a set of training sequences. The task of sequence prediction has
numerous applications in various domains. For example, it can be used
to predict the next webpage that a user will visit based on previously
visited webpages by the user and other users. </p>
    <p>The <strong>LZ78 prediction model </strong> is quite simple
and can be outperformed by newer models such as CPT+ in terms of
prediction accuracy.</p>
    <p>This implementation has been obtained from the <a href="https://github.com/tedgueniche/IPredict">ipredict project</a>.</p>
  </blockquote>
  <p>What is the input of LZ78?</p>
  <blockquote>
    <p>The input of <strong>LZ78</strong> is a sequence database
containing training sequences. These sequences are used to train the
prediction model. </p>
    <p>In the context of <strong>LZ78</strong>, a <strong>sequence
database</strong> is a set of sequences where each sequence is a list
of items (symbols). For example, the table shown below contains four
sequences. The first sequence, named S1, contains 5 items. This
sequence means that item 1 was followed by items 2, followed by item 3,
followed by item 4, and followed by item 6. This database is provided
in the file "<strong>contextCPT.txt</strong>" of the SPMF distribution<strong>.</strong></p>
  </blockquote>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(1), (2), (3), (4), (6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(4), (3), (2), (5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(5), (1), (4), (3), (2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(5), (7), (1), (4), (2), (3)</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <p>What is the output of <strong>LZ78</strong>?</p>
  <blockquote>
    <p><strong>LZ78 </strong> performs sequence prediction. After <strong>LZ78
    </strong>has been trained with the input sequence database, it can
predict the next symbol of a new sequence.</p>
    <p>For example, if <strong>LZ78</strong> is trained with the
previous sequence database and parameters, it will predict that the
next symbol following the sequence (1),(4) is the symbol (2).</p>
  </blockquote>
  <p>Input file format</p>
  <blockquote>
    <p>The <strong>input file format</strong> is defined as follows.
It is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is separated by single space and a
-1. The value "-2" indicates the end of a sequence (it appears at the
end of each line). For example, the input file "contextCPT.txt"
contains the following four lines (four sequences).</p>
    <blockquote>
      <p>1 -1 2 -1 3 -1 4 -1 6 -1 -2<br>
4 -1 3 -1 2 -1 5 -1 -2<br>
5 -1 1 -1 4 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 -1 4 -1 2 -1 3 -1 -2</p>
    </blockquote>
    <p>The first line represents a sequence where the item 1 was
followed by items 2, followed by item 3, followed by item 4, and
followed by item 6. The next lines follow the same format.</p>
  </blockquote>
  <p>Performance</p>
  <blockquote>
    <p>LZ78 is a sequence prediction model that is quite simple and
thus is generally memory efficient. But, it can often be outperformed
in terms of prediction accuracy by newer models such as CPT+. </p>
  </blockquote>
  <p>Where can I get more information about LZ78?</p>
  <blockquote>
    <p>The LZ78 sequence prediction model was proposed in this paper:</p>
    <p> <em>Ziv, J., Lempel, A.: Compression of individual sequences
via variable-rate coding. Information Theory, IEEE Transactions on
24(5), 530-536 (1978)</em></p>
  </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="compareseqmodel" id="cptPlus8"> </a></span></strong> Example 114 :
Comparing Several Sequence Prediction Models</h3>

<blockquote>
  <p>How to run this example?</p>
  <ul>
    <li><strong>This Example </strong> is not available in the graphical
user interface of SPMF.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestCompareSequencePredictionModels.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
  </ul>
  <p>This example illustrates how to automatically compare the
accuracy, coverage, training time and prediction time of various
sequence prediction models, on several datasets. This capability was
used for example to generate the experimental results shown in the CPT+
paper (Gueniche et al., 2015).</p>
  <p>To understand this example, you should open the file <strong><span class="Style2">"MainTestCompareSequencePredictionModels.java"</span></strong>
in the package "<strong class="Style9">ca.pfv.SPMF.tests</strong>".</p>
  <p>The first line creates an instance of the Evaluator class to
automatically compares several sequence prediction models. It takes as
parameter a file path where datasets should be stored. For example, in
this example, it is assumes that datasets are located in the folder
"/home/ted/java/IPredict/datasets" on the local computer. <strong>Note
that the datasets are not included in the source code of SPMF due to
the large size of some datasets. But they can be downloaded from the <a href="http://www.philippe-fournier-viger.com/spmf/index.php?link=datasets.php">dataset
page </a>on the SPMF website.</strong></p>
  <blockquote>
    <p> Evaluator evaluator = new
Evaluator("/home/ted/java/IPredict/datasets");</p>
  </blockquote>
  <p>The next lines indicates which datasets should be used for the
experiments. For example, the following lines indicates to load the
BMS.dat dataset and SIGN.dat datasets, and to respectively use the
first 5000 and 1000 lines of these datasets.</p>
  <blockquote>
    <p> evaluator.addDataset("BMS", 5000);<br>
evaluator.addDataset("SIGN", 1000);<br>
... </p>
  </blockquote>
  <p>The next lines specify which sequence prediction models should be
compared and their parameters. For example, the following lines
indicates to compare DG, TDAG and CPT+. Moreover, the look-ahead
parameter of DG is set to 4 and the parameters CCF and CBS of CPT+ are
set to true.</p>
  <blockquote>
    <p> evaluator.addPredictor(new DGPredictor("DG", "lookahead:4"));<br>
evaluator.addPredictor(new TDAGPredictor());<br>
evaluator.addPredictor(new CPTPlusPredictor("CPT+", "CCF:true
CBS:true"));<br>
...</p>
  </blockquote>
  <p>Then, the next line indicates to run the experiment with a k-fold
cross-validation of k = 14, and to print the results, dataset
statistics, and execution statistics.</p>
  <blockquote>
    <p> //Start the experiment<br>
StatsLogger results = evaluator.Start(Evaluator.KFOLD, 14 , true, true,
true);</p>
  </blockquote>
  <p>When this example is run, it will show a comparison of the
performance of the various sequence prediction models.</p>
</blockquote>

<h3><strong><span class="centered"><a name="pfpm" id="e5"> </a></span></strong>Example 115 : Mining Periodic Frequent Patterns Using the PFPM Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style2">PFPM</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPFPM.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
    (4) set the minimum periodicity, the maximum periodicity, the minimum average periodicity and the maximm average periodicity, respectively to <span class="Style2">1, 3, 1, 2 </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> PFPM</strong> contextPFPM.txt output.txt 1 3 1 2</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextPFPM.txt</span>. </li>
  <li><strong>If you are using the source code version of SPMF, </strong>to
    run respectively <strong>PFPM</strong><strong>, </strong>launch
    the file <strong><span class="Style2">"MainTestPFPM.java"</span></strong>in
    the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is PFPM ?</p>
<blockquote>
  <p><strong>PFPM </strong> is an algorithm for discovering periodic frequent
    itemsets in a sequence of transactions (a transaction database). It was proposed by Fournier-Viger et al. (2016). PFPM can discover patterns that periodically appears in a sequence of transactions. Periodic pattern mining has many applications such as discovering periodic behavior of customers, and finding recurring events. </p>
</blockquote>
<p>What is the input of the PFPM algorithm?</p>
<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (a sequence of transactions) and four parameters that are set by the user: </p>
    <ul>
      <ul>
        <li>a minimum periodiccity threshold <em>minper ( a positive integer)</em></li>
        <li>a maximum periodiccity threshold <em>maxper ( a positive integer)</em></li>
        <li>a minimum average periodiccity threshold <em>minavgper ( a positive integer)</em></li>
        <li>a maximum average periodiccity threshold <em>maxavgper ( a positive integer)</em></li>
      </ul>
    </ul>
    <p>A <strong>transaction database</strong> is a set of
      transactions. Each <strong>transaction</strong> is a set of items. For
      example, consider the following transaction database. It contains 7
      transactions (t1, t2, ..., t7) and 5 items (1,2, 3, 4, 5). For example,
      the first transaction represents the set of items 1 and 3. This
      database is provided as the file <strong>contextPFPM.txt</strong> in the SPMF distribution. It is important to note that an item is not
      allowed to appear twice in the same transaction and that items are
      assumed to be sorted by lexicographical order in a transaction.  </p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t6</strong></td>
        <td>{1,3,5}</td>
      </tr>
      <tr>
        <td><strong>t7</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>
<p>What is the output of the algorithm?</p>
<blockquote>
  <p><strong>PFPM</strong> is an algorithm for discovering periodic frequent patterns, which are also called <strong>periodic frequent itemsets</strong>. An itemset is a group of items. The PFPM algorithm finds itemsets that appears periodically in a sequence of transactions. To measure if an itemset  is periodic, the algorithm calculates its periods. To explain this in more details, it is necessary to introduce a few definitions.</p>
  <p>The set of transactions containing an itemset X is denoted as g(X). For example, consider the itemset {1, 3}. The set g({1,3}) is equal to {t1, t3, t5, t6}. In other words, the itemset {1, 3} appears in the transactions t1, t3, t5 and t6. It is also said that these are four occurrences of the itemset {1,3}.</p>
  <p>Now, to assess the <strong>periodicity</strong> of an itemset X, its list of periods is calculated. A <strong>period</strong> is the time in terms of number of transactions between two occurences of an itemset in the database (see the paper for the formal definition). For example, the periods of the itemset {1, 3} are {1,2,2,1,1}. The first period of {1,3} is 1 because {1,3} appears in the first transaction after the creation of the database. The second period of {1,3} is 2 because the itemset appears in transaction t3, which is two transactions after t1. The third period of {1,3} is 2 because the itemset appears in transactions t5, which is two transactions after t3. Then, the fourth period of {1,3} is 1 because the itemset appears in t6, which is one transaction after t5. Finally, the fifth period of {1,3} is 1 because there is one transaction appearing after the last occurrence of {1,3} in the database (in t6).</p>
  <p>The PFPM algorithms utilize the list of periods of an itemset X to calculate its average periodicity, minimum periodicity and maximum periodicity. The <strong>average periodicity</strong> is calculated as the average of the periods of the itemset. The<strong> minimum periodicity </strong>is the smallest period among the periods of the itemset (note that the first and last periods are excluded from the calculation of the minimum periodicity - see the paper for details). The <strong>maximum periodicity</strong> is the largest period among the periods of the itemset.</p>
  <p>The PFPM algorithm finds all the itemsets that have a minimum periodicity, maximum periodicity that are not less than the <em>minper</em> and <em>maxper</em> thresholds, set by the user, and an average periodicity that is not less than <em>minavgper	</em> and not greater than	maxavgper.</p>
  <p>For example, if<strong> PFPM</strong> is run on the previous
    transaction database with a <em>minper = </em>1, 	<em>maxper</em> = 3	, <em>minavgper</em> = 1 and <em>maxavgper</em> = 2, the <strong> PFPM </strong>algorithm finds 11 periodic frequent itemsets.</p>
</blockquote>
<table align="center" border="1" width="424">
  <tbody>
    <tr>
      <td width="114"><strong>itemset</strong></td>
      <td width="69"><strong>support (number of transactions where the itemset appear)</strong></td>
      <td width="69"><strong>minimum periodicity</strong></td>
      <td width="69"><strong>maximum periodicity</strong></td>
      <td width="69"><strong>average periodicity</strong></td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{4}</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{3, 4}</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>{5}</td>
      <td>5</td>
      <td>1</td>
      <td>2</td>
      <td>1.17</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>4</td>
      <td>1</td>
      <td>3</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>{3}</td>
      <td>6</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>How should I interpret the results?</p>
<blockquote>
  <p>Each frequent periodic itemset is annotated with its support (number of transactions where it appears) as well as its minimum/maximum periodicity and average periodicity. For example, the itemset {1, 3} has a support of 4 because
    it appears in four transactions (t1, t3, t5 and t6. The average periodicity of {1,3} is 1.4 because on average it appears every 1.4 transactions in terms of time. The smallest period of {1,3} (minimum periodicity) is 1 and the largest period of {1,3} is 2 transactions.This indicates that {3} appears quite periodically.</p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> used by PFPM is defined
    as follows. It is a text file. An item is represented by a positive
    integer. A transaction is a line in the text file. In each line
    (transaction), items are separated by a single space. It is assumed
    that all items within a same transaction (line) are sorted according to
    a total order (e.g. ascending order) and that no item can appear twice
    within the same line.</p>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <p>3 1<br>
5<br>
3 5 1 2 4<br>
3 5 2 4<br>
3 1 4<br>
3 5 1<br>
3 5 2</p>
  <p>Note that it is also possible to use <strong>the ARFF format</strong> as an alternative to the default input format. The specification of the
    ARFF format can be found <a rel="nofollow" href="http://weka.wikispaces.com/ARFF+%28stable+version%29">here</a>.
    Most features of the ARFF format are supported except that (1) the
    character "=" is forbidden and (2) escape characters are not
    considered. Note that when the ARFF format is used, the performance of
    the data mining algorithms will be slightly less than if the native
    SPMF file format is used because a conversion of the input file will be
    automatically performed before launching the algorithm and the result
    will also have to be converted. This cost however should be small. </p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
    is a text file, where each line represents a frequent periodic itemset. On each
    line, the items of the itemset are first listed. Each item is
    represented by an integer and it is followed by a single space. After,
    all the items, the keyword "#SUP:" appears, which is followed by an
    integer indicating the support of the itemset, expressed as a number of
    transactions. Then, the keyword #MINPER: appears and is followed by a space,  an integer indicating the minimum periodicity of the itemset, and another space.Then, the keyword #MAXPER: appears and is followed by a space,  an integer indicating the maximal periodicity of the itemset, and another space. Then, the keyword #AVGPER: appears and is followed by a space,  a double value indicating the average periodicity of the itemset.</p>
  <p>For example, here is the output file for this example.
    The first line indicates that the itemset {2} is a frequent periodic itemset, having a support of 3 transactions, a minimum periodicity of 1 transactions, a maximum periodicity of 3 transactions and an average periodicity f 1.75 transactions..</p>
  <p>2 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    2 5 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    2 5 3 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    2 3 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    4 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    4 3 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    1 #SUP: 4 #MINPER: 1 #MAXPER: 2 #AVGPER: 1.4<br>
    1 3 #SUP: 4 #MINPER: 1 #MAXPER: 2 #AVGPER: 1.4<br>
    5 #SUP: 5 #MINPER: 1 #MAXPER: 2 #AVGPER: 1.1666666666666667<br>
    5 3 #SUP: 4 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.4<br>
    3 #SUP: 6 #MINPER: 1 #MAXPER: 2 #AVGPER: 1.0<br>
  </p>
  <p>Note that if the ARFF format is used as input instead of the
    default input format, the output format will be the same except that
    items will be represented by strings instead of integers.</p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p>PFPM is currently the only algorithm designed to mine periodic frequent itemset in a sequence of transaction (a transaction database), offered in SPMF. Another algorithm called PHM is offered for mining periodic patterns in sequence of transactions containing profit informaiton (items have weights/unit profit, and transactions indicate purchase quantities for items)</p>
</blockquote>
<p>Where can I get more information about the PFPM algorithm?</p>
<blockquote>
  <p>This is the article proposing the PFPM algorithm:</p>
  <p><em>Fournier-Viger, P., Lin, C.-W., Duong, Q.-H., Dam, T.-L., Sevcic, L., Uhrin, D., Voznak, M. (2016). PFPM: Discovering Periodic Frequent Patterns with Novel Periodicity Measures. Proc. 2nd Czech-China Scientific Conference 2016, Elsevier, 10 pages.</em><em></em></p>
</blockquote>
<h3><strong><span class="centered"><a name="phm" id="e6"> </a></span></strong>Example 116 : Mining Periodic High--Utility Itemsets Using the PHM Algorithm</h3>
<p>How to run this example?</p>
<ul>
  <li><strong>If you are using the graphical interface, </strong>(1)
    choose the <strong>"<span class="Style2">PHM</span>"</strong> algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">DB_UtilityPerHUIs.txt"</span></strong>,
    (3) set the output file name (e.g. "<span class="Style2">output.txt</span>")
    (4) set the minimum utility threshold, minimum periodicity, the maximum periodicity, the minimum average periodicity and the maximm average periodicity, respectively to <span class="Style2">20, 1, 3, 1, 2 </span>and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
    then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong> PHM</strong> DB_UtilityPerHUIs.txt output.txt 20 1 3 1 2</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2"><strong>DB_UtilityPerHUIs</strong>.txt</span>. </li>
  <li><strong>If you are using the source code version of SPMF, </strong>to
    run respectively <strong>PHM</strong><strong>, </strong>launch
    the file <strong><span class="Style2">"MainTestPHM.java"</span></strong>in
    the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
</ul>
<p>What is PHM ?</p>
<blockquote>
  <p><strong>PHM </strong> is an algorithm for discovering periodic high-utility
    itemsets in a sequence of transactions (a transaction database) having information about the utility of items. It was proposed by Fournier-Viger et al. (2016). PHM can discover patterns that periodically appears in a sequence of transactions and that generate a high profit (have a hih utility). Periodic high utility itemset mining has many applications such as discovering periodic purchase behaviors of customers of a retail store, which yield a high profit.</p>
  <p>The PHM algorithm is similar to the PFPM algorithm also offered in SPMF. The main difference is that PHM consider the additional constraint of utility (profit generated by itemsets). Thus, there is an additional constraint that we not only want to find periodic patterns, but also profitable patterns.</p>
</blockquote>
<p>What is the input of the PHM algorithm?</p>
<blockquote>
  <blockquote>
    <p>The input is a <strong>transaction database</strong> (a sequence of transactions) and four parameters that are set by the user: </p>
    <ul>
      <ul>
        <li>a minimum utility threshold <em>minutil ( a positive integer</em>)</li>
        <li>a minimum periodiccity threshold <em>minper ( a positive integer)</em></li>
        <li>a maximum periodiccity threshold <em>maxper ( a positive integer)</em></li>
        <li>a minimum average periodiccity threshold <em>minavgper ( a positive integer)</em></li>
        <li>a maximum average periodiccity threshold <em>maxavgper ( a positive integer)</em></li>
      </ul>
    </ul>
    <p>A <strong>transaction database</strong> is a set of
      transactions. Let's consider the following
database consisting of 7 transactions (t1,t2...t5,t6, t7) and 7 items (1, 2,
3, 4, 5, 6, 7). This database is provided in the text file "<strong>DB_utilityPerHUIs.txt</strong>"
in the package <strong>ca.pfv.spmf.tests </strong>of the SPMF
distribution<strong>.</strong></p>
    <table align="center" border="1" width="721">
      <tbody>
        <tr>
          <td width="56"><br></td>
          <td width="156"><strong>Items</strong></td>
          <td width="190"><strong>Transaction utility</strong></td>
          <td width="291"><strong>Item utilities for this transaction</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>{1, 3}</td>
          <td>6</td>
          <td>5 1</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>{5}</td>
          <td>3</td>
          <td>3</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>{1, 2, 3, 4, 5}</td>
          <td>25</td>
          <td>5 10 1 6 3</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>{2, 3, 4, 5}</td>
          <td>20</td>
          <td>8 3 6 3</td>
        </tr>
        <tr>
          <td><strong>t5</strong></td>
          <td>{1, 3, 4}</td>
          <td>8</td>
          <td>5 1 2</td>
        </tr>
        <tr>
          <td><strong>t6</strong></td>
          <td>{1,3,5}</td>
          <td>22</td>
          <td>10 6 6</td>
        </tr>
        <tr>
          <td><strong>t7</strong></td>
          <td>{2, 3, 5}</td>
          <td>9</td>
          <td>4 2 3</td>
        </tr>
      </tbody>
    </table>
    <blockquote>
      <p>Each line of the <strong>database</strong> is:</p>
      <ul>
        <li> a set of items (the first column of the table), </li>
        <li>the sum of the utilities (e.g. profit) of these items in this
          transaction (the second column of the table),</li>
        <li>the utility of each item for this transaction (e.g. profit
          generated by this item for this transaction)(the third column of the
          table).</li>
      </ul>
      <p>Note that the value in the second column for each line is the
        sum of the values in the third column.</p>
      <p>What are real-life examples of such a database? There are
        several applications in real life. One application is a customer
        transaction database. Imagine that each transaction represents the
        items purchased by a customer on a given day. The first transaction named "<strong>t1</strong>"
        represents the purchase of items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
        item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
        of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
    </blockquote>
  </blockquote>
</blockquote>
<p>What is the output of the algorithm?</p>
<blockquote>
  <p><strong>PHM</strong> is an algorithm for discovering <strong>periodic high-utility itemsets</strong>. An itemset is a group of items. The PHM algorithm finds itemsets that appears periodically in a sequence of transactions and generate a high profit (have a high utility). To measure if an itemset  is periodic, the algorithm calculates its periods. To explain this in more details, it is necessary to introduce a few definitions.</p>
  <p>The set of transactions containing an itemset X is denoted as g(X). For example, consider the itemset {1, 3}. The set g({1,3}) is equal to {t1, t3, t5, t6}. In other words, the itemset {1, 3} appears in the transactions t1, t3, t5 and t6. It is also said that these are four occurrences of the itemset {1,3}.</p>
  <p>Now, to assess the <strong>periodicity</strong> of an itemset X, its list of periods is calculated. A <strong>period</strong> is the time in terms of number of transactions between two occurences of an itemset in the database (see the paper for the formal definition). For example, the periods of the itemset {1, 3} are {1,2,2,1,1}. The first period of {1,3} is 1 because {1,3} appears in the first transaction after the creation of the database. The second period of {1,3} is 2 because the itemset appears in transaction t3, which is two transactions after t1. The third period of {1,3} is 2 because the itemset appears in transactions t5, which is two transactions after t3. Then, the fourth period of {1,3} is 1 because the itemset appears in t6, which is one transaction after t5. Finally, the fifth period of {1,3} is 1 because there is one transaction appearing after the last occurrence of {1,3} in the database (in t6).</p>
  <p>The PHM algorithms utilize the list of periods of an itemset X to calculate its average periodicity, minimum periodicity and maximum periodicity. The <strong>average periodicity</strong> is calculated as the average of the periods of the itemset. The<strong> minimum periodicity </strong>is the smallest period among the periods of the itemset (note that the first and last periods are excluded from the calculation of the minimum periodicity - see the paper for details). The <strong>maximum periodicity</strong> is the largest period among the periods of the itemset.</p>
  <p>The <strong>utility of an
      itemset in a transaction </strong>is the sum of the utility of its
    items in the transaction. For example, the utility of the itemset {1 3}
    in transaction t1 is 5 + 1 = 6 and the utility of {1 3} in transaction
    t3 is 5 + 1 = 6. The<strong> utility of an itemset in a database</strong> is the sum of its utility in all transactions where it appears. For
    example, the utility of {13} in the database is the utility of {1 3}
    in t1 plus the utility of {1 3} in t3,  plus the utility of {1 3} in t5,  plus the utility of {1 3} in t6, for a total of 6 + 6 + 6 + 16= 34. A<strong> high utility itemset</strong> is an itemset such that its utility is no
  less than <em>min_utility.</em></p>
  <p>The PHM algorithm finds all the high-utility itemsets that have a minimum periodicity and maximum periodicity that are not less than the <em>minper</em> and <em>maxper</em> thresholds, and an average periodicity that is not less than <em>minavgper </em> and not greater than	maxavgper.Those itemsets are called <strong>periodic high-utility itemsets<em>.</em></strong></p>
  <p>For example, if<strong> PHM</strong> is run on the previous
    transaction database with a <em>minutil </em> = 20, <em>minper = </em>1, <em>maxper</em> = 3	, <em>minavgper</em> = 1 and <em>maxavgper</em> = 2, the <strong> PHM </strong>algorithm finds 7 periodic high-utility itemsets.</p>
</blockquote>
<table align="center" border="1" width="499">
  <tbody>
    <tr>
      <td width="114"><strong>itemset</strong></td>
      <td width="69"><strong>utility</strong></td>
      <td width="69"><strong>support (number of transactions where the itemset appear)</strong></td>
      <td width="69"><strong>minimum periodicity</strong></td>
      <td width="69"><strong>maximum periodicity</strong></td>
      <td width="69"><strong>average periodicity</strong></td>
    </tr>
    <tr>
      <td>{2}</td>
      <td>22</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{2, 5}</td>
      <td>31</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{2, 3, 5}</td>
      <td>37</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{2, 3}</td>
      <td>28</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1.75</td>
    </tr>
    <tr>
      <td>{1}</td>
      <td>25</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>{1, 3}</td>
      <td>34</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>1.4</td>
    </tr>
    <tr>
      <td>{3, 5}</td>
      <td>27</td>
      <td>4</td>
      <td>1</td>
      <td>3</td>
      <td>1.4</td>
    </tr>
  </tbody>
</table>
<p>How should I interpret the results?</p>
<blockquote>
  <p>Each  periodic high-utility itemset is annotated with its utility (e.g. profit), support (number of transactions where it appears) as well as its minimum/maximum periodicity and average periodicity. For example, the itemset {1, 3} yield a profit of 22$, has a support of 4 because
    it appears in four transactions (t1, t3, t5 and t6. The average periodicity of {1,3} is 1.4 because on average it appears every 1.4 transactions in terms of time. The smallest period of {1,3} (minimum periodicity) is 1 and the largest period of {1,3} is 2 transactions.This indicates that {3} appears quite periodically and is quite profitable.</p>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong> used by the algorithm is defined
    as follows. It is a text file. An item is represented by a positive
    integer. A transaction is a line in the text file. In each line
    (transaction), items are separated by a single space. It is assumed
    that all items within a same transaction (line) are sorted according to
    a total order (e.g. ascending order) and that no item can appear twice
    within the same line.</p>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <p>3 1:6:1 5<br>
    5:3:3<br>
    3 5 1 2 4:25:1 3 5 10 6<br>
    3 5 2 4:20:3 3 8 6<br>
    3 1 4:8:1 5 2<br>
    3 5 1:22:6 6 10<br>
  3 5 2:9:2 3 4</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
    is a text file, where each line represents a  periodic itemset. On each
    line, the items of the itemset are first listed. Each item is
    represented by an integer and it is followed by a single space. After,
    all the items, the keyword #UTIL: appears followed by a single space, the utility of the itemset, and a single space. Then, the keyword "#SUP:" appears, which is followed by an
    integer indicating the support of the itemset, expressed as a number of
    transactions. Then, the keyword #MINPER: appears and is followed by a space,  an integer indicating the minimum periodicity of the itemset, and another space.Then, the keyword #MAXPER: appears and is followed by a space,  an integer indicating the maximal periodicity of the itemset, and another space. Then, the keyword #AVGPER: appears and is followed by a space,  a double value indicating the average periodicity of the itemset.</p>
  <p>For example, here is the output file for this example.
    The first line indicates that the itemset {2} is a  periodic high-utility itemset, having a utility of 22$, a support of 3 transactions, a minimum periodicity of 1 transactions, a maximum periodicity of 3 transactions and an average periodicity f 1.75 transactions..</p>
  <p>2 #UTIL: 22 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    2 5 #UTIL: 31 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    2 5 3 #UTIL: 37 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    2 3 #UTIL: 28 #SUP: 3 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.75<br>
    1 #UTIL: 25 #SUP: 4 #MINPER: 1 #MAXPER: 2 #AVGPER: 1.4<br>
    1 3 #UTIL: 34 #SUP: 4 #MINPER: 1 #MAXPER: 2 #AVGPER: 1.4<br>
    5 3 #UTIL: 27 #SUP: 4 #MINPER: 1 #MAXPER: 3 #AVGPER: 1.4<br>
  </p>
</blockquote>
<p>Performance</p>
<blockquote>
  <p>PHM is currently the only algorithm  for mining high-utility periodic patterns in sequence of transactions containing profit informaiton (items have weights/unit profit, and transactions indicate purchase quantities for items), which is offered in SPMF. It was shown that mining periodic high-utility itemsets can be faster than mining all high-utility itemsets, because PHM can filter many non-periodical patterns.</p>
</blockquote>
<p>Where can I get more information about the PHM algorithm?</p>
<blockquote>
  <p>This is the article proposing the PHM algorithm:</p>
  <p><em>Fournier-Viger, P., Lin, C.W., Duong, Q.-H., Dam, T.-L. (2016). PHM: Mining Periodic High-Utility Itemsets. Proc. 16th Industrial Conference on Data Mining. Springer LNAI 9728, 15 pages.</em></p>
</blockquote>
<p></p>
<p>&nbsp;</p>
<h3><strong><span class="centered"><a name="textclassifier" id="textclassifier"> </a></span></strong> Example 117 :
  Classifying Text documents using a Naive Bayes approach</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTextClassifier.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this algorithm? </p>

<blockquote>
  <p>The algorithm is a <strong>text document classifier </strong>implemented
by <strong>Sabarish Raghu</strong>. It can classify texts
automatically into categories. For example, it can be used to
classified text by subjects. </p>
  <p>The input is a training set of texts classified by categories
(with known categories) and a set of texts to be classified (with
unknown categories). </p>
  <p>The output is a category for each text from the testing set.</p>
</blockquote>

<p>How the algorithm works?</p>

<blockquote>
  <p>The Naive Bayes classifier is a probabilistic classifier. It
compute the probability of a document <strong>d</strong> being in a
class <strong>c</strong> as follows:<br>
  <strong>P(c|d) â P(c) Y 1â¤kâ¤nd P(tk |c)</strong> where<br>
  <strong>nd</strong> is the length of the document. (number of tokens)<br>
  <strong>P(tk |c)</strong> is the conditional probability of term tk
occurring in a document of class <strong>c</strong><br>
  <strong>P(tk |c)</strong> as a measure of how much evidence tk
contributes that <strong>c</strong> is the correct class.<br>
  <strong>P(c)</strong> is the prior probability of <strong>c</strong>.<br>
If a documentâs terms do not provide clear evidence for one class vs.
another, we choose the <strong>c</strong> with highest <strong>P(c)</strong>
  </p>
  <p>-------------------------------------<br>
  <strong>Pseudocode</strong> <strong>of the algorithm</strong></p>
  <p> Algorithm ():<br>
NaÃ¯ve Bayes(Test_Data_Dir, Training_Data_Dir)<br>
{<br>
For(each test file in test data directory)<br>
For each class<br>
Map&lt;class, probability&gt; ProbabilityMap;<br>
For each word in test file<br>
Wordprobability=Probability of occurance of that word in the class<br>
ProbabilityMap.put(className,probability*Wordprobability)<br>
Classified_class=Key of Max probability value<br>
}</p>
  <p>-------------------------------------<br>
  <strong>The algorithm contains a set of classes, as follows:</strong></p>
  <p><strong>Class "TestRecord"</strong><br>
Holds the Test record as an object.</p>
  <p>* String RecordId Filename of the Test File<br>
* String fullRecord Test record as a single string.<br>
* ArrayList&lt;String&gt; words words in the test record.</p>
  <p><strong>Class "OccurrenceProbabilties"</strong><br>
Used as a cache to store the probabilities of words associated with a
particular class.<br>
* String className Classname<br>
* Hashmap&lt;String,Double&gt; Probability of the each word</p>
  <p><br>
  <strong>Class "MemoryFile"</strong><br>
Holds the training record as an object.<br>
* String className Class name of the training file<br>
* ArrayList&lt;String&gt; content Words in the class.</p>
  <p>-------------------------------------<br>
  <strong>Flow of the Code:</strong><br>
1. Read each test file, remove stopwords, perform stemming and load in
to objects.<br>
2. Read each training file, remove stopwords, perform stemming and load
in to objects.<br>
3. For each test file, for each class name, for each word; check if the
probability already exist in cache.<br>
4. Else compute the probability of each word and multiply them to get
overall probability for the test file.<br>
5. Check which probability has maximum among the classes for the test
file which gives the class value of the file.</p>
  <p>----------------------------------------------------------------------------------------<br>
  <strong>There is two modes of execution</strong><br>
Take your choice depending upon the size of the dataset and computing
power you have in the machine.</p>
  <p><strong>* In Memory</strong><br>
Training Data is loaded in to memory as objects.<br>
Executes much faster<br>
Significantly less number of file reads.<br>
Higher memory load.</p>
  <p><strong>* File Read</strong><br>
Handles Training data as files as it is.<br>
Executes slower<br>
More number of file reads.<br>
Significantly less memory load.<br>
  </p>
  <p>----------------------------------------------------------------------------------------<br>
  <strong>How to use this algorithm</strong></p>
  <p> An example of how to use the algorithm is provided in the file
MainTestTextClassifier of the package ca.pfv.spmf.test. To run the
algorithm, one needs to create an instance of the algorithm and call
the runAlgorithm() method:<br>
  <br>
  <em>AlgoNaiveBayesClassifier nbClassifier=new
AlgoNaiveBayesClassifier();<br>
nbClassifier.runAlgorithm(&lt;Training_Directory&gt;,&lt;Test_Directory&gt;,&lt;Output_Directory&gt;,&lt;Memory_Flag&gt;);</em></p>
  <p>The output is a file indicating the categories of each text from
the testing set. </p>
  <p><em>Output âoutput.tsvâ would be found in the output directory
âoutput.tsvâ</em></p>
  <p>----------------------------------------------------------------------------------------</p>
  <p><strong>Input of the algorithm</strong></p>
  <p>The algorithm takes a input two directories. The first directory
is a set of training texts. The second directory is a set of testing
texts that need to be classified.</p>
  <p>In the package <strong>ca.pfv.spmf.test.text_classification_set</strong>,
there are some sample files for training and testing.</p>
  <p> Please follow the following structure of directory for Test and
training directory.</p>
  <p><strong>For training directory</strong> <strong>(to train the
algorithm)</strong>:</p>
  <p>TrainingDirectoryName<br>
---&gt;ClassName1<br>
---&gt;Trainingfile1<br>
---&gt;Trainingfile2<br>
  <br>
---&gt;TrainingFileN<br>
---&gt;ClassName2<br>
---&gt;Trainingfile1<br>
---&gt;Trainingfile2<br>
  <br>
.........</p>
  <p><strong>For the directory of test files (to be classified)</strong></p>
  <p>----&gt;TestDirectoryName<br>
----&gt;Testfile1<br>
----&gt;Testfile2<br>
----&gt;Testfile3<br>
...</p>
  <p><br>
----------------------------------------------------------------------------------------</p>
  <p><strong>Output of the algorithm</strong></p>
</blockquote>

<blockquote>
  <p>The algorithm outputs a file <em> âoutput.tsvâ</em> in the output
directory indicating the categories (classes) attributed to each text
of the test set.</p>
  <p> </p>
</blockquote>

<h3><strong><span class="centered"><a name="example1" id="TextClusterer"> </a></span></strong>
Example 118 : Clustering Texts with a text clusterer</h3>

<p>How to run this example?</p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">TextClusterer</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">input_text_clustering.txt</span><span class="Style2">"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <strong>"<span class="Style9">Perform stemming:</span>"</strong>
to true, (5) set <strong>"<span class="Style9">Remove stopwords:</span>"</strong>
to true, and (5) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> TextClusterer </span></strong><strong>input_text_clustering</strong>.txt
output.txt true true</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2"><strong>input_text_clustering</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTextClusterer.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this algorithm? </p>

<blockquote>
  <p>The algorithm is a <strong>text clusterer </strong>implemented
by <strong>Sabarish Ragh</strong>u. It can group texts automatically
into clusters. Furthermore, it offers the possiblity of performing
stemming and removing stop words before performing clustering.</p>
  <p>The text clusterer works as follows:<br>
1. Load the input file<br>
2. Remove the stopwords (optional)<br>
3. Stem the words (optional)<br>
4. Calculate <strong>tf*idf</strong> value for each record in the
input file.<br>
5. Calculate similarity matrix by using the tfidf values of the records.<br>
6. Take most similar records per each record and make them as clusters
initially.<br>
7. Use the transitive rule A,B are most similar and B,C are most
similar; A and C are likely to be similar. This imply that A, B, C are
in the same cluster.<br>
8. Merge the clusters based on the above rule for all the records.<br>
9. Write the final output i.e,; final sets of clusters to the output
file.</p>
  <p><strong>StopWords</strong><br>
Words that are insignificant to identify the clusters. This algorithm
by defauly uses the list of most popular stopwords. Anyways we can
define our own stopword list or even not remove any word.</p>
  <p><strong>Stemming</strong><br>
Stemming is deriving the base word of a word. <br>
For example: Identification is stemmed to Identity</p>
  <p> For stemming, we use the famous Porter stemmer algorithm, which
uses rules given by Porter to stem the words. This implementation is
taken from Brian Goetz's implementation from the internet.</p>
  <p><strong>tf:term frequency</strong><br>
Term frequency defines how frequently a term occur in a document</p>
  <p> <strong>Idf:Inverse document frequency</strong><br>
Inverse document frequency is the frequency of a word in the whole set
of documents.</p>
  <p> <strong>similarity matrix:</strong><br>
2 dimensional matrix representation of a record's similarity with all
other records.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a text file.</p>
  <p>Each line of the text file represents a text.</p>
  <p>A line starts with an integer id. Then, it is followed by a tab,
and then a text where words are separated by spaces.</p>
  <p>An example of input is provided in the file "<span class="Style3"><strong>input_text_clustering</strong>.txt</span>"
of the SPMF distribution. It contains 100 texts (100 lines). For
example, here are two lines from this file:</p>
  <blockquote>
    <p>692770 inventory taker description team members required
physically count inventory various retailers enter information
equipment inventory counted varies depending type store audited items
may located floor tables shelves various heights items generally
counted shelves may moved required inventories take approximately hours
complete however may take longer depending size store level inventory
counted </p>
    <p>574319 leading hotel furniture supplier seeking project managers
manage national international accounts bid packages unique exciting
opportunity right individuals qualified applicants should send resumes
talli globalallies com phone calls please </p>
  </blockquote>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is a set of clusters of similar texts. </p>
  <p>The <strong>output file format </strong>is defined as follows.
The first line indicates the file format. Then, each following line
contains a text id followed by a cluster id. For example, here are the
first five lines of the output file obtained by applying the text
clusterer on the sample input file:</p>
  <p>RecordId Clusternum<br>
171056 0<br>
770853 0<br>
247263 1<br>
870007 1</p>
  <p>Results can be interpreted as follows. The texts with ids "171056"
and "770853" both belongs to the same cluster, having the id "0".
Moreover, the texts with ids "247263" and "870007" both belongs to the
same cluster, having the id "1".</p>
</blockquote>

<h3><strong><span class="centered"><a name="id3" id="example29"> </a></span></strong>
Example 119 : Creating a decision tree with the ID3
algorithm to predict the value of a target attribute</h3>

<p>How to run this example?</p>

<blockquote>
  <p><strong>To run this example with the source code version of SPMF,</strong>
launch the file <strong><span class="Style2">"MainTestID3.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.</p>
  <p><strong>This example is not available in the release version of
SPMF.</strong></p>
</blockquote>

<p>What is the ID3 algorithm?</p>

<blockquote>
  <p>The <strong>ID3 algorithm</strong> is a classic data mining
algorithm for classifying instances (a <em>classifier</em>). It is
well-known and described in many artificial intelligence and data
mining books.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a set of training data for building a decision tree.</p>
  <p>For instance, in this example, we use the following database (from
the book "Machine Learning by Tom Mitchell). This database is provided
in the file "<strong>tennis.txt</strong>" of the SPMF distribution<strong>.</strong>
  </p>
  <p>This database defines five attributes and contains fourteen
instances. </p>
</blockquote>

<table align="center" border="1" width="200">

  <tbody>
    <tr>
      <td><strong>outlook</strong></td>
      <td><strong>temp</strong></td>
      <td><strong>humid</strong></td>
      <td><strong>wind</strong></td>
      <td><strong>play?</strong></td>
    </tr>
    <tr>
      <td>sunny</td>
      <td>hot</td>
      <td>high</td>
      <td>weak</td>
      <td>no</td>
    </tr>
    <tr>
      <td>sunny</td>
      <td>hot</td>
      <td>high</td>
      <td>strong</td>
      <td>no</td>
    </tr>
    <tr>
      <td>overcast</td>
      <td>hot</td>
      <td>high</td>
      <td>weak</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>rain</td>
      <td>mild</td>
      <td>high</td>
      <td>weak</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>rain</td>
      <td>cool</td>
      <td>normal</td>
      <td>weak</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>rain</td>
      <td>cool</td>
      <td>normal</td>
      <td>strong</td>
      <td>no</td>
    </tr>
    <tr>
      <td>overcast</td>
      <td>cool</td>
      <td>normal</td>
      <td>strong</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>sunny</td>
      <td>mild</td>
      <td>high</td>
      <td>weak</td>
      <td>no</td>
    </tr>
    <tr>
      <td>sunny</td>
      <td>cool</td>
      <td>normal</td>
      <td>weak</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>rain</td>
      <td>mild</td>
      <td>normal</td>
      <td>weak</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>sunny</td>
      <td>mild</td>
      <td>normal</td>
      <td>strong</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>overcast</td>
      <td>mild</td>
      <td>high</td>
      <td>strong</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>overcast</td>
      <td>hot</td>
      <td>normal</td>
      <td>weak</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>rain</td>
      <td>mild</td>
      <td>high</td>
      <td>strong</td>
      <td>no</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>By applying the <strong>ID3 algorithm</strong>, a decision tree
is created. To create the decision tree, we have to choose a target
attribute. Here, we choose the attribute "<strong>play</strong>". </p>
  <p>The following decision tree is created:</p>
  <p><img src="SPMF:%20A%20Java%20Open-Source%20Data%20Mining%20Library_files/id3.PNG" alt="id3 decision tree" height="266" width="649"></p>
  <p>We can now use the tree to predict the value of the target
attribute "<strong>play</strong>" for a new instance.</p>
  <p>For example, consider this new instance, where the value for
"play" is unknown.</p>
</blockquote>

<table align="center" border="1" width="200">

  <tbody>
    <tr>
      <td>sunny</td>
      <td>hot</td>
      <td>normal</td>
      <td>weak</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>By applying the decision tree, the value for the attribute "<strong>play</strong>"
for this instance is "<strong>yes</strong>".</p>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The input file format is a text file. The first lines contains a
list of attribute names separated by a single space. A attribute name
is simply a string without spaces. The next lines represents instances,
where each line contains a string value for each attribute, separated
by single spaces. For example, consider the file "tennis.txt" of the
previous example. </p>
  <blockquote>
    <p>play outlook temp humid wind<br>
no sunny hot high weak<br>
no sunny hot high strong<br>
yes overcast hot high weak<br>
yes rain mild high weak<br>
yes rain cool normal weak<br>
no rain cool normal strong<br>
yes overcast mild high strong<br>
no sunny mild high weak<br>
yes sunny cool normal weak<br>
yes rain mild normal weak<br>
yes sunny mild normal strong<br>
yes overcast hot normal weak<br>
yes overcast cool normal strong<br>
no rain mild high strong</p>
  </blockquote>
  <p>The first line defines the attribute names : "play", "outlook",
"temp", "humid" and "wind". Then, consider the second line. It
represents an instance having the value "no", "sunny", "hot", "high"
and "weak" respectively for the five attributes. The next lines follow
the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>There is no output file for the ID3 algorithm. It is only
available in the source code version of SPMF and it does not generate
an output file.</p>
</blockquote>

<p>Where can I get more information about the ID3 algorithm?</p>

<blockquote>
  <p>The ID3 algorithm was proposed by Quinlan (1986). It is one of the
most popular algorithm for learning decision trees. By searching on the
web, you can find plenty of information on this algorithm. It is also
described in several data mining books and artificial intelligence
books.</p>
</blockquote>

<h3><strong><span class="centered"><a name="convseq" id="example38"> </a></span></strong>
Example 120 : Converting a Sequence Database to SPMF
Format</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Convert_a_sequence_database_to_SPMF_format</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextCSV</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>input_format =</em><span class="Style2"> CSV_INTEGER</span>
and <em>sequence count = </em><span class="Style2">5</span>.(5) click
"<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Convert_a_sequence_database_to_SPMF_format</span></strong><strong>
contextCSV</strong>.txt output.txt CSV 5</span> in a folder containing <span class="Style2">spmf.jar</span> and the example input file <span class="Style2">contextCSV.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestConvertSequenceDatabase.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>The <strong>tool for converting a sequence databases to SPMF format
</strong>takes three prameters as input:</p>

<ul>

  <li>a sequence database,</li>
  <li>the name of the sequence database format (<em>CSV_INTEGER</em>, <em>Kosarak</em>,
    <em>Snake</em>, <em>IBMGenerator</em> or <em>BMS</em>),</li>
  <li>the number of sequences to be converted</li>
</ul>

<p>The algorithm outputs a sequence database in SPMF format.</p>

<p>The <strong>CSV_INTEGER format</strong> is defined as follows:</p>

<ul>

  <li>each line is a sequence</li>
  <li>each sequence is a list of items represented by positive integers
(&gt;0) separated by commas.</li>
</ul>

<p>For example, the follwing sequence databasee is in CSV_INTEGER
format and contains four sequences: </p>

<pre>1,2,3,4<br>5,6,7,8<br>5,6,7<br>1,2,3</pre>

<p>The <strong>Kosarak format </strong>is defined as follows:</p>

<ul>

  <li>each line is a sequence</li>
  <li>each sequence is a list of items represented by integers and
separated by a space.</li>
</ul>

<p>For example, the follwing sequence databasee is in Kosarak format
and contains four sequences: </p>

<pre>1 2 3 4<br>5 6 7 8<br>5 6 7<br>1 2 3</pre>

<p>The <strong>IBMGenerator format </strong>is the format used by the
<strong>IBM Data Quest Generato</strong>r. The format is defined as
follows:</p>

<ul>

  <li>the file is a binary file in little indian format</li>
  <li>the file is a list of integers without spaces</li>
  <li>a positive integer represents an item</li>
  <li>-1 represents the end of an itemset</li>
  <li>-2 represents the end of a sequence</li>
</ul>

<p>For example, the follwing sequence databasee is in Kosarak format
and contains four sequences: </p>

<pre>1 -1 2 -1 3 -1 4 -1 -2<br>5 -1 6 -1 7 -1 8 -1 -2<br>5 -1 6 -1 7 -1 -2<br>1 -1 2 -1 3 -1 -2</pre>

<p>The <strong>Snake format </strong>is defined as follows:</p>

<ul>

  <li>each line is a sequence</li>
  <li>each sequence is a list of letters </li>
</ul>

<p>For example, the follwing sequence databasee is in Snake format and
contains four sequences: </p>

<pre>ABCD<br>ABAB<br>CACD<br>ADAC<br></pre>

<p>The <strong>BMS format </strong>is defined as follows:</p>

<ul>

  <li>each line contains a sequence id and an item, both represented as
integers</li>
</ul>

<p>For example, the follwing sequence databasee is in BMS format and
contains four sequences with the ids 10, 20, 30 and 40, respectively: </p>

<pre>10 1<br>10 2<br>10 3<br>10 4<br>20 5<br>20 6<br>20 7<br>20 8<br>30 5<br>30 6<br>30 7<br>40 1<br>40 2<br>40 3<br></pre>

<h3><strong><span class="centered"><a name="convtdb" id="example4"> </a></span></strong>
Example 121 : Converting a Transaction Database to SPMF
Format</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Convert_a_transaction_database_to_SPMF_format</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextCSV</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
(4) set <em>input_format =</em><span class="Style2"> CSV_INTEGER</span>
and <em>sequence count = </em><span class="Style2">5</span>.(5) click
"<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Convert_a_transaction_database_to_SPMF_format</span></strong><strong>
contextCSV</strong>.txt output.txt CSV_INTEGER 5</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2">contextCSV.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestConvertTransactionDatabaseCSVtoSPMF.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>The <strong>tool for converting a transaction databases to SPMF
format. </strong>It takes three prameters as input:</p>

<ul>

  <li>an input file,</li>
  <li>the input file format database format (<em>CSV_INTEGER</em>),</li>
  <li>the number of transactions to be converted</li>
</ul>

<p>The algorithm outputs a transaction database in SPMF format.</p>

<p>The <strong>CSV_INTEGER format</strong> is defined as follows:</p>

<ul>

  <li>each line is a transaction</li>
  <li>each transaction is a list of items represented by positive
integers (&gt;0) separated by commas.</li>
</ul>

<p>For example, the follwing sequence database is in CSV_INTEGER format
and contains four sequences: </p>

<pre>1,2,3,4<br>5,6,7,8<br>5,6,7<br>1,2,3</pre>

<p> Other formats will be added eventually. </p>

<h3><strong><span class="centered"><a name="convseqtrans" id="example42">
</a></span></strong> Example 122 : Converting a Sequence
Database to a Transaction Database</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Convert_sequence_database_to_transaction_database</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixspan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
and <em>sequence count = </em><span class="Style2">5</span> (5) click
"<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Convert_sequence_database_to_transaction_database </span></strong><strong>contextPrefixspan</strong>.txt
output.txt 5</span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2"><strong>contextPrefixspan</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestConvertSeqDBToTransDB.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool converts a sequence database to a transaction database
by removing the ordering between items. This tool is useful if you have
a sequence database and you want to apply an algorithm that is designed
to be applied on a sequence database. For example, you could take a
sequence database and convert it to a transaction database to then
apply and association rule mining algorithm.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool</strong> takes two prameters as input:</p>
  <ul>
    <li>an input file (the sequence database),</li>
    <li>the number of lines of the input file to be converted</li>
  </ul>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong><span class="Style2">contextPrefixspan.txt</span></strong>"
of the SPMF distribution<strong>.</strong> Note that it is assumed that
no items appear twice in the same itemset and that items in an itemset
are lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>The output is a transaction database in SPMF format. A transaction
database is a set of transactions. Each transaction an unordered set of
items (symbols) represented by positive integers. For example, consider
the following database. The output for this example would be the
following transaction database<strong>.</strong> It contains five
transactions. The first transaction contains the set of items {1, 3, 4,
6}.</p>
  <blockquote> </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 2, 3, 4, 6}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{1, 2, 3, 4, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 4, 5, 6}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{1, 2, 3, 5, 6, 7}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong> output file format</strong> is defined as follows.
It is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
  <p>For example, for the previous example, the output file is defined
as follows:</p>
  <p>1 2 3 4 6 <br>
1 2 3 4 5 <br>
1 2 3 4 5 6 <br>
1 2 3 5 6 7 </p>
</blockquote>

<h3><strong><span class="centered"><a name="convtransseq" id="example46">
</a></span></strong> Example 123 : Converting a
Transaction Database to a Sequence Database</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Convert_transaction_database_to_sequence_database</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPasquier99</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>")
and <em>sequence count = </em><span class="Style2">5</span> (5) click
"<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Convert_transaction_database_to_sequence_database</span></strong><strong>
contextPasquier99</strong>.txt output.txt 5</span> in a folder
containing <span class="Style2">spmf.jar</span> and the example input
file <span class="Style2"><strong>contextPasquier99</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestConvertTransDBtoSeqDB.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool converts a transaction database to a sequence database.
It should be used carefully since it assumes that each transaction is a
sequence, and that items in each transaction are sequentially ordered,
which is usually not the case in real-life transaction databases. </p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool</strong> takes two prameters as input:</p>
  <ul>
    <li>an input file (the transaction database),</li>
    <li>the number of lines of the input file to be converted</li>
  </ul>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>What is the output?</p>

<blockquote>
  <p>The output is a sequence database in SPMF format. A sequence
database is a set of sequences. Each sequence is an ordered list of
itemsets. Each itemset is an unordered set of items (symbols)
represented by positive integers. The output for this example is the
following sequence database<strong>.</strong> It contains five
sequences. The first sequence indicates that item 1 is followed by item
3, which is followed by item 4.</p>
  <blockquote> </blockquote>
  <table align="center" border="1" width="401">
    <tbody>
      <tr>
        <td width="144"><strong>Sequence id</strong></td>
        <td width="241"><strong>Itemsets</strong></td>
      </tr>
      <tr>
        <td><strong>s1</strong></td>
        <td>{1},{3}, {4}</td>
      </tr>
      <tr>
        <td><strong>s2</strong></td>
        <td>{2},{3},{5}</td>
      </tr>
      <tr>
        <td><strong>s3</strong></td>
        <td>{1}, {2}, {3}, {5}</td>
      </tr>
      <tr>
        <td><strong>s4</strong></td>
        <td>{2}, {5}, </td>
      </tr>
      <tr>
        <td><strong>s5</strong></td>
        <td>{1}, {2}, {3}, {5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is defined as follows. It
is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the output file for this example contains five
lines (five sequences).</p>
  <blockquote>
    <p>1 -1 3 -1 4 -1 -2<br>
2 -1 3 -1 5 -1 -2<br>
1 -1 2 -1 3 -1 5 -1 -2<br>
2 -1 5 -1 -2<br>
1 -1 2 -1 3 -1 5 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the item 1 is followed
by item 3, which is followed by item 4.</p>
</blockquote>

<h3><strong><span class="centered"><a name="genseq" id="genseq"> </a></span></strong>
Example 124 : Generating a Synthetic Sequence Database</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Generate_a_sequence_database</span>"
    </strong>algorithm<strong>, </strong> (2) set the output file name
(e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (3)
choose 100 sequences, 1000 maximum distinct items, 3 items by itemset,
7 itemsets per sequence (4) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Generate_a_sequence_database</span></strong>
no_input_file output.txt 100 1000 3 7</span> in a folder containing <span class="Style2">spmf.jar</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGenerateSequenceDatabase.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool is a random generator of sequence databases. It can be
used to <strong>generate synthetic sequence databases </strong>to
compare the performance of data mining algorithms that takes a sequence
database as input.</p>
  <p>Synthetic databases are often used in the data mining litterature
to evaluate algorithms. In particular, they are useful for comparing
the scalability of algorithms. For example, one can generate sequence
databases having various size and see how the algorithms react in terms
of execution time and memory usage with respect to the database size.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool for generating a sequence databases </strong>takes
four prameters as input:</p>
  <blockquote>
    <p>1) the number of sequences to be generated (an integer &gt;= 1)</p>
    <p>2) the maximum number of distinct item that the database should
contain (an integer &gt;= 1),</p>
    <p>3) the number of items that each itemset should contain (an
integer &gt;= 1)</p>
    <p>4) the number of itemsets that each sequence should contain (an
integer &gt;= 1)</p>
  </blockquote>
</blockquote>

What is the output?
<blockquote>
  <p>The algorithm outputs a sequence database respecting these
parameters. The database is generated by using a random number
generator.</p>
</blockquote>

<h3><strong><span class="centered"><a name="genseqt" id="genseqt"> </a></span></strong>
Example 125 : Generating a Synthetic Sequence Database
with Timestamps</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Generate_a_sequence_database_with_timetamps</span>"</strong>
algorithm<strong>, </strong> (2) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (3) choose 100
sequences, 1000 maximum distinct items, 3 item by itemset, 7 itemsets
per sequence (4) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Generate_a_sequence_database_with_timestamps</span></strong>
no_input_file output.txt 100 1000 3 7</span> in a folder containing <span class="Style2">spmf.jar</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGenerateSequenceDatabaseWithTimeStamps.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool is a random generator of <strong>sequence databases
with timestamps</strong>. It can be used to <strong>generate synthetic
sequence databases with timestamps </strong>to compare the performance
of data mining algorithms that takes a sequence database with
timestamps as input.</p>
  <p>Synthetic databases are often used in the data mining litterature
to evaluate algorithms. In particular, they are useful for comparing
the scalability of algorithms. For example, one can generate sequence
databases having various size and see how the algorithms react in terms
of execution time and memory usage with respect to the database size.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool for generating a sequence databases with
timestamps </strong>takes four prameters as input:</p>
  <blockquote>
    <p>1) the number of sequences to be generated (an integer &gt;= 1)</p>
    <p>2) the maximum number of distinct item that the database should
contain (an integer &gt;= 1),</p>
    <p>3) the number of items that each itemset should contain (an
integer &gt;= 1)</p>
    <p>4) the number of itemsets that each sequence should contain (an
integer &gt;= 1)</p>
  </blockquote>
</blockquote>

What is the output?
<blockquote>
  <p>The algorithm outputs a sequence database with timestamps
respecting these parameters. The database is generated by using a
random number generator.</p>
</blockquote>

<h3><strong><span class="centered"><a name="gentrans" id="gentrans"> </a></span></strong>
Example 126 : Generating a Synthetic Transaction Database</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Generate_a_transaction_database</span>"</strong>
algorithm<strong>, </strong> (2) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (3) choose 100
transactions, 1000 maximum distinct items and 10 items per transaction
(4) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">Generate_a_transaction_database</span></strong>
no_input_file output.txt 100 1000 10</span> in a folder containing <span class="Style2">spmf.jar</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGenerateTransactionDatabase.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool is a random generator of <strong>transaction databases</strong>.
It can be used to <strong>generate synthetic transaction databases
with timestamps </strong>to compare the performance of data mining
algorithms that takes a transaction database as input.</p>
  <p>Synthetic databases are often used in the data mining litterature
to evaluate algorithms. In particular, they are useful for comparing
the scalability of algorithms. For example, one can generate sequence
databases having various size and see how the algorithms react in terms
of execution time and memory usage with respect to the database size.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool for generating a transaction </strong>takes
three prameters as input:</p>
  <blockquote>
    <blockquote>
      <p>1) the number of transactions to be generated (an integer
&gt;= 1)</p>
      <p>2) the maximum number of distinct item that the database
should contain (an integer &gt;= 1),</p>
      <p>3) the number of items that each transaction should contain
(an integer &gt;= 1)</p>
    </blockquote>
  </blockquote>
</blockquote>

What is the output?
<blockquote>
  <p>The algorithm outputs a transaction database database respecting
the parameters provided. A random number generator is used to generate
the database.</p>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. It is important to
note that an item is not allowed to appear twice in the same
transaction and that items are assumed to be sorted by lexicographical
order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong> output file format</strong> is defined as follows.
It is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
  <p>For example, for the previous example, an output file could be the
following:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
</blockquote>

<h3><strong><span class="centered"><a name="genutilvalues" id="gentrans2"> </a></span></strong> Example 127 :
Generating synthetic utility values for a transaction database without
utility values</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Generate_utility_values_for_transaction_database</span>"</strong>
algorithm<strong>, </strong> (2) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>") (3) choose 10 as
max quantity and 1 as multiplicative factor (4) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">Generate_utility_values_for_transaction_database</span></strong>
contextPasquier99.txt output.txt 10 1</span> in a folder containing <span class="Style2">spmf.jar</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestTransactionDatabaseUtilityGenerator.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool that generate synthetic <strong>utility values</strong>
for a <strong>transaction database </strong>without utility value.
This is useful to generate datasets that can be used for <strong>high
utility-itemset mining</strong>.</p>
  <p>Transaction database with synthetic utiliy values are often used
in the data mining litterature to evaluate <strong>high utility
itemset mining algorithms</strong>.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool</strong> takes as parameter an input file (a
transaction database), and two parameters that are used for generating
the synthetic utility values:</p>
  <ul>
    <li><strong>the maximum internal utility </strong>(quantity) of
each item in a transaction, such that internal utility values will be
generated in the [1, maximum_internal_utility] interval</li>
    <li><strong>a multiplicative factor X</strong>, such that the
external utility of each item will be generated by calling
Random.nextGaussian() multiplied by X.</li>
  </ul>
  <p>A <strong>transaction database</strong> is a set of transactions.
Each <strong>transaction</strong> is a set of items. For example,
consider the following transaction database. It contains 5 transactions
(t1, t2, ..., t5) and 5 items (1,2, 3, 4, 5). For example, the first
transaction represents the set of items 1, 3 and 4. This database is
provided as the file <strong>contextPasquier99.txt</strong> in the
SPMF distribution. It is important to note that an item is not allowed
to appear twice in the same transaction and that items are assumed to
be sorted by lexicographical order in a transaction.</p>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <blockquote> </blockquote>
  </blockquote>
</blockquote>

What is the output?
<blockquote>
  <p>The output is a transaction database with utility information.</p>
  <p>It is a transaction database where each item appearing in a
transaction has a purchase quantity (a.k.a internal utility).
Furthermore, each item has a weight (a.k.a external utility or weight)
that can be interpreted as a unit profit when buying one unit of the
item.</p>
  <p>In SPMF, the format of transaction database with utility
information is represented as follows. Consider the following database:</p>
  <table align="center" border="1" width="721">
    <tbody>
      <tr>
        <td width="56"> <br>
        </td>
        <td width="156"><strong>Items</strong></td>
        <td width="190"><strong>Transaction utility</strong></td>
        <td width="291"><strong>Item utilities for this transaction</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>1 3 4</td>
        <td>9</td>
        <td>1 3 5</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>2 3 5</td>
        <td>14</td>
        <td>3 3 8</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>1 2 3 5</td>
        <td>9</td>
        <td>1 5 2 1</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>2 5</td>
        <td>12</td>
        <td>6 6</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>1 2 3 5</td>
        <td>11</td>
        <td>2 3 4 2</td>
      </tr>
    </tbody>
  </table>
  <blockquote>
    <p>Each line of the <strong>database</strong> is:</p>
    <ul>
      <li> a transaction, defined as set of items (the first column of
the table), </li>
      <li>the sum of the utilities (e.g. profit) of these items in this
transaction (the second column of the table),</li>
      <li>the utility of each item for this transaction (e.g. external
utility multiplied by internal utility)(the third column of the table).</li>
    </ul>
    <p>Note that the value in the second column for each line is the
sum of the values in the third column.</p>
    <p>What are real-life examples of such a database? There are
several applications in real life. One application is a customer
transaction database. Imagine that each transaction represents the
items purchased by a customer. The first customer named "<strong>t1</strong>"
bought items 1, 3 and 4. The amount of money spent for each item is
respectively 1 $, 3 $ and 5 $. The total amount of money spent in this
transaction is 1 + 3 + 5 = 9 $.</p>
  </blockquote>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is defined as follows. It
is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
  <p>For example, for the previous example, the input file is defined
as follows:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong> output file format</strong> is defined as follows.
It is a text file. Each lines represents a transaction with utility
information. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
item is represented by a positive integer. Each item is separated from
the next item by a single space. It is assumed that all items within a
same transaction (line) are sorted according to a total order (e.g.
ascending order) and that no item can appear twice within the same
transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, an output file that can be generated for the example
input file could the following:</p>
  <blockquote>
    <p>1 3 4:9:1 3 5<br>
2 3 5:14:3 3 8<br>
1 2 3 5:9:1 5 2 1<br>
2 5:12:6 6<br>
1 2 3 5:11:2 3 4 2 </p>
    <table align="center" border="1" width="721">
      <tbody>
        <tr>
          <td width="56"> <br>
          </td>
          <td width="156"><strong>Items</strong></td>
          <td width="190"><strong>Transaction utility</strong></td>
          <td width="291"><strong>Item utilities for this transaction</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>1 3 4</td>
          <td>9</td>
          <td>1 3 5</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>2 3 5</td>
          <td>14</td>
          <td>3 3 8</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>1 2 3 5</td>
          <td>9</td>
          <td>1 5 2 1</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>2 5</td>
          <td>12</td>
          <td>6 6</td>
        </tr>
        <tr>
          <td><strong>t5</strong></td>
          <td>1 2 3 5</td>
          <td>11</td>
          <td>2 3 4 2</td>
        </tr>
      </tbody>
    </table>
  </blockquote>
  <p>Consider the first line. It means that the transaction {1, 3, 4}
has a total utility of 9 and that items 1, 3 and 4 respectively have a
utility of 1, 3 and 5 in this transaction. </p>
</blockquote>

<blockquote>
  <blockquote> </blockquote>
</blockquote>

<h3><strong><span class="centered"><a name="statsseq" id="statsseq"> </a></span></strong>
Example 128 : Calculate Statistics for a Sequence Database</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Calculate_stats_for_a_sequence_database</span>"</strong>
algorithm<strong>, </strong> (2) choose the input file <span class="Style2"><strong><span class="Style9">contextPrefixSpan</span></strong>.txt</span>
(3) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Calculate_stats_for_a_sequence_database</span></strong><strong>
    </strong> <strong><span class="Style9">contextPrefixSpan</span></strong>.txt
no_output_file</span> in a folder containing <span class="Style2">spmf.jar</span>
and the input file <span class="Style2"><strong><span class="Style9">contextPrefixSpan</span></strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGenerateSequenceDatabaseStats.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
  </li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool is a tool for generating statistics about a sequence
database. It can be used to know for example if the database is dense
or sparse before applying a data mining algorithms.</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The input is a sequence database. A sequence database is a set of
sequence. Each sequence is an ordered list of itemsets. An itemset is
an unordered list of items (symbols). For example, consider the
following database. This database is provided in the file "<strong>contextPrefixSpan.txt</strong>"
of the SPMF distribution<strong>.</strong> It contains 4 sequences. The
second sequence represents that the set of items {1 4} was followed by
{3}, which was followed by {2, 3}, which were followed by {1, 5}. It is
a <em>sequence database </em>(as defined in <a href="http://www.philippe-fournier-viger.com/spmf/prefixspan.pdf" rel="nofollow">Pei et al., 2004</a>). </p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>The output is statistics about the sequence database. For example,
if we use the tool on the previous sequence database given as example,
we get the following statistics:</p>
</blockquote>

<ul>

  <li>the number of distinct items in this database: 7</li>
  <li>the largest item id: 7</li>
  <li>the average number of itemsets per sequence: 5</li>
  <li>the average number of distinct items per sequence: 5.5</li>
  <li>the average number of occurences in a sequence for each item
appearing in a sequence: 1.41</li>
  <li>the average number of items per itemset: 1.55</li>
</ul>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<h3><strong><span class="centered"><a name="stattrans" id="statsseq2"> </a></span></strong>
Example 129 : Calculate Statistics for a Transaction
Database</h3>

<blockquote>
  <p>How to run this example? </p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Calculate_stats_for_a_transaction_database</span>"</strong>
algorithm<strong>, </strong> (2) choose the input file <span class="Style2"><strong><span class="Style9">contextApriori</span></strong>.txt</span>
(3) click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Calculate_stats_for_a_transaction_database</span></strong><strong>
      </strong><strong><span class="Style9">contextPasquier99</span></strong>.txt
no_output_file</span> in a folder containing <span class="Style2">spmf.jar</span>
and the input file <span class="Style2"><strong><span class="Style9">contextPasquier99</span></strong>.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestGenerateTransactionDatabaseStats.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
    </li>
  </ul>
  <p>What is this tool?</p>
  <blockquote>
    <p>This tool is a tool for generating statistics about a
transaction database. It can be used to know for example if the
database is dense or sparse before applying a data mining algorithms.</p>
  </blockquote>
  <p>What is the input?</p>
  <blockquote>
    <p>The input is a transaction database (aka formal context). A
transaction database is a set of transactions. Each transactions an
unordered set of items (symbols) represented by positive integers. For
example, consider the following database. This database is provided in
the file "<strong>contextPasquier99.txt</strong>" of the SPMF
distribution<strong>.</strong> It contains five transactions. The first
transactions contains the set of items {1, 3, 4}.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 4}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
      <tr>
        <td><strong>t4</strong></td>
        <td>{2, 5}</td>
      </tr>
      <tr>
        <td><strong>t5</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
  <p>What is the output?</p>
  <blockquote>
    <p>The output is statistics about the transaction database. For
example, if we use the tool on the previous sequence database given as
example, we get the following statistics:</p>
    <ul>
      <li>The number of transactions is 5</li>
      <li>File
C:\Users\ph\Desktop\SPMF\ca\pfv\spmf\test\contextPasquier99.txt</li>
      <li>The number of distinct items is 5</li>
      <li>The smallest item id is 1</li>
      <li>The largest item id is 5</li>
      <li>The average number of items per transactions is 3 (standard
deviation: 0.7 variance: 0.5)</li>
      <li>The average support of items in this database is : 2.4
(standard deviation: 0.8 variance: 0.64 min value: 1 max value: 3)<br>
      </li>
    </ul>
  </blockquote>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is defined as follows. It
is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
  <p>For example, for the previous example, an output file could be the
following:</p>
  <p>1 3 4<br>
2 3 5<br>
1 2 3 5<br>
2 5 <br>
1 2 3 5</p>
</blockquote>

<h3><strong><span class="centered"><a name="addtimestamps" id="example47"> </a></span></strong> Example 130 : Add
consecutive timestamps to a sequence database without timestamps</h3>

<p>How to run this example? </p>

<ul>

  <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Add_consecutive_timestamps_to_sequence_database</span>"</strong>
algorithm<strong>, </strong> (2) select the input file <strong>"<span class="Style2">contextPrefixspan</span><span class="Style2">.txt"</span></strong>,
(3) set the output file name (e.g. "<span class="Style2"><strong>output</strong>.txt</span>"),
(4) click "<span class="Style2">Run algorithm</span>".</li>
  <li><strong>If you want to execute this example from the command line</strong>,
then execute this command: <br>
    <span class="Style2">java -jar spmf.jar run<strong><span class="Style9"> Add_consecutive_timestamps_to_sequence_database </span></strong><strong>contextPrefixspan</strong>.txt
output.txt </span> in a folder containing <span class="Style2">spmf.jar</span>
and the example input file <span class="Style2"><strong>contextPrefixspan</strong>.txt</span>.</li>
  <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestAddTimeStampsToSequenceDatabase.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span></li>
</ul>

<p>What is this tool?</p>

<blockquote>
  <p>This tool converts a sequence database to a sequence database with
timestamps. This is useful for applying an algorithm that requires
timestamp information. This tool assumes that each itemset in a
sequence have consecutive timestamps, i.e. that timestamps are assigned
as 0,1,2 ... .</p>
</blockquote>

<p>What is the input?</p>

<blockquote>
  <p>The <strong>tool</strong> takes a sequence database as input.</p>
  <p>A <strong>sequence database</strong> is a set of sequences where
each sequence is a list of itemsets. An itemset is an unordered set of
items. For example, the table shown below contains four sequences. The
first sequence, named S1, contains 5 itemsets. It means that item 1 was
followed by items 1 2 and 3 at the same time, which were followed by 1
and 3, followed by 4, and followed by 3 and 6. It is assumed that items
in an itemset are sorted in lexicographical order. This database is
provided in the file "<strong><span class="Style2">contextPrefixspan.txt</span></strong>"
of the SPMF distribution<strong>.</strong> Note that it is assumed that
no items appear twice in the same itemset and that items in an itemset
are lexically ordered.</p>
</blockquote>

<table align="center" border="1" width="461">

  <tbody>
    <tr>
      <td><strong>ID</strong></td>
      <td><strong>Sequences</strong></td>
    </tr>
    <tr>
      <td>S1</td>
      <td>(1), (1 2 3), (1 3), (4), (3 6)</td>
    </tr>
    <tr>
      <td>S2</td>
      <td bgcolor="#ffffff">(1 4), (3), (2 3), (1 5)</td>
    </tr>
    <tr>
      <td>S3</td>
      <td>(5 6), (1 2), (4 6), (3), (2)</td>
    </tr>
    <tr>
      <td>S4</td>
      <td>(5), (7), (1 6), (3), (2), (3)</td>
    </tr>
  </tbody>
</table>

<p>What is the output?</p>

<blockquote>
  <p>The output is the same sequence database, except that consecutive
timestamps have been added to each itemset in each sequence. For
example, consider the following database. The timestamps are indicated
in bold. For example, the first sequence indicates that item 1 appeared
at time <strong>0</strong>, that it was followed by items 1, 2 and 3
at time <strong>1</strong>, which was followed by items 1 and 3 at
time <strong>2</strong>, which was followed by item 4 at time <strong>3</strong>,
was followed by items 3 and 6 at time <strong>4</strong>, </p>
  <table align="center" border="1" width="461">
    <tbody>
      <tr>
        <td><strong>ID</strong></td>
        <td><strong>Sequences</strong></td>
      </tr>
      <tr>
        <td>S1</td>
        <td>(<strong>0,</strong> 1), (<strong>1,</strong> 1 2 3), (<strong>2,
        </strong>1 3), (<strong>3, </strong>4), (<strong>4, </strong>3
6)</td>
      </tr>
      <tr>
        <td>S2</td>
        <td bgcolor="#ffffff">(<strong>0,</strong> 1 4), (<strong>1,</strong>
3), (<strong>2, </strong>2 3), (<strong>3, </strong>1 5)</td>
      </tr>
      <tr>
        <td>S3</td>
        <td>(<strong>0,</strong> 5 6), (<strong>1,</strong> 1 2), (<strong>2,
        </strong>4 6), (<strong>3, </strong>3), (<strong>4, </strong>2)</td>
      </tr>
      <tr>
        <td>S4</td>
        <td>(<strong>0,</strong> 5), (<strong>1,</strong> 7), (<strong>2,
        </strong>1 6), (<strong>3, </strong>3), (<strong>4, </strong>2),
(<strong>5, </strong>3)</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong>input file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each item from a sequence is a positive integer and items
from the same itemset within a sequence are separated by single space.
Note that it is assumed that items within a same itemset are sorted
according to a total order and that no item can appear twice in the
same itemset. The value "-1" indicates the end of an itemset. The value
"-2" indicates the end of a sequence (it appears at the end of each
line). For example, the input file "contextPrefixSpan.txt" contains the
following four lines (four sequences).</p>
  <blockquote>
    <p>1 -1 1 2 3 -1 1 3 -1 4 -1 3 6 -1 -2<br>
1 4 -1 3 -1 2 3 -1 1 5 -1 -2<br>
5 6 -1 1 2 -1 4 6 -1 3 -1 2 -1 -2<br>
5 -1 7 -1 1 6 -1 3 -1 2 -1 3 -1 -2</p>
  </blockquote>
  <p>The first line represents a sequence where the itemset {1} is
followed by the itemset {1, 2, 3}, followed by the itemset {1, 3},
followed by the itemset {4}, followed by the itemset {3, 6}. The next
lines follow the same format.</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong>output file format</strong> is defined as follows. It
is a text file where each line represents a sequence from a sequence
database. Each line is a list of itemsets, where each itemset has a
timestamp represented by a positive integer and each item is
represented by a positive integer. Each itemset is first represented by
it timestamp between the "&lt;" and "&gt; symbol. Then, the items of
the itemset appear separated by single spaces. Finally, the end of an
itemset is indicated by "-1". After all the itemsets, the end of a
sequence (line) is indicated by the symbol "-2". Note that it is
assumed that items are sorted according to a total order in each
itemset and that no item appears twice in the same itemset.</p>
  <p>For example, the output file of the example contains the following
four lines (four sequences).</p>
  <blockquote>
    <p>&lt;0&gt; 1 -1 &lt;1&gt; 1 2 3 -1 &lt;2&gt; 1 3 -1 &lt;3&gt; 4
-1 &lt;4&gt; 3 6 -1 -2<br>
&lt;0&gt; 1 4 -1 &lt;1&gt; 3 -1 &lt;2&gt; 2 3 -1 &lt;3&gt; 1 5 -1 -2<br>
&lt;0&gt; 5 6 -1 &lt;1&gt; 1 2 -1 &lt;2&gt; 4 6 -1 &lt;3&gt; 3 -1
&lt;4&gt; 2 -1 -2<br>
&lt;0&gt; 5 -1 &lt;1&gt; 7 -1 &lt;2&gt; 1 6 -1 &lt;3&gt; 3 -1 &lt;4&gt;
2 -1 &lt;5&gt; 3 -1 -2 </p>
  </blockquote>
  <p>Consider the first line. It indicates that item 1 appeared at time
  <strong>0</strong>, that it was followed by items 1, 2 and 3 at time <strong>1</strong>,
which was followed by items 1 and 3 at time <strong>2</strong>, which
was followed by item 4 at time <strong>3</strong>, was followed by
items 3 and 6 at time <strong>4</strong>, </p>
</blockquote>

<h3><strong><span class="centered"><a name="arff" id="arff"> </a></span></strong>
Example 131 : Using the ARFF format in the source code
version of SPMF</h3>

<p>The GUI interface and command line interface of SPMF can read the
ARFF file format since version 0.93 of SPMF and this is totally
transparent to the user. But what if you want to use the ARFF format
when running algorithms from the source code? This example explains how
to do it and it is quite simple.</p>

<p>But before presenting the example, let's explain a few things about
how the ARFF support is implemented in SPMF:</p>

<ul>

  <li>The ARFF format is only supported for algorithms that take a
transaction database as input (most association rule and itemset mining
algorithm such as Apriori and FPGrowth) because ARFF does provide
constructs for representing sequential data.</li>
  <li>The full ARFF format is supported except that (1) the character
"=" is forbidden and (2) escape characters are not considered.</li>
  <li>ARFF generally represents items in files as strings while the
SPMF format and algorithms use an integer representation which is much
more memory and time efficient for the algorithms concerned. To support
ARFF while keeping the very efficient integer representation of our
algorithm, we decided to implement the ARFF support in a way that did
not require to modify the source code of the algorithms. The solution
is to convert the input before running an algorithm and then, to
convert the output.</li>
</ul>

<p>Having said that, we will now explain how to use the ARFF format in
the source code with an example. We will use the Apriori algorithm but
the steps are the same for the other algorithms. We will first show how
to run the Apriori algorithm if the input file is in SPMF format. Then,
we will show how to run the Apriori algorithm if the input is in ARFF
format to illustrate the differences.</p>

<p><strong>If the input is in SPMF format</strong></p>

<p>To run Apriori with a file "input.txt" in SPMF format with the
parameter minsup = 0.4, the following code is used:</p>

<blockquote>
  <p> AlgoApriori apriori = new AlgoApriori();<br>
apriori.runAlgorithm(0.4, "input.txt", "output.txt");</p>
</blockquote>

<p><strong>If the input is in ARFF format</strong></p>

<p>Now let's say that the input file is in the ARFF format. </p>

<blockquote>
  <p>// We first need to convert the input file from ARFF to SPMF
format. To do that, we create a transaction database converter. Then we
call its method "convertARFFandReturnMap" to convert the input file to
the SPMF format. It produces a converted input file named
"input_converted.arff". Moreover, the conversion method returns a map
containing mapping information between the data in ARFF format and the
data in SPMF format.</p>
  <p>TransactionDatabaseConverter converter = new
TransactionDatabaseConverter();<br>
Map&lt;Integer, String&gt; mapping =
converter.convertARFFandReturnMap("input.arff", "input_converted.txt",
Formats.ARFF, Integer.MAX_VALUE);</p>
  <p>// Then we run the algorithm with the converted file
"input_converted.txt". This creates a file "output.txt" containing the
result.</p>
  <p>AlgoApriori apriori = new AlgoApriori();<br>
apriori.runAlgorithm(0.4, "input_converted.txt", "output.txt");</p>
  <p>// Finally, we need to use the mapping to convert the output file
so that the result is shown using the names that are found in the ARFF
file rather than the integer-based representation used internally by
the Apriori algorithm. This is very simple and performed as follows.
The result is a file named "final_output.txt".</p>
  <p>ResultConverter converter = new ResultConverter();<br>
converter.convert(mapping, "output.txt", "final_output.txt");</p>
</blockquote>

<p>What is the cost of using the ARFF format in terms of performance?
The only additional cost when using ARFF is the cost of converting the
input and output files, which is generally much smaller than the cost
of performing the data mining. In the future, we plan to add support
for SQL databases, Excel files and other formats by using a similar
conversion mechanism that does not affect the performance of the mining
phase. We also plan to add support for the visualizations of patterns.</p>

<h3><strong><span class="centered"><a name="fixtdb" id="statsseq3"> </a></span></strong>
Example 132 : Fix a Transaction Database</h3>

<blockquote>
  <p>How to run this example? </p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
choose the <strong>"<span class="Style9">Fix_a_transaction_database</span>"</strong>
algorithm<strong>, </strong> (2) choose the input file <span class="Style2"><strong><span class="Style9">contextIncorrect</span></strong>.txt</span>
(3) click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run<strong><span class="Style9">Fix_a_transaction_database</span></strong><strong> </strong><strong><span class="Style9">contextIncorrect</span></strong>.txt no_output_file</span>
in a folder containing <span class="Style2">spmf.jar</span> and the
input file <span class="Style2"><strong><span class="Style9">contextIncorrect</span></strong>.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
the file <strong><span class="Style2">"MainTestFixTransactionDatabase.java"</span></strong>
in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>.
    </li>
  </ul>
  <p>What is this tool?</p>
  <blockquote>
    <p>The tool "Fix_a_transaction_database" is a small program that
fix some common problems in a transaction database file in SPMF format.
The tool fixes two common problems: (1) an item appears more than once
in a transaction, (2) transactions are not sorted. To fix the first
problem the tool will keep only one occurrence of each item in a
transaction. So if an items appears more than once in a transaction, it
will appears only once after applying the tool. To fix the second
problem, the tool sorts each transaction according to the
lexicographical ordering (because it is required by most itemset and
association rule mining algorithms).</p>
  </blockquote>
  <p>What is the input?</p>
  <blockquote>
    <p>The input is a transaction database in SPMF format that need to
be fixed. A transaction database is a set of transactions. Each
transactions an unordered set of items (symbols) represented by
positive integers. For example, consider the following database. This
database is provided in the file "<strong>contextIncorrect.txt</strong>"
of the SPMF distribution<strong>.</strong> It contains three
transactions. The first transactions contains the set of items {1, 3,
4}. However, this transaction database has some problems. The first
transaction contains an item 3 that appears more than once. Moreover,
transactions are not sorted.</p>
  </blockquote>
  <table align="center" border="1" width="316">
    <tbody>
      <tr>
        <td width="144"><strong>Transaction id</strong></td>
        <td width="156"><strong>Items</strong></td>
      </tr>
      <tr>
        <td><strong>t1</strong></td>
        <td>{1, 3, 3 4, 3}</td>
      </tr>
      <tr>
        <td><strong>t2</strong></td>
        <td>{5, 3, 2}</td>
      </tr>
      <tr>
        <td><strong>t3</strong></td>
        <td>{1, 2, 3, 5}</td>
      </tr>
    </tbody>
  </table>
  <p>What is the output?</p>
  <blockquote>
    <p>The output is a transaction database where each transaction is
sorted and no item appears more than once. For example, the output
using the above example is:</p>
    <table align="center" border="1" width="316">
      <tbody>
        <tr>
          <td width="144"><strong>Transaction id</strong></td>
          <td width="156"><strong>Items</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>{1, 3, 4}</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>{2, 3, 5}</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>{1, 2, 3, 5}</td>
        </tr>
      </tbody>
    </table>
  </blockquote>
</blockquote>

<p>Input file format</p>

<blockquote>
  <p>The <strong> input file format</strong> is defined as follows. It
is a text file. An item is represented by a positive integer. A
transaction is a line in the text file. In each line (transaction),
items are separated by a single space. It is assumed that all items
within a same transaction (line) are sorted according to a total order
(e.g. ascending order) and that no item can appear twice within the
same line.</p>
  <p>For example, for the previous example, an input file that is
incorrect is provided:</p>
  <p>1 3 3 4 3<br>
5 3 2<br>
1 2 3 5</p>
</blockquote>

<p>Output file format</p>

<blockquote>
  <p>The <strong> output file format</strong> is the same as the input
format. But the problems contained in the input file have been fixed.</p>
  <p>1 3 4<br>
    2 3 5<br>
  1 2 3 5</p>
</blockquote>
<h3><strong><span class="centered"><a name="removeutility" id="statsseq4"> </a></span></strong> Example 133 : Remove utility information from a transaction database</h3>
<blockquote>
  <p>How to run this example? </p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
      choose the <strong>"<span class="Style9">Remove_utility_information_from_a_transaction_database</span>"</strong> algorithm<strong>, </strong> (2) choose the input file <span class="Style2"><strong><span class="Style9">DB_utility</span></strong>.txt</span> (3) click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
      line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">Remove_utility_information_from_a_transaction_database</span></strong><strong> </strong><strong><span class="Style9">DB_Utility</span></strong>.txt output.txt</span> in a folder containing <span class="Style2">spmf.jar</span> and the
      input file <span class="Style2"><strong><span class="Style9">DB_Utility</span></strong>.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
      the file <strong><span class="Style2">"MainTestTransactionUtilityRemover.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
  </ul>
  <p>What is this tool?</p>
  <blockquote>
    <p>This tool is a small program that is designed to convert a transaction database with utility information to a transaction database that does not contain utility information. For example, this tool can be used to convert a database such as Foodmart, available on the dataset page of the SPMF website so that the dataset can be used with frequent itemset mining algorithm such as Apriori, FPGrowth, etc., and association rule mining algorithms.</p>
  </blockquote>
  <p>What is the input?</p>
  <blockquote>
    <p><strong>The input is </strong>a transaction database
with utility information.For example, lLet's consider the following database consisting of 5 transactions
      (t1,t2...t5) and 7 items (1, 2, 3, 4, 5, 6, 7). This database is
      provided in the text file "<strong>DB_utility.txt</strong>" in the
      package <strong>ca.pfv.spmf.tests </strong>of the SPMF distribution<strong>.</strong></p>
    <table align="center" border="1" width="721">
      <tbody>
        <tr>
          <td width="56"><br></td>
          <td width="156"><strong>Items</strong></td>
          <td width="190"><strong>Transaction utility</strong></td>
          <td width="291"><strong>Item utilities for this transaction</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>3 5 1 2 4 6</td>
          <td>30</td>
          <td>1 3 5 10 6 5</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>3 5 2 4</td>
          <td>20</td>
          <td>3 3 8 6</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>3 1 4</td>
          <td>8</td>
          <td>1 5 2</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>3 5 1 7</td>
          <td>27</td>
          <td>6 6 10 5</td>
        </tr>
        <tr>
          <td><strong>t5</strong></td>
          <td>3 5 2 7</td>
          <td>11</td>
          <td>2 3 4 2</td>
        </tr>
      </tbody>
    </table>
    <blockquote>
      <p>Each line of the <strong>database</strong> is:</p>
      <ul>
        <li> a set of items (the first column of the table), </li>
        <li>the sum of the utilities (e.g. profit) of these items in this
          transaction (the second column of the table),</li>
        <li>the utility of each item for this transaction (e.g. profit
          generated by this item for this transaction)(the third column of the
          table).</li>
      </ul>
      <p>Note that the value in the second column for each line is the
        sum of the values in the third column.</p>
      <p>What are real-life examples of such a database? There are
        several applications in real life. One application is a customer
        transaction database. Imagine that each transaction represents the
        items purchased by a customer. The first customer named "<strong>t1</strong>"
        bought items 3, 5, 1, 2, 4 and 6. The amount of money spent for each
        item is respectively 1 $, 3 $, 5 $, 10 $, 6 $ and 5 $. The total amount
        of money spent in this transaction is 1 + 3 + 5 + 10 + 6 + 5 = 30 $.</p>
    </blockquote>
  </blockquote><p>What is the output?</p>
  <blockquote>
    <p>The output is a transaction database where the utility information has been removed. For example, the output
      of the above example is:</p>
    <table align="center" border="1" width="228">
      <tbody>
        <tr>
          <td width="56"><br></td>
          <td width="156"><strong>Items</strong></td>
        </tr>
        <tr>
          <td><strong>t1</strong></td>
          <td>3 5 1 2 4 6</td>
        </tr>
        <tr>
          <td><strong>t2</strong></td>
          <td>3 5 2 4</td>
        </tr>
        <tr>
          <td><strong>t3</strong></td>
          <td>3 1 4</td>
        </tr>
        <tr>
          <td><strong>t4</strong></td>
          <td>3 5 1 7</td>
        </tr>
        <tr>
          <td><strong>t5</strong></td>
          <td>3 5 2 7</td>
        </tr>
      </tbody>
    </table>
    <p>The output is written to a file (<strong>output.txt </strong>in this example).</p>
  </blockquote>
</blockquote>
<p>Input file format</p>
<blockquote>
  <p>The <strong> input file format</strong><strong> </strong>is
    defined as follows. It is a text file. Each lines represents a
  transaction. Each line is composed of three sections, as follows.</p>
  <ul>
    <li>First, the items contained in the transaction are listed. An
      item is represented by a positive integer. Each item is separated from
      the next item by a single space. It is assumed that all items within a
      same transaction (line) are sorted according to a total order (e.g.
      ascending order) and that no item can appear twice within the same
      transaction.</li>
    <li>Second, the symbol ":" appears and is followed by the
      transaction utility (an integer).</li>
    <li>Third, the symbol ":" appears and is followed by the utility of
      each item in this transaction (an integer), separated by single spaces.</li>
  </ul>
  <p>For example, for the previous example, the input file is defined
    as follows:</p>
  <blockquote>
    <p>3 5 1 2 4 6:30:1 3 5 10 6 5<br>
      3 5 2 4:20:3 3 8 6<br>
      3 1 4:8:1 5 2<br>
      3 5 1 7:27:6 6 10 5<br>
      3 5 2 7:11:2 3 4 2 </p>
  </blockquote>
  <p>Consider the first line. It means that the transaction {3, 5, 1,
    2, 4, 6} has a total utility of 30 and that items 3, 5, 1, 2, 4 and 6
    respectively have a utility of 1, 3, 5, 10, 6 and 5 in this
    transaction. The following lines follow the same format.</p>
</blockquote>
<p>Output file format</p>
<blockquote>
  <p>The <strong> output file format</strong> is a transaction database.  An item is represented by a positive integer. A
    transaction is a line in the text file. In each line (transaction),
  items are separated by a single space. </p>
  <p>3 5 1 2 4 6<br>
3 5 2 4<br>
3 1 4<br>
3 5 1 7<br>
3 5 2 7</p>
</blockquote>
<h3><strong><span class="centered"><a name="resizedatabase" id="statsseq5"> </a></span></strong> Example 134 : Remove utility information from a transaction database</h3>
<blockquote>
  <p>How to run this example? </p>
  <ul>
    <li><strong>If you are using the graphical interface, </strong>(1)
      choose the <strong>"<span class="Style9">Resize_a_database</span>"</strong> algorithm<strong>, </strong> (2) choose the input file <span class="Style2"><strong><span class="Style9">DB_UtilityPerHUIs</span></strong>.txt</span> (3) select 0.7 as percentage (which means 70%), and (4) click "<span class="Style2">Run algorithm</span>".</li>
    <li><strong>If you want to execute this example from the command
      line</strong>, then execute this command: <br>
      <span class="Style2">java -jar spmf.jar run <strong><span class="Style9">Resize_a_database</span></strong><strong> </strong><strong><span class="Style9">DB_Utility</span></strong>PerHUIs.txt output.txt</span> in a folder containing <span class="Style2">spmf.jar</span> and the
      input file <span class="Style2"><strong><span class="Style9">DB_Utility</span></strong>.txt</span>.</li>
    <li><strong>If you are using the source code version of SPMF, </strong>launch
      the file <strong><span class="Style2">"MainTestResizeDatabaseTool.java"</span></strong> in the package <span class="Style9"><strong>ca.pfv.SPMF.tests</strong></span>. </li>
  </ul>
  <p>What is this tool?</p>
  <blockquote>
    <p>This tool is a small program that is designed to resize a database by using X% of the transactions of an original database. The tool takes as input an original database, and a percentage X. Then it outputs a new file containing X% of the lines of data from the original database.For example, if a database contains 100,000 transactions and this tool is used with a percentage of 75 %, the output will be a database containing the  75,000 first transactions from the original database. This program is designed to work with any database file in SPMF format (text file). This tool is useful for performing scalability experiments when comparing algorithms. For example, one may wants to see the behavior of some algoritms when using 25%, 50%, 75% and 100% of the database.</p>
  </blockquote>
  <p>What is the input?</p>
  <blockquote>
    <p><strong>The input is </strong>a text file in SPMF format. It could be for example a transaction database, a sequence database, or other types of databases used by algorithms offered in SPMF. Moreover the user has to specify a percentage X.</p>
  </blockquote>
  <p>What is the output?</p>
  <blockquote>
    <p>The output is a new file containing 
    X% of the lines of data from the input file.  </p>
  </blockquote>
</blockquote>
<p>Example</p>
<blockquote>
  <p>For example, if the user applies the tool for resizing a database with X = 70 % on the following file <strong>DB_UtilityPerHUIs.txt</strong> in this example:</p>
  <p>3 1:6:1 5<br>
    5:3:3<br>
    3 5 1 2 4:25:1 3 5 10 6<br>
    3 5 2 4:20:3 3 8 6<br>
    3 1 4:8:1 5 2<br>
    3 5 1:22:6 6 10<br>
  3 5 2:9:2 3 4</p>
  <p>The output is a new file (<strong>output.txt </strong>in this example) containing 5 transactions (because 70 % of 7 transactions is 5 transactions):</p>
  <p>3 1:6:1 5<br>
    5:3:3<br>
    3 5 1 2 4:25:1 3 5 10 6<br>
    3 5 2 4:20:3 3 8 6<br>
  3 1 4:8:1 5 2</p>
</blockquote>

</td>

</tr>

<tr>

<td colspan="3" bgcolor="#FFFFCC"><div align="center"><small>Copyright Â© 2008-2016 <a href="http://www.philippe-fournier-viger.com/">Philippe Fournier-Viger</a>. All rights reserved.</small></div></td>

</tr>

</tbody></table>


</body></html>